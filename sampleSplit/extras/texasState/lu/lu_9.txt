Learning to Judge Image Search Results 
∗ 
Xinmei Tian 
† 
University of Science and Technology of China Hefei, Anhui, China 230027 xinmeitian@gmail.com 
Yijuan Lu Texas State University San Marcos, TX 78666 yl12@txstate.edu 
ABSTRACT 
Given the explosive growth of the Web and the popularity of image sharing Web sites, image retrieval plays an increas- ingly important role in our daily lives. Search engines aim to provide beneﬁcial image search results to users in response to queries. The quality of image search results depends on many factors: chosen search algorithms, ranking functions, indexing features, the base image database, etc. Applying diﬀerent settings for these factors generates search result lists with varying levels of quality. Previous research has shown that no setting can always perform optimally for all queries. Therefore, given a set of search result lists generated by diﬀerent settings, it is crucial to automatically determine which result list is the best in order to present it to users. This paper proposes a novel method to automatically iden- tify the best search result list from a number of candidates. There are three main innovations in this paper. First, we propose a preference learning model to quantitatively study the best image search result identiﬁcation problem. Second, we propose a set of valuable preference learning related fea- tures by exploring the visual characters of returned images. Third, our method shows promising potential in applications such as reranking ability assessment and optimal search en- gine selection. Experiments on two image search datasets show that our method achieves about 80% prediction ac- curacy for reranking ability assessment, and selects optimal search engine for about 70% queries correctly. 
Categories and Subject Descriptors 
H.3.3 [Information Search and Retrieval]: Retrieval models 
General Terms 
Algorithm, Experimentation, Performance 
Keywords 
Image retrieval, search results performance comparison, rerank- ing ability assessment 
†This work was performed while the ﬁrst author was a post- doctoral researcher at Texas State University.∗ 
Area chair: Hari Sundaram 
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for proﬁt or commercial advantage and that copies bear this notice and the full citation on the ﬁrst page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior speciﬁc permission and/or a fee. 
MM’11, November 28–December 1, 2011, Scottsdale, Arizona, USA. Copyright 2011 ACM 978-1-4503-0616-4/11/11 ...$10.00. 
Linjun Yang Microsoft Research Asia 
Beijing, China 100190 linjuny@microsoft.com 
Qi Tian 
University of Texas 
at San Antonio San Antonio, TX 78249 qitian@cs.utsa.edu 
1.0  
AP 
Bing 
Google 
0.8  
0.6  
0.4  
0.2  
0.0  
1 
2 
3 
4 
5 
6 
7 
8 
9 
10 
11 
12 
13 
14 
15 
16 
17 
18 
19 
20 
21 
22 
23 
24 
25 
26 
27 
28 
29 
query 
(a) AP@40 on each query and MAP over all queries for Bing and Google (queries are sorted according to their AP diﬀerence for better view). 
MAP 
Query: “White House” 
Bing 
Google 
(b) The top-10 images returned on query “White House”. Figure 1: Image search result comparison between Bing and Google. 
1. INTRODUCTION 
Given the explosive growth of the Web and the popularity of image sharing Web sites, image retrieval plays an increas- ingly important role in our daily lives. Extensive research has been conducted to retrieve images relevant to a given query. Many factors can inﬂuence image search results. Ex- isting work aims to get better search results by focussing their eﬀorts on various aspects of the search process, such as designing eﬀective visual features [15, 16], building eﬃ- cient image indexes [17], developing new ranking algorithms [6, 22] and designing user-friendly interface [24]. The algo- rithms used in these aspects generate result lists of varying quality when used with diﬀerent settings. Following are two examples for illustration. 
In our ﬁrst example we compare image search results gen- erated by two popular search engines, Bing and Google. We submitted 29 text queries to them, and collected the images they returned1. Fig. 1(a) gives the AP@40 (av- erage precision, ref Section 5.1 for details) for each query of the two search engines and the overall performance MAP (mean AP over all queries). We ﬁnd that although Bing and Google have comparable MAP values (52.24 and 52.36 re- spectively), their performance on individual queries is quite diﬀerent. Google achieves better performance on about half the queries. For example, on the query “White House”, the top images returned by Google are more related to the “White House” than those returned by Bing (Fig. 1(b)). If 
1The data was collected in 2008. 
363 

========1========

Table 1: MAP@40 (×100) of Bing, Google, and after optimal search engine selection for each query. 
MAP@40 
Bing 52.24 
Google 52.36 
SelectOpt 60.71 
Table 2: MAP@20 (×100) of the text-based search (Text) and the two reranking methods, PRF and BR. 
MAP@20 
Gain 
Text 50.28 
- 
PRF 60.65 20.62% 
BR 63.64 26.57% 
an algorithm could automatically determine which search engine would generate a better result list for each query, one could achieve better performance by selecting the opti- mal search engine for each query. Table 1 shows the MAP value after this selection, which is 60.71, about 16% relative improvement over Bing and Google. 
In our second example we compare the performance of text-based image search and visual reranking. Most exist- ing image search engines are implemented by indexing and searching textual information associated with images, e.g., surrounding text, URLs. The text-based image search ap- proach is eﬃcient for large scale image databases. How- ever, it suﬀers when the associated text is incapable of ade- quately describing the image. To address this diﬃculty, vi- sual reranking has been developed to reﬁne the text search results by incorporating visual information from images. Re- cent research has shown that visual reranking can gener- ally improve the performance of text-based image search to some extent [19, 22]. However, it is not guaranteed to ben- eﬁt every query. It is widely observed that visual reranking can greatly improve retrieval performance for some queries, while for others reranking can even degrade the performance of the initial text-based search. 
As an illustration, we apply two popular reranking meth- ods, BR [19] and PRF [21], on a public image search dataset (Web353). This dataset was collected by Krapac et al. [13]. It contains 71478 images returned by a Web search engine for 353 general textual queries. Table 2 presents the aver- age performance of the text-based search engine (Text) and the performance of the two reranking methods, in terms of MAP@20 over 353 queries. Table 3 lists the number of queries with improved, degraded, or equivalent performance after reranking. We ﬁnd that, although overall performance of all 353 queries is improved, there are still around 100 queries (25 - 30 percent) that suﬀer performance decrease after reranking. By further investigating the reranking per- formance on each query, we ﬁnd that the performance of many queries has decreased signiﬁcantly. For some queries, the decrease in AP value is as great as 0.7. Thus, given a query, it becomes crucial for the search engine to predict its visual reranking performance and decide whether the vi- sual reranking process should be performed or not. Doing so would allow us to avoid presenting reranking results which are even worse than text-based search results to users. The above two examples raise the same problem: for a query, given a set of result lists, which one is the best (has the highest retrieval performance)? In other words, which result list should be presented to users? This paper aims to solve this problem: given a set of search result lists returned by multiple search executions of a query, how can we design an algorithm to automatically compare the quality of those 
Table 3: The number of queries with improved, de- graded, or equivalent performance after reranking. 
queries PRF vs. Text BR vs. Text 
Improved 
237 
256 
Unchanged 
6 
10 
Degraded 
110 
87 
result lists in order to identify the result list with the high- est performance. To solve this problem, we build a model to investigate the quality of search results using machine learn- ing. It consists of two stages: training and testing. In the training stage we explore the visual distribution character- istics of good and bad search result lists and derive a set of light-weight features to capture their diﬀerences. Then, by forming the search result lists of training queries into prefer- ence pairs, we derive a preference learning model (PLM) by training on these pairs with RankSVM [12]. Finally, in the testing stage, the developed PLM is applied to predict the preference score for search result lists of any testing query. 
To the best of our knowledge this is the ﬁrst attempt that automatically evaluates the quality of Web image search re- sult lists. The proposed approach has a wide range of ap- plications. For example, it is capable of selecting the best search engine to solve the problem in example 1 and au- tomatically determining whether reranking can beneﬁt the query to solve the problem in example 2. There are also many other promising potential applications. For example, given diﬀerent search algorithm settings (various visual fea- tures, visual reranking methods, etc.), our approach can au- tomatically select the optimum settings for each query. 
The main contributions introduced in this paper are sum- marized as follows: 
• We quantitatively study and formulate the image search 
result preference learning problem. We propose a novel 
framework and a set of valuable features to automati- 
cally compare the quality of image search result lists. 
• Our proposed approach shows promising application 
potential for optimal search engine selection, merging 
of search result lists, and selecting the best visual fea- 
ture and reranking approach for each individual query. 
• Our work will explicitly guide the research in visual 
reranking ability estimation and provide a path for 
query diﬃculty modeling. 
2. RELATED WORK 
Image search plays an important role in our daily lives. Considerable research has been proposed to improve image search from various aspects, such as image annotation [2] and visual reranking [19, 21, 22]. All these research eﬀorts have the same objective of returning good image search re- sults to users. Diﬀerent search result lists are generated by diﬀerent image search methods and their performance on each query varies greatly. These works show their strength on certain aspects. There is no single method which can always work the best for all queries. Therefore, in addition to developing (overall) eﬀective search approaches, it is also very important to select the most suitable search method for each query. Through this selection, better image search re- sults can be derived. This paper conducts this best method selection for each query by investigating the quality of the image search result lists generated by diﬀerent search meth- ods. The search result list with the highest performance is picked out and presented to users. 
364 

========2========

The most related work to this paper is the query diﬃculty prediction. Query diﬃculty prediction in document retrieval has been explored for many years [3, 7, 8, 9, 11, 14, 23]. It aims to predict whether a query will have a high retrieval performance in a document collection. It includes two cate- gories, pre-retrieval prediction and post-retrieval prediction. In pre-retrieval, query diﬃculty prediction attempts to eval- uate search performance before the retrieval step [8, 9, 14]. It mainly relies on statistics of query terms over document collections. He and Ounis [8] proposed several pre-retrieval predictors by considering the intrinsic statistical features of queries, including query length, standard deviation of the in- verse document frequency (idf) of terms in the query, query scope, and a simpliﬁed clarity score (SCS). Kwok et al. [14] employed support vector regression to train a query diﬃculty prediction model with simple features such as log document frequency and query term frequency. Imran and Sharan [9] proposed two pre-retrieval query diﬃculty predictors based on the co-occurrence information among query terms. They assumed that higher co-occurrence of query terms means more information is conveyed, which leads to an easier query or a lower query diﬃculty level. 
In post-retrieval prediction, the retrieval step is conducted ﬁrst and query diﬃculty prediction evaluates the perfor- mance of the returned results. Our work is most related to it. In [3], a clarity score was proposed that measures the ambi- guity of a query through the Kullback-Leibler divergence be- tween the language models created from top-retrieved doc- uments and all documents in the collection. Hauﬀ et al. [7] proposed an improved clarity score to solve the parameter sensitivity problem of the previous one. Elad Yom-Tov et al. [23] estimated the quality of search result lists by measur- ing the agreement between the top results returned by the full query and its sub-queries. Jensen et al. [11] predicted query diﬃculty by using features extracted from surrogate documents represented in the search result list to train a re- gression model. In image retrieval, little research has been conducted on query diﬃculty estimation. Xing et al. [20] used textual features to predict whether a query is diﬃcult to be represented by images or not. This work does not in- vestigate the image search performance, but only classiﬁes the queries into two categories “easy” or “hard”. 
Query diﬃculty prediction estimates the performance of a search result list for a given query. Unlike query diﬃculty prediction, our work targets comparing several search result lists generated for a particular query. Instead of predict- ing their exact performance, we only need to know which search result list is better than the others. Furthermore, in query diﬃculty prediction, the search result lists are inde- pendent of each other since they are generated for diﬀerent queries. In our problem, the compared search result lists are generated for the same query and are thus correlated. We can utilize the correlation between them. Additionally, both the query and documents in the query diﬃculty prediction problem are in the textual domain. In our problem queries are textual and images are visual, creating a more complex problem. Our image search result performance comparison problem faces many challenges. As a ﬁrst attempt, this pa- per only focuses on exploiting visual information, which is the essential description of images. In the case where textual information of images (URL, surrounding texts et al.)isalso available, we will further exploit the joint usage of textual and visual information for this problem in the future. 
365 
3. PROBLEM FORMULATION 
For query q and an image collection {x1,···,xN},multi- ple search result lists can be derived using diﬀerent search algorithms. Each search result list is a permutation/ranking of the N images sorted in descending order by their ranking scores, which are generated by the search algorithm . We use ranking list variable l to denote a search result list. Assum- ing there are nq ranking lists generated for query q,they(q) 
constitute a set of search result lists L = {l1,···,lnq}. Our objective is to automatically determine which l in L(q) has the highest performance, 
l∗ =argmaxl∈L(q) y(l), (1) where y(l) denotes the performance of l. y(l)canbemea- sured by commonly used information retrieval measures, such as precision, recall, Average Precision (AP) [1] and Normalized Discounted Cumulated Gain (NDCG) [10]. 
For two ranking lists, the one with more relevant images ranked at the top gives a better performance than the one with fewer relevant images ranked at the top. If we have the ground truth label of each image (its relevance to query q), then y(l) can be derived by using AP or NDCG and the best search result selection in problem (1) is straight forward. However, in real applications, the ground truth relevance labels for images are unavailable. In this situa- tion, how can we know which ranking list performs better? In this paper, we propose to solve this problem via machine learning. Speciﬁcally, we want to learn a preference model f(l)=wTψ(l) from a training set, wherew is the weight- ing coeﬃcient vector and ψ(l) is a vector which reﬂects the characteristics of l. This model should satisfy the following constraints on the training set 
∀(li,lj),if y(li) >y(lj), then f(li) >f(lj). (2) For two ranking lists li and lj in training set, if the ground truth performance y(li) is better than y(lj)(li is preferred to lj), f(li) should be larger than f(lj). In other words, the ordinal relationship of pair (f(li),f(lj)) must be consis- tent with that of (y(li),y(lj)) , to reﬂect the ground truth preference of two ranking lists. 
In this paper we formulate the learning problem of f(·) by using the powerful RankSVM [12] algorithm. It mini- mizes the prediction errors on a set of training queries Q = {q(1),···,q(m)}, 
 
min 
1wT 
2 
w + C ξijk (3) 
s.t. ∀k, k =1,···,m. ∀(li,lj)∈S(q(k)), 
wTψ(li)≥wTψ(lj)+1− ξijk,ξijk ≥ 0 
where ξ is the slack variable and C>0 controls the trade- oﬀ between model complexity and training errors. S(q) is the set of preference ranking list pairs for query qgenerated from the ranking list set L(q) = {l1,···,lnq} 
S(q) = {(li,lj)|y(li) >y(lj);i, j =1,···,nq}. (4) 
The preference learning model f(·) can be derived by solv- ing problem (3). Then, this model can be applied to any testing query q for which ground truth relevance labels are unavailable. Suppose there are nq ranking lists generated 
for this query, L(q 
) 
= {l1,···,lnq}. f(·)canpredicta value for each list. For any two ranking lists li and lj,if f(li)>f(lj),weknowthatli performs better thanlj,and vice versa. The ranking list with the highest prediction value is the one which has the best performance. 

========3========

Ratio8 
dense 
7 
6 
5 
4 
3 
2 
1 
0 
1 
12 
23 
34 
45 
56 
67 
78 
89 
100 
111 
122 
133 
144 
155 
166 
177 
188 
199 
210 
221 
232 
243 
254 
265 
276 
287 
298 
309 
320 
331 
342 
353 
Figure 2: Sorted Ratiodense values in 353 queries. 
4. FEATURE CONSTRUCTION 
A crucial factor in f(l)=wTψ(l)isthevectorψ(l). It is not trivial to design a feature vector to capture visual characteristics of an arbitrary ranking list l. By analyzing the visual distribution of images in the collection, we propose a set of lightweight features. 
4.1 Two Basic Assumptions 
Given two ranking lists returned for query q over image collection {x1,···,xN}, the key is to investigate the visual diﬀerence between relevant and irrelevant images. The rel- ative feature vector ψ(l) discussed in this paper is designed based on the following two basic assumptions: 
• Density Assumption: Relevant images have higher den- 
sity than irrelevant images; 
• Visual Similarity Assumption: Relevant-relevant im- 
age pairs share higher visual similarity than relevant- 
irrelevant and irrelevant-irrelevant image pairs. 
4.1.1 Density Assumption 
Our density assumption is that relevant images have higher density than irrelevant images. To verify whether this as- sumption is true or not, we calculate the density of each of the N images in query q and then analyze their statistic characteristics. The density pxi for image xi is calculated via Kernel Density Estimation (KDE)[18], 
pxi = 
1 
|N(xi)| 
k(xi − xj), (5) 
xj∈N(xi) 
where N(xi) is the set of neighbors of image xi among 
Ratio8 
sim1 
7 
6 
5 
4 
3 
2 
1 
0 
1 
12 
23 
34 
45 
56 
67 
78 
89 
100 
111 
122 
133 
144 
155 
166 
177 
188 
199 
210 
221 
232 
243 
254 
265 
276 
287 
298 
309 
320 
331 
342 
353 
(a) 
15 
Ratiosim2 14 13 12 11 10 9 8 7 6 5 4 3 2 1 0 
1 
12 
23 
34 
45 
56 
67 
78 
89 
100 
111 
122 
133 
144 
155 
166 
177 
188 
199 
210 
221 
232 
243 
254 
265 
276 
287 
298 
309 
320 
331 
342 
353 
(b) 
Figure 3: Sorted Ratiosim1 (a) and Ratiosim2 (b) on 353 queries. 
From Fig. 2, we see that, among 353 queries, there are 329 queries whose average density of relevant images is larger than the average density of irrelevant images (Ratiodense > 1). To verify whether AvgDense+ is signiﬁcantly larger than AvgDense−, we further perform a statistical signiﬁcance test. We used the T-test with a 5% level of signiﬁcance. The T-test result shows that in 286 queries the average density of relevant images is signiﬁcantly larger than the average den- sity of irrelevant images. This phenomenon demonstrates that the density assumption holds for most queries. 
4.1.2 Visual Similarity Assumption 
Our visual similarity assumption is that relevant-relevant image pairs share higher visual similarity than relevant-irrelevant and irrelevant-irrelevant image pairs. For query q,wecal- culate the visual similarity sim(xi,xj) for any image pair (xi,xj). There are various ways to calculate sim(xi,xj). We use the popular bag-of-visual words representation with 
both k(x) > 0and 
adopted in this paper andσ is empirically set as the average of pair-wise distances of all images. 
Without ambiguity, we use xi to denote both the image and its visual feature vector in this paper. Various visual features can be used in (5). In this paper we adopt the pop- ular visual bag-of-word image representation. More details will be introduced in Section 5.1. 
To show the density diﬀerence between relevant and irrel- evant images, we calculate the average density of all relevant images AvgDense+ and the average density of all irrelevant images AvgDense− in each query. They are calculated as, 
 
AvgDense+ =|X+| 
1 
pxi, (6) 
xi∈X+ 
 
AvgDense− =|X−| 
1 
pxi, (7) 
xi∈X− 
the N images and 
k(x) is a kernel function that satisﬁes 
intersection kernel [5]. 
k(x)d(x) = 1. The Gaussian kernel isTo verify the visual similarity assumption we calculate the 
where X+ is the set of all relevant images and X− is the set of all irrelevant images. Ratiodense is deﬁned as, Ratiodense = AvgDense+/AvgDense−. 
We compute Ratiodense for all 353 queries in Web353 and plot them in Fig. 2 by sorting them in descending order. 
366 
average similarity of relevant-relevant, relevant-irrelevant, and irrelevant-irrelevant image pairs for each query in the Web353 dataset. They are denoted as AvgSim++, AvgSim+−, and AvgSim−− respectively. We plot the sorted Ratiosim1 =AvgSim 
++ 
and Ratiosim2 = 
AvgSim++ 
AvgSim+− AvgSim 
in Fig. 3. It shows−− 
that, among 353 queries, there are more than 340 queries whose average similarity of relevant-relevant pairs is larger than the average similarity of relevant-irrelevant and irrelevant- irrelevant pairs. The statistical signiﬁcance test (T-test with a 5% level of signiﬁcance) reveals that AvgSim++ is signiﬁ- cantly larger than AvgSim+− in 347 queries, and AvgSim++ is signiﬁcantly larger than AvgSim−− in 339 queries. This proves the validity of our visual similarity assumption. 
Due to the well-known semantic gap problem, some queries (especially the queries with large intra-class appearance vari- ance) are hard to be well represented by descriptive visual features. That is the reason why our two assumptions fail for some queries, as shown in Figs. 2 and 3. By further investi- gating the two assumptions on each query, we ﬁnd that the assumptions are valid for the queries with small appearance variance, such as “pantheon rome”, “ﬂag Italy”, “mona lisa”, 

========4========

Relevant 
l1 rank 1 
l2 
Irrelevant 
l1: 
1-3 4-6 
HL 
1-3 
H 
L 
1-3 4-6 part1 part2 
2 
4-6 
L 
L 
3 
l2: 
1-3 4-6 
4 
5 
LH 1-3 4-6 part1 part2 
1-3 
L 
L 
4-6 
L 
H 
6 
Density Vector p 
Similarity matrix M 
Figure 4: Illustration of density and similarity distribu- tion diﬀerence between two ranking lists. 
“log NBA”, etc. They are likely to fail for the queries with large appearance variance, such as “ﬂower”, “dog”, etc.Al- though the assumptions fail for some queries, they are valid for a majority of queries ( Figs. 2 and 3). Therefore, it is reasonable to apply them in our method. Our experimental results reported in Section 5 also validate this. 
4.2 Preference Learning Feature Extraction 
Inspired by the above two assumptions, we propose a set of related features by mining the distribution of density and visual similarity in l. We demonstrate it by using a toy example for illustration, as shown in Fig. 4. Suppose there are 6 images returned for query q, 3 relevant (denoted by square) and 3 irrelevant (denoted by circle). Given the two ranking lists l1 and l2, obviously the performance of l1 is better than l2, i.e. y(l1) >y(l2). According to the two assumptions, for the better ranking list l1, its top ranked images should have high (H) density and share high visual similarity while bottom ranked images should have low (L) density and low visual similarity. This density and similarity distribution diﬀerence between the two ranking lists can be utilized for extracting preference learning related features. 4.2.1 Similarity Distribution Feature 
For query q, given a ranking result l, a visual similarity matrix M ∈ RN×N can be obtained by calculating pair- wise image similarity. The (i, j) element mij in M denotes the visual similarity between the i-th ranked image and j-th ranked image. We split the N images into k groups along their ranks equally. As a consequence, the N × N simi- larity matrix M is split into k × k grids, as shown in Fig. 5. Then, we analyze the sub similarity matrix in diagonal blocks. Speciﬁcally, we calculate the mean and variance of similarities in each block to derive the similarity distribution feature vector FSD: 
FSD(i)=[mean(M(i,i)),var(M(i,i))],i=1,···,k. (8) where M(i,i) is the sub similarity matrix in block Bii. Then, 
Ranking list l 
Similarity Matrix M 
rank 
1 
2 
3 
part 1 
12… 
k 
1 
B11 
2 
2 
B22 
… 
… 
… 
… 
... 
... 
N-1 N 
k 
k 
Bkk 
Figure 5: The N images are split into k parts along their ranks equally. Therefore, the N ×N similarity matrix M is split into k × k blocks. 
density of the i-th ranked image in l as deﬁned in (5). We also split the N images into k groups and calculate the mean and variance of the density of the images in each part, 
FDD(i)=[mean(p(i)),var(p(i))],i=1,···,k. (9) where p(i) is the sub density vector for images in part i. By concatenating FDD(i), i =1,···,k , we can get a 2k- dimensional density distribution feature vector FDD. 
4.2.3 Feature from Top-T Ranked Images 
Both FSD and FDD roughly capture the overall density and visual similarity distribution of all N images in l (mean and variance). The following features are designed to exploit them in ﬁne granularity as a complementation. Especially in the case when users only focus on the performance of images ranked in the ﬁrst several pages. Therefore, we propose a histogram of density and visual similarity to elaborately analyze the top-T ranked images in l. 
Speciﬁcally, the density value is in range [0, 1] and we equally divide it into C-bins. Then, the densities of top-T ranked images {p1, p2, ···, pT} can be quantiﬁed into a C-bin histogram by mapping them into the corresponding bins. We denote this density histogram feature as FHD, 
FHD(c)= 
1 
T 
|{i|i =1,···,T,pi ∈c-th bin}|, (10) where c =1,···,C . 
Similarly, we can get a C-bin visual similarity histogram FHS by mapping the T ×T similarity matrix of the top-T ranked images into C-bins, 
FHS(c)= 
1 
T2|{(i, j)|i, j 
=1,···,T,mij ∈ c-th bin}|, (11) where c =1,···,C . 
Given FSD, FDD, FHD,andFHS, the ﬁnal preference learning feature vector ψ(l) can be derived by concatenating these four individual features. 
is derived. The intuition behind this feature is that, for a good ranking result list, more relevant images have higher ranks. In other words, images belonging to the top part may share higher similarity than those in other parts. 
4.2.2 Density Distribution Feature 
Similar to the visual similarity distribution feature, we also propose a density distribution feature based on the den- sity assumption. For the N images {x1,···,xN},wecanT 
derive a density vector p =[p1,···,pN] ,wherepi is the 
a2k-dimensional similarity distribution feature vectorF 
4.3 Training Sample EnlargementSD 
We have shown how to extract the preference learning 
feature vector ψ(l) for a ranking list l. We then build the preference learning model by training RankSVM on a set of queries {q(1),···,q 
(m)}, as presented in problem (3). In the training set, for each query, there are usually only a few ranking lists in L(q), which may cause the small (insuﬃcient) sample problem. For example, in our example 1 in Section 1, there are only two ranking lists (one from Bing and the other from Google). To solve this problem, we can construct additional ranking lists to enlarge the training set. 
367 

========5========

We can manually create ranking list lmanual for each query by permutating the N images in query q according to certain rules. Then, lmanual is added into the ranking list set L(q): 
L(q) ←L(q) ∪{lmanual} (12) In this paper, we create three manual ranking lists for each query, including: 
1) Perfect ranking list: order all relevant images at the top and all irrelevant images at the bottom; 
2) Worst ranking list: order all irrelevant images at the top and all relevant images at the bottom; 
3) Random ranking list: permutate the images randomly. Our experiments show that this training set enlargement works well for solving the small training sample problem. 
5. EXPERIMENTS 
5.1 Reranking Ability Assessment 
In this section, we investigate the eﬀectiveness of the pro- posed preference learning model (PLM) by applying it to reranking ability assessment. In reranking, each query q has two ranking lists: lText generated by text-based search en- gine and lrerank generated by the reranking process. The∗ 
reranking ability t 
q 
is deﬁned as the performance improve- ment of reranking over text-based search, t∗q = y(lrerank) − y(lText). The reranking ability measures to what degree reranking can improve text-based search results. For a query, if its reranking ability is positive (suitable to be reranked), the reranking result list will be presented to users; other- wise the text-based search result list will be presented. In other words, the search engine can achieve guaranteed per- formance enhancement by only reranking queries which are suitable for reranking while leaving the remaining unsuit- able ones unchanged. With this motivation, we apply PLM to assess reranking ability. Speciﬁcally, with the model f(·), PLM can predict a value for lText and lrerank respectively. The prediction diﬀerence f(lrerank)−f(lText)isusedtoap-∗ 
proximate the ground truth reranking ability t 
q. 
Dataset: In order to demonstrate the capacity of PLM for reranking ability assessment, we conduct experiments on a large public web image search dataset “Web353”, collected by Krapac et al. [13]. This dataset consists of 71478 im- ages returned by the French search engine Exalead 
2 
for 353 search queries, which were sampled from the most fre- quent terms searched by Exalead users. These 353 queries are very diverse and cover a broad range of topics, including landmark, design (painting, map, logo, ﬂag), people (movie, sports, singer star), object (vehicle, instrument, building, sports tool), and others (animal, plant, product, place, event, abstract word). Queries are somewhat evenly distributed across these topics. For each query, there are about 200 im- ages returned by Exalead. The ground-truth relevance label for each image is given a binary value: “relevant” or “irrel- evant”. In this dataset, there are 43.86% images labeled as relevant. For each query, we conduct BR (Bayesian Rerank- ing) [19] to generate its reranking result lrerank. 
Ranking List Performance y(l): For query q,givena ranking result list l, its ground truth performance y(l)is measured via non-interpolated average precision (AP) [1], which is widely used in information retrieval. AP is the mean of the precision values obtained when each relevant image occurs. The AP of top-T ranked images is deﬁned as 
2http://www.exalead.com/search/image 
1 
 
AP@T = 
T 
Z 
[precison(i) × rel(i)] (13)T 
i=1 
where precison(i) is the precision of top-i ranked images and rel(i) is a binary function denoting the relevance of the ranked image with “1” for relevant and “0” for irrelevant. ZT is a normalization constant which is chosen to guarantee AP@T = 1 for a perfect ranking result list. 
Model training: We use the leave-one-out method for PLM training. At each step we train the model on 352 queries and test this model on the leftover query. We repeat the process 353 times to ensure that each query has been used as test query at least once. 
Image visual representation: The density and visual similarity features described in Section 4 are calculated based on visual representation of images. In this paper we use a bag-of-visual word histogram to visually represent an image. Scale-invariant feature transform (SIFT) [16] local descrip- tors are extracted from each image on a dense grid. Then, a codebook is generated by clustering all local descriptors into 1000 groups [5]. By quantizing local descriptors into vi- sual words, each image is represented as a 1000-dimensional histogram. Spatial pyramid matching [15] is used to encode spatial information. Calculating the similarity between two histograms is done using an intersection kernel. 
Evaluation: For each query q, there are two ranking lists: lText and lrerank. The query’s ground truth reranking abil-∗ 
ity is t 
q 
= y(lrerank) − y(lText), and the reranking ability estimated by the learned PLM is tq = f(lrerank)− f(lText). We evaluate PLM from the following two aspects. 
1. Prediction Accuracy (AC): 
AC = 
Correctly predicted queries 
Total queries 
(14) Correctly predicted queries are those which satisfy t∗qtq > 0. AC examines whether PLM can correctly predict the binary relationship (improved or not) between the result lists of reranking and text-based search. In addition to this overall accuracy, we also examine the prediction accuracy P+, P− of positive and negative queries. A positive (negative) query is one in which reranking performs better (worse) than the text-based search, i.e., t∗q > 0(t∗q < 0). P+ and P− are deﬁned as: 
∗ 
P+ = 
Correctly predicted positive queries(tq > 0andtq > 0) 
Total positive queries 
Correctly predicted negative queries(tq 
∗ 
P− = 
< 0andtq < 0) 
Total negative queries 
We examine P+ and P− because we want to investigate the model’s capacity for negative query detection as well as the percentage of sacriﬁced positive queries. 
2. Correlation Coeﬃcient: Accuracy only measures the binary prediction of reranking ability, i.e.,improvedornot after reranking. To further verify the eﬀectiveness of PLM in terms of reranking ability degree prediction, we check the consistency between the ground truth reranking ability vec- tor t∗ =[t∗q(1),···,t∗q(353)]T and the one predicted by PLM t =[tq(1),···,tq(353)]T. As widely used in query diﬃculty prediction [11, 23], we calculate the Kendall’s τ rank corre- lation coeﬃcient between t∗ and t. Kendall’s τ is deﬁned as τ = 
nc−nd 
nc+n 
,wherend c and nd are the numbers of con- cordant and discordant pairs respectively. A pair of two queries (q(i),q(j)) is concordant if the orders of (t∗q(i),t∗q(j)) and (tq(i),tq(j)) agree. Kendall’s τ value falls within the range [-1, 1], where -1 means perfect negative correlation, 1 
368 

========6========

Table 4: Correlation coeﬃcients and accuracy in rerank- ing ability assessment. 
Kendall’s τ 
AC(%) 
P+(%) 
P-(%) 
T=20 
QD PLM 
0.0543 0.3654 
57.51 75.64 
67.97 92.19 
33.33 35.63 
T=40 
QD PLM 
0.1485 0.4414 
65.44 78.75 
77.44 91.35 
30.49 42.68 
T=60 
QD PLM 
0.0826 0.4782 
65.44 82.44 
77.09 92.73 
26.03 49.32 
T=80 
QD PLM 
0.1244 0.4820 
68.84 80.45 
81.09 89.82 
27.40 50.68 
T=100 
QD PLM 
0.1400 0.4659 
70.82 80.74 
82.44 89.25 
28.99 52.17 
Table 5: MAP (×100) comparison in reranking ability assessment. MAP is the mean of AP over all queries. 
Text 
BR 
SelectQD 
SelectPLM 
SelectOpt 
MAP@20 MAP@40 MAP@60 MAP@80 MAP@100 
50.28 45.24 43.06 42.60 43.08 
63.64 57.37 54.35 52.90 52.99 
59.65 55.29 51.89 51.24 51.65 
64.32 57.97 55.02 53.45 53.38 
66.80 59.40 55.96 54.32 54.24 
means perfect positive correlation, and 0 means t∗ and t are independent to each other. 
Reranking Ability Assessment: We evaluate PLM at 5 truncation levels, i.e., y(l)=AP@T,T = {20,40,60,80,100}. We implement the document query diﬃculty method pro- posed in [11] as a baseline, since this method conducts query diﬃculty prediction through supervised model training. Based on [11], we extract textual features for each image from its associated textual information (URL, surrounding text, etc.) and train a regression model. The reranking ability is de- noted as the query diﬃculty diﬀerence between the two rank- ing lists (lText and lrerank). We note the method in [11] as QD. Table 4 shows the Kendall’s τ correlation coeﬃcients and accuracy of our approach and the baseline QD. It reveals that our method outperforms QD in both correlation coef- ﬁcients and accuracy. By further investigating P+ and P-, we conclude that PLM removes about half of the negative queries while keeping most positive queries. For example, in T=80, PLM detects 50.68% of the negative queries, pre- venting performance decrease, while sacriﬁcing performance gain on only 10.18% of the positive queries. 
With the predicted reranking ability, we can choose to exe- cute reranking only on those queries whose reranking ability is positive. This operation can prevent large performance decreases on some queries, possibly improving the user ex- perience. The reranking process selection for each query can also lead to a better overall performance (mean AP over all queries, MAP). We select a better one from lText and lRerank for each query via QD (SelectQD)andPLM (SelectPLM.) Table 5 lists the MAP values for text-based search (Text), reranking (BR), SelectQD, and SelectPLM. Column SelectOpt shows the maximal MAP by selecting the best search result list between Text and BR for each query according to their ground truth performance, which gives the upper bound of the MAP value we can achieve. Table 5 shows that SelectPLM performs better than both Text and 
Table 6: MAP (×100) comparison in reranking ability assessment for the queries which do not satisfy the as- sumptions. 
Text 
BR 
SelectPLM 
SelectOpt 
MAP@20 MAP@40 MAP@60 MAP@80 MAP@100 
41.00 36.57 34.58 35.32 36.38 
41.29 36.91 35.38 36.15 37.90 
40.78 37.20 35.46 36.16 37.50 
47.51 41.09 38.37 38.92 40.19 
Table 7: MAP (×100) comparison in reranking ability assessment for each of the 5 query categories (T=60). 
Text 
BR 
SelectPLM 
SelectOpt 
landmark design people object others 
50.92 41.94 47.08 33.66 40.38 
62.44 57.83 61.61 35.33 50.70 
63.75 58.59 61.59 36.64 51.31 
64.88 59.00 62.08 39.00 52.10 
BR, while SelectQD only achieves a moderate performance betweenTextandBR.ThereasonwhytheQDmethodin [11] does not work well here is that textual features in im- age retrieval are not the essential descriptions for the images, therefore more noise (e.g., mismatching between surround- ing text and image content) may be introduced. 
As we discussed in Section 4.1, the assumptions are not always valid for all queries. For those queries which do not satisfy the assumptions, the extracted preference learning features may be noisy. Consequently the preference predic- tion may be unreliable. To investigate the implication of the failures of the assumptions on the prediction results, we examined the performance of the proposed PLM on the 67 queries which do not satisfy either the density assumption or the visual similarity assumption. The experimental results show that our approach still achieves (50±4.9)% prediction accuracy (AC over T =20,40,60,80,100) in reranking abil- ity assessment, which is close to random prediction. And the SelectPLM is comparable to Text and BR, as shown in Table 6. It reveals that our method does not worsen the search engine’s performance even on the queries which do not satisfy the assumptions. 
To further investigate the eﬀectiveness of PLM for diﬀer- ent categories of queries, the 353 queries are grouped into 5 categories: landmark (53), design (60), people (98), ob- ject (54) and others (88). We conducted experiments on each of the 5 categories. The experimental results show that our method works well on all query categories. The predic- tion accuracy (ACs) are 86.79%, 95.00%, 87.76%, 61.11%, 78.41% respectively. Even in the category “object”, of which queries usually have images with a large visual appearance variance, moderate AC (61.11%) is obtained and the MAP value of SelectPLM is better than both Text and BR, as shown in Table 7. Better performance is achieved in “land- mark” and “design” since the images of those categories are more visually consistent. For “people”, high AC is obtained, but the SelectPLM is very close to BR. The reason is that BR already improves most of queries in this category a lot, hence the improvement space of PLM over BR is limited. 
Reranking Feature Selection:PLMcanalsobeap- plied to select optimal reranking features. The reranking 
369 

========7========

Table 8: MAP (×100) comparison in reranking feature selection from {Text, BRSIFT and BRCF} 
Text 
BRSIFT 
BRCF 
MAP@20 MAP@40 MAP@60 MAP@80 MAP@100 
50.28 45.24 43.06 42.60 43.08 
63.64 57.37 54.35 52.90 52.99 
60.21 53.99 50.65 49.20 49.08 
SelectRandom 
SelectPLM 
SelectOpt 
58.04 52.20 49.35 48.23 48.38 
64.74 58.44 55.17 53.85 53.67 
69.13 61.35 57.34 55.58 55.36 
Table 9: MAP (×100) comparison in reranking algorithm selection from {Text, BRSIFT and PRFSIFT}. 
Text 
BRSIFT 
PRFSIFT 
MAP@20 MAP@40 MAP@60 MAP@80 MAP@100 
50.28 45.24 43.06 42.60 43.08 
63.64 57.37 54.35 52.90 52.99 
60.65 54.75 51.97 50.68 50.57 
(%) 70 
Best 
Middle 
Worst 
60 
50 
40 
30 
20 
10 
0 
20 40 60 80 100 
Figure 6: 
T 
Reranking feature selection. Percentages of queries for which PLM selects the Best/Middle/Worst ranking list. 
(%) 70 
Best 
Middle 
Worst 
60 
50 
40 
30 
20 
10 
0 
20 40 60 80 100 
T 
Figure 7: Reranking algorithm selection. Percentages of queries for which PLM selects the Best/Middle/Worst ranking list. 
feature is the visual feature used in the reranking process which has a great inﬂuence on reranking performance. We use two diﬀerent visual features for BR reranking and ob- tain their corresponding ranking lists. One feature is the aforementioned SIFT based bag-of-visual word histogram, denoted as SIFT. The other one is a combination of several low-level features adopted in [4], denoted as CF. 
With two ranking lists generated by BRSIFT and BRCF, as well as the text-based search result list (Text), we ap- ply PLM to select the best result list for each individual query. Table 8 gives the MAP comparison between their in- dividual performances, as well as the performance after se- lection. We compare PLM with random selection. Without prior knowledge, random selection just selects from the three ranking lists randomly. It shows that our model SelectPLM not only outperforms random selection SelectRandom, but also outperforms all three individual ranking methods. In Fig. 6, we further give the % of the Best/Middle/Worst selection queries, which shows the percent of all queries for which our method chose each one of the three ranking lists. Fig. 6 clearly demonstrates that, for most queries (60%- 70%), PLM selects the Best ranking list from {Text, BRSIFT, BRCF}. This is a substantial improvement over the random 
SelectRandom 
SelectPLM 
SelectOpt 
58.19 52.45 49.79 48.73 48.88 
63.76 57.79 55.07 53.39 53.62 
69.44 60.99 57.26 55.56 55.43 
(%) Best 2nd 3rd 4th Worst 
50 
45 
40 
35 
30 
25 
20 
15 
10 
5 
0 
20 40 60 80 100 
T 
Figure 8: Reranking feature and algorithm mixture selec- tion. Percentages of queries for which PLM selects the Best/2nd/3rd/4th/Worst ranking list. 
selection method, which chooses the Best ranking list for 33.33% of queries, and veriﬁes the eﬀectiveness of our model. 
Reranking Method Selection: Similar with reranking feature selection, PLM can also be applied to select the opti- mal reranking method for each query. This selection is con- ducted on reranking results generated by BR [19] and PRF [21]. The SIFT feature is used in both reranking methods. Table 9 and Fig. 7 give the MAP value comparison and % of the Best/Middel/Worst selection queries, respectively. It also demonstrates the eﬀectiveness of our method in select- ing better ranking method for each query. 
Reranking Feature and Algorithm Mixture Selec- tion: With two reranking features (SIFT and CF) and two reranking algorithms (BR and PRF), four reranking result lists are generated by their combination, i.e.,BRSIFT,PRFSIFT, BRCF and PRFCF. We further test PLM on this mixture selection. For each query, four reranking lists as well as the Text result list are ranked according to the value predicted by PLM. Table 10 gives the MAP value comparison We note an increase in performance after PLM selection, producing better results than all ﬁve basic methods and random se- lection. We also analyzed the number of queries for which PLM selects the i-th best ranking list (i =1,···,5;i =1 means the best and i = 5 means the worst), as shown in Fig. 8. It shows that PLM selects the Best/second best ranking list for about 40%/20% of the queries. It obviously outper- forms random selection, which would select each of the ﬁve ranking lists for about 20% of the queries. 
5.2 Search Engine Selection 
In this section we investigate the eﬀectiveness of the pro- posed method by applying it to optimal image search engine 
370 

========8========

Table 10: MAP (×100) comparison in reranking feature and algorithm mixture selection from {Text, BRSIFT,BRCF, 
PRFSIFT and PRFCF}. 
Text 
BRSIFT 
BRCF 
PRFSIFT 
MAP@20 MAP@40 MAP@60 MAP@80 MAP@100 
50.28 45.24 43.06 42.60 43.08 
63.64 57.37 54.35 52.90 52.99 
60.21 53.99 50.65 49.20 49.08 
60.65 54.75 51.97 50.68 50.57 
selection and search result merging. Speciﬁcally, each query q has two ranking lists generated by two search engines: Bing and Google. Our objective is to determine which search en- gine returns better performance for any query q. 
Dataset: A dataset was collected from two popular im- age search engines, Bing (Live) and Google. We selected 29 queries 
3 
from the top-1000 queries of Live Image Search and popular tags on Flickr. The 29 queries satisfy all the following three criteria: 1) Popularity: they are either top queries of Live Image Search or popular tags of Flickr; 2) Broad topic coverage: the 29 queries cover wide topics, e.g., animals, plants, scene, objects, etc.;3)Including both sim- 
PRFCF 
SelectRandom 
SelectPLM 
SelectOpt 
54.00 49.67 47.52 50.68 47.58 
57.76 52.20 49.51 49.21 48.66 
64.86 58.57 55.45 53.83 54.00 
72.26 63.68 59.45 57.57 57.29 
Table 11: Correlation coeﬃcients and Accuracy in search engine selection from {Bing, Google} 
T=20 
T=40 
T=60 
T=80 
T=100 
Kendall’s τ 
AC(%) 
0.1724 62.07 
0.1970 68.97 
0.1232 65.52 
0.1626 79.31 
0.2315 75.86 
2. Correlation coeﬃcient: Kendall’s τ correlation coeﬃ- cients between the ground truth performance diﬀerence vec- tor Δ∗ =[δ∗q(1),···,δ 
∗ T 
q(29)] 
and the one predicted by our PLM Δ =[δq(1),···,δq(29)]T. 
ple queries which normally consist of one term (e.g.,“Cat”, “Flower”, etc.) and compound queries which are reﬁned terms based on some certain attribute, e.g., color (“White Cat”), time (“White House Night”), emotion (“Funny Dog”), etc. We submitted each query to Bing and Google respec- tively, and collected the top 1000 images returned, resulting in 50566 total images. For each query, the returned im- ages are labeled as either “relevant” or “irrelevant”. In this dataset, there are 42.23% images labeled as relevant. 
To the best of our knowledge, there is no publicly available dataset which collects the search results of the same queries from several search engines and the queries are sampled from real query logs. To collect such a large query set is diﬃcult since it may need the collaboration between academia and industry and it will involve a large amount of eﬀorts in get- ting query logs from popular search engines , collecting the huge amount of data and employing human for labeling. As the ﬁrst try, we collected a moderate-size dataset with 50K images [22] with the collaboration with Bing search. In the future, with the continuing collaboration with industry, we plan to collect a larger dataset from several search engines. 
Experimental setting is the same as that in Section 5.1. We used the bag-of-visual words histogram for image repre- sentation and the leave-one-out method for model training. 
Evaluation: For each query q, there are two ranking lists: lBing and lGoogle. Each query’s ground truth performance∗ 
diﬀerence is denoted as δ 
q 
= y(lBing) − y(lGoogle), and the performance diﬀerence estimated by PLM is δq = f(lBing)− f(lGoogle). We also evaluate PLM’s preference prediction ability from the following two aspects: 
1. Prediction Accuracy deﬁned in (14). In this applica- tion, the correctly predicted queries are those which satisfy δ∗qδq > 0, i.e., the preference relationship between the two ranking lists is correctly predicted. 
ple and compound queries: the 29 queries contain both sim-Search Engine Selection: We also evaluate 5 diﬀer-ent 
Ts as in Section 5.1. Table 11 shows the correlation 
3Animal, Beach, Beijing Olympic 2008, Building, Car, Cat, Clouds, Earth, Flower, Fox, Funny Dog, George W. Bush, Grape, Hearts, Hello Kitty, Hiking, Mercedes Logo, Panda, Sky, Statue of Liberty, Sun, Trees, Wedding, White Cat, White House Night, White House, Winter, Yellow Rose, Zebra. 
371 
coeﬃcients and accuracy. It shows that moderate corre- 
lation coeﬃcients are achieved and the AC is more than 
70% when T=80 and 100. It demonstrates that PLM can 
choose the better search engine between Bing and Google for 
the majority of queries. Therefore, better performance will 
be achieved after this suitable search engine selection. The 
MAP values of Bing (TextBing), Google (TextGoogle)andthe 
one generated after our PLM Selection (SelectPLM) are given 
in Table 12. Column SelectOpt is the maximal MAP value 
arrived at by selecting the optimal search engine accord- 
ing to their ground truth. Table 12 shows that SelectPLM 
achieves consistent performance improvements over both TextBing 
and TextGoogle for all Ts. From Tables 11 and 12, we con- 
clude that the proposed PLM method can be successfully 
applied to optimal search engine selection. 
Search Results Merging: In search engine selection, 
for each query, we choose a better one between lBing and 
lGoogle. In addition to this binary selection, we can also 
merge the two result lists to get a new one. For query q, 
when we have no idea of the performance of the two search 
results, they may contribute equally to the ﬁnal merged re- 
sult list. If we have the prior knowledge of which one is 
better than the other, then higher merging weight can be 
assigned to this better one. Our PLM can serve this role by 
using the predicted δq to set appropriate merging weights. 
To complete this goal, the Δ is ﬁrst normalized into [-1, 
1]. We denote the normalized performance diﬀerence as δ˜q. 
Then, for query q, the merging weights for lBing and lGoogle 
are deﬁned as wBing = 
1 1 
2 
(1 + δ˜q)andwGoogle = (1 − δ˜2 q) 
respectively (wBing ≥ 0, wGoogle ≥ 0, wBing +wGoogle =1). 
To form the merged result list, we assign a merging score to 
each image in lBing and lGoogle. The merging score for the 
i-th ranked images in lBing is i × (1 − wBing). The merging 
score for the i-th ranked image in lGoogle is i×(1−wGoogle). 
The ﬁnal merged ranking list is derived by sorting all im- 
ages in lBing and lGoogle in ascending order of their merging 
scores. The performance of this weighted merging result 
is given in Table 12, comparing with the equal merging in 
which wBing = wGoogle =0.5. The search engine selection 

========9========

Table 12: MAP (×100) comparison in search engine selection and search results merging from {Bing, Google} 
TextBing 
TextGoogle 
SelectRandom 
MAP@20 MAP@40 MAP@60 MAP@80 MAP@100 
57.91 52.24 49.18 46.52 44.52 
64.26 52.36 44.35 39.41 36.20 
61.09 52.30 46.77 42.97 40.36 
discussed above is actually a hard merge of the two search results with weight either 1 or 0. Table 12 clearly demon- strates that merging by leveraging our preference prediction outperforms equal merging. 
6. CONCLUSION AND FUTURE WORK 
In this paper, we proposed a method to automatically compare a set of ranking result lists for a given query by mining their visual information. The method is formulated within the RankSVM framework and a set of lightweight features are designed to reﬂect the visual diﬀerence between two ranking lists. The proposed method is successfully ap- plied to reranking ability estimation and automatic search engine selection. Experimental results have demonstrated the eﬀectiveness of our approach and its promising applica- tions on reranking feature and model selection, as well as merging of image search results. 
Currently, our preference learning model is built based on visual features of images only, and their textual information is not considered. In the future, we plan to further exploit this ranking list performance comparison problem by inves- tigating both visual and textual features, to achieve better performance. Another direction of our future work is to ap- ply this model to more potential applications, e.g.,query suggestion, query language selection and so on. 
Acknowledgments This work was supported by Research Enhancement Program (REP) and start-up funding from the Texas State University, and was supported in part to Dr. Qi Tian by NSF IIS 1052851, Faculty Research Awards by Google, FXPAL and NEC Laboratories of America, re- spectively. 
7. REFERENCES 
[1] Trecvid video retrieval evaluation. 
hppt://www-nlpir.nist.gov/projects/trecvid/. [2] G. Carneiro, A. Chan, P. Moreno, and N. Vasconcelos. 
Supervised learning of semantic classes for image 
annotation and retrieval. PAMI, 29(3):394–410, 2007. [3] S. Cronen-Townsend, Y. Zhou, and W. B. Croft. 
Predicting query performance. ACM SIGIR, pages 
299–306, 2002. 
[4] J. Cui, F. Wen, and X. Tang. Real time google and 
live image search re-ranking. ACM Multimedia, pages 
729–732, 2008. 
[5] J. Deng, A. C. Berg, K. Li, and F.-F. Li. What does 
classifying more than 10,000 image categories tell us? 
ECCV, pages 71–84, 2010. 
[6] B. Geng, L. Yang, C. Xu, and X.-S. Hua. 
Content-aware ranking for visual search. CVPR, pages 
3400–3407, 2010. 
[7] C. Hauﬀ, V. Murdock, and R. Baeza-Yates. Improved 
query diﬃculty prediction for the web. ACM CIKM, 
pages 439–448, 2008. 
SelectPLM 
SelectOpt 
MergeEqual 
MergeWeight 
65.80 56.12 50.78 47.77 45.16 
71.51 60.71 54.63 50.64 48.18 
65.36 59.57 54.78 51.31 48.61 
67.21 59.80 55.85 52.76 49.83 
[8] B. He and I. Ounis. Inferring query performance using 
pre-retrieval predictors. SPIRE, pages 43–54, 2004. [9] H. Imran and A. Sharan. Co-occurrence based 
predictors for estimating query diﬃculty. IEEE ICDM 
Workshops, pages 867–874, 2010. 
[10] K. J¨arvelin and J. Kek¨al¨ainen. Cumulated gain-based 
evaluation of ir techniques. ACM Trans. Inf. Syst., 
20(4):422–446, 2002. 
[11] E. Jensen, S. Beitzel, D. Grossman, O. Frieder, and 
A. Chowdhury. Predicting query diﬃculty on the web 
by learning visual clues. SIGIR, pages 615–616, 2005. [12] T. Joachims. Optimizing search engines using 
clickthrough data. SIGKDD, pages 133–142, 2002. [13] J. Krapac, M. Allan, J. Verbeek, and F. Juried. 
Improving web image search results using query- 
relative classiﬁers. CVPR, pages 1094–1101, 2010. [14] K.-L. Kwok, L. Grunfeld, H. L. Sun, and P. Deng. 
Trec 2004 robust track experiments using pircs. 
TREC, 2004. 
[15] S. Lazebnik, C. Schmid, and J. Ponce. Beyond bags of 
features: Spatial pyramid matching for recognizing 
natural scene categories. CVPR, pages 2169–2178, 
2006. 
[16] D. G. Lowe. Distinctive image features from 
scale-invariant keypoints. IJCV, 60(2):91–110, 2004. [17] K. Min, L. Yang, J. Wright, L. Wu, X.-S. Hua, and 
Y. Ma. Compact projection: Simple and eﬃcient near 
neighbor search with practical memory requirements. 
CVPR, pages 3477–3484, 2010. 
[18] E. Parzen. On estimation of a probability density 
function and mode. The annals of mathematical 
statistics, 33(3):1065–1076, 1962. 
[19] X. Tian, L. Yang, J. Wang, Y. Yang, X. Wu, and 
X.-S. Hua. Bayesian video search reranking. ACM 
Multimedia, pages 131–140, 2008. 
[20] X. Xing, Y. Zhang, and M. Han. Query diﬃculty 
prediction for contextual image retrieval. ECIR,pages 
581–585, 2010. 
[21] R. Yan, A. G. Hauptmann, and R. Jin. Multimedia 
search with pseudo-relevance feedback. CIVR,pages 
238–247, 2003. 
[22] L. Yang and A. Hanjalic. Supervised reranking for web 
image search. ACM Multimedia, pages 183–192, 2010. [23] E. Yom-Tov, S. Fine, D. Carmel, and A. Darlow. 
Learning to estimate query diﬃculty: including 
applications to missing content detection and 
distributed information retrieval. ACM SIGIR,pages 
512–519, 2005. 
[24] Z.-J. Zha, L. Yang, T. Mei, M. Wang, and Z. Wang. 
Visual query suggestion. ACM Multimedia,pages 
15–24, 2009. 
372 

========10========

