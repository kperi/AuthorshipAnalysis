Enabling Fast Lazy Learning for Data Streams 
Peng Zhang 
†, Byron J. Gao †, Xingquan Zhu ‡, and Li Guo 
Institute of Computing Technology, Chinese Academy of Sciences, Beijing, 100190, China † 
Department of Computer Science, Texas State University, San Marcos, TX, 78666, USA 
‡ 
QCIS, Faculty of Eng. & IT, University of Technology, Sydney, NSW 2007, Australia 
zhangpeng@ict.ac.cn, bgao@txstate.edu, xqzhu@it.uts.edu.au, guoli@ict.ac.cn 
Abstract—Lazy learning, such as k-nearest neighbor learn- ing, has been widely applied to many applications. Known for well capturing data locality, lazy learning can be advantageous for highly dynamic and complex learning environments such as data streams. Yet its high memory consumption and low predic- tion eﬃciency have made it less favorable for stream oriented applications. Speciﬁcally, traditional lazy learning stores all the training data and the inductive process is deferred until a query appears, whereas in stream applications, data records ﬂow continuously in large volumes and the prediction of class labels needs to be made in a timely manner. In this paper, we provide a systematic solution that overcomes the memory and eﬃciency limitations and enables fast lazy learning for concept drifting data streams. In particular, we propose a novel Lazy-tree (L- tree for short) indexing structure that dynamically maintains compact high-level summaries of historical stream records. L-trees are M-Tree [5] like, height-balanced, and can help achieve great memory consumption reduction and sub-linear time complexity for prediction. Moreover, L-trees continuously absorb new stream records and discard outdated ones, so they can naturally adapt to the dynamically changing concepts in data streams for accurate prediction. Extensive experiments on real-world and synthetic data streams demonstrate the performance of our approach. 
Keywords-data stream mining, data stream classiﬁcation, Spatial indexing, lazy learning, concept drifting. 
I. INTRODUCTION 
Data stream classiﬁcation has drawn increasing attention from the data mining community in recent years with a vast amount of real-world applications. For example, in informa- tion security, data stream classiﬁcation plays an important role in real-time intrusion detection, spam detection, and malicious Web page detection. In these applications, stream data ﬂow continuously and rapidly, and the ultimate goal is to accurately predict the class label of each incoming stream record in a timely manner. 
Existing data stream classiﬁcation models, such as incre- mental learning [8] or ensemble learning models [12], [10], [19], [20], [21], all belong to the eager learning category [11]. Being eager, the training data are greedily compiled into a concise hypothesis (model) and then completely discarded. Examples of eager learning methods include decision trees, neural networks, and naive Bayes classiﬁers. Obviously, eager learning methods have low memory con- 
b1 
b1 
b2 
Time t1 
Time t2 
Time t3 
Figure 1. 
Lazy learning on concept drifting data streams. 
sumption and high predicting eﬃciency in answering queries, which are crucial for most data stream applications. 
Lazy learning [3], such as k-Nearest Neighbor (kNN) clas- siﬁers, represents some instance-based and non-parametric learning methods, where the training data are simply stored in memory and the inductive process is deferred until a query is given. Compared to eager methods, lazy learning methods incur none or low computational costs during training but much higher costs in answering queries also with greater storage requirements, not scaling well to large datasets. In stream applications, data streams come continuously in large volumes, making it impractical to store all the training records. In addition, stream applications are time- critical where class prediction needs to be made in a timely manner. Lazy learning methods fall short in meeting these requirements and have not been considered for data stream classiﬁcation. 
In fact, lazy learning has many characteristics that are promising for data stream classiﬁcation. While eager learn- ing strives to learn a single global model that is good on average, lazy learning well captures locality and can achieve high accuracy when the learning environment is complex and dynamic. In data stream applications, stream records cannot be fully observed at a speciﬁc time stamp and it is often diﬃcult to construct satisfactory global models based on partially observed training data. This diﬃculty is further aggravated by the inherent concept drifting problem in data streams, where the underlying patterns irregularly change over time resulting in much complicated decision boundary. A motivating example is shown in Example 1. 
Example 1: Fig. 1 shows a typical scenario for classiﬁ- cation in dynamic data streams. During the three continuous 

========1========

time stamps, the classiﬁcation boundary (concept) drifts from b1 at time t1 to b2 at time t3. We can observe that before and after the concept drifts i.e., at time t1 and time t3, both eager and lazy learning methods can perform equally well given the simplicity of the linear classiﬁcation boundaries. However, when concept drifting occurs at time t2, since only a small portion of examples in the red circle region are observed (the region represents emerging new concepts), the new boundary b2 cannot be fully constructed yet. While eager learners tend to consider the examples in the red circle as noise and still return b1 as the decision boundary, lazy learners can correctly capture partially formed new concepts in the red circle area. 
Motivated by the above observations, the goal of this study is to overcome the high memory consumption and low predicting eﬃciency limitations and enable lazy learning on data streams. We expect that the study can unleash the potential of lazy learning in data stream classiﬁcation and open up new possibilities for tackling the inherent problems and challenges in data stream applications. In particular, we propose a novel Lazy-tree (L-tree for short) indexing structure that dynamically maintains compact high-level summaries of historical stream records. L-tree is inspired by M-tree [5] that has been widely used as an indexing tool in metric spaces. When building an L-tree, historical stream records are condensed into compact high-level exemplars to reduce memory consumption. An exemplar is a sphere of certain size generalizing one or more stream examples. All the exemplars are organized into a height-balanced L-tree that can help achieve sub-linear predicting time. L-trees are associated with three key operations. The search operation traverses the L-tree to retrieve the k-nearest exemplars of an incoming stream record for classiﬁcation purposes. The insertion operation adds new stream records into some exemplars in the L-tree. The deletion operation removes outdated exemplars from the L-tree. The L-tree approach achieves a logarithmic predicting time with bounded mem- ory consumption. It also adapts quickly to new trends and patterns in stream data. 
This paper makes the following contributions: 
• We are the ﬁrst to systematically investigate lazy learn- 
ing on data streams. (Section II) 
• We propose a compact high-level structure exemplar 
to summarize stream examples and reduce memory 
consumption for lazy learning. (Section III) 
• We propose an L-tree structure to organize exemplars 
and achieve sub-linear time for prediction. L-trees 
continuously absorb new stream records and discard 
outdated ones, well adapting to drifting concepts and 
result in accurate prediction results. (Section IV) 
• We conduct extensive experiments on real-world data 
streams and demonstrate the performance gain of our 
approach compared to benchmark methods. (Section V) 
II. PROBLEM DESCRIPTION 
We study the problem of enabling lazy learning on data streams, for which we focus on signiﬁcantly reducing memory consumption and predicting time so that the critical requirements of stream applications can be satisﬁed. 
Consider a data stream S consisting of an inﬁnite se- quence of records {s1,· · ·, si,· · ·}, where si = (xi,yi) rep- resents a record arriving at time stamp ti. For each recordτ 
si, xi ∈ R represents an τ-dimensional attribute vector, and yi ∈ {c1,· · ·,cl} represents the class label. Assume that the current time stamp is tn, and the incoming record is denoted by sn = {xn,yn} with unknown yn. Our learning objective is to accurately predict yn as fast as possible. This is equivalent to maximizing the posterior probability in Eq. (1) with maximum eﬃciency. 
yn = argmaxc∈{c 
1,···,cl} 
P(c|xn, s1,· · ·, sn 
 1) (1) 
Without loss of generality, we consider k-NN as the underlying lazy learning algorithm. Then, instead of using all the (n  1) stream records to predict yn, we use only 
k nearest neighbors, denoted by {s 
1, 
· · ·, sk}, from all the (n  1) records for prediction. Note that k  (n  1), and {s1,· · ·, sk} ∈ {s1,· · ·, sn 
 1}. This way, the posterior probability in Eq. (1) can be revised to Eq. (2), 
yn = argmaxc∈{c 
1,···,cl} 
P(c, s 
 
1, 
· · ·, sk|xn, s1,· · ·, sn 
 1) (2) 
Decomposing the objective function in Eq. (2) into two continuous probability estimations, we have Eq. (3) below, 
P(s1,· · ·, sk|xn, s1,· · ·, sn 
 1)P(c|xn, 
s1,· · ·, sk, s1,· · ·, sn 
 1) 
(3) Since {s1,· · ·, sk}∩{s1,· · ·, sn 
 1} 
= {s1,· · ·, sk}, Eq. (3) can be simpliﬁed as in Eq. (4), 
P(s1,· · ·, sk|xt, s1,· · ·, s 
t 
 1)P(c|xt, 
s1,· · ·, sk) (4) 
From Eq. (4), it is clear that estimating Eq. (1) using k-NN takes two steps. First, estimating xt’s k neighbors {s1,· · ·, sk} from all the historical stream records {s1,· · ·, st 
 1}. Second, estimating xt’s class label using all the k estimated neigh- bors. 
In data stream environment, estimating these two proba- bilities is very challenging because of the memory consump- tion and predicting time constraints. Speciﬁcally, in order to estimate Eq. (4), two concerns need to be addressed: (1) How to estimate P(s1,· · ·, sk|xn, s1,· · ·, sn 
 1) in a bounded memory space. In data streams, it is impractical to maintain all the (n  1) records {s1,· · ·, sn 
 1} 
for estimation. Thus, a memory-eﬃcient algorithm needs to be designed for this purpose. 
(2) How to estimate the probability P(yn|xn, s1,· · ·, sk) as fast as possible. Given xn, ﬁnding its k nearest neighbors 
s 
1, 
· · ·, sk typically requires a linear scan of all the (n  1) records, corresponding to an O(n) time complexity, which 

========2========

is unacceptable for stream applications. Thus, an eﬃcient search algorithm is needed to reduce the predicting time to a sub-linear complexity of O(log(n)). 
In the following, we use Example 2 to illustrate the streaming lazy learning problem. 
5 
4.5 
4 
3.5 
x 
D5 
3 
2.5 
D4 
2 
D2 
1.5 
D3 
1 
0.5 
D1 
00 
1 
2 
3 
4 
5 
Figure 2. An illustration of Example 2. 
Example 2: Consider a data stream S having three classes {c1,c2,c3}, and each stream record has two dimensions (γ1, γ2), where γi ∈ R. Suppose that at time stamp t500, to- tally 500 stream records are observed as shown in Fig. 2. For simplicity, we assume that these 500 records are distributed uniformly in ﬁve clusters D1,· · ·,D5, and all records in a cluster Di (1 ≤ i ≤ 5) share the same class label, where class c1 is denoted by the symbol “·”, class c2 is denoted by “×”, and class c3 is denoted by “”. The classiﬁcation objective is to predict the class label of the next incoming record (e.g., the small red circle x = (2.2,3.5) in Fig.2) as fast as possible. If the original k-NN method is chosen as the solution, we have to maintain all the 500 stream records for prediction, which is very demanding for memory consumption. Moreover, even if memory consumption is not an issue, a straightforward approach for comparing x with these records would take 500 comparisons, which is ineﬃcient in terms of predicting time. 
To reduce the memory consumption and improve the prediction eﬃciency for lazy learning on data streams, we ﬁrst summarize all the historical training examples into compact exemplars, and then organize these exemplars as leaf nodes in a height-balanced L-Tree structure. In doing so, all the historical stream records can be condensed into a bounded memory space without losing much information, and each incoming record can be eﬃciently estimated in a sub-linear time complexity by traversing the L-tree. 
III. THE EXEMPLAR STRUCTURE 
Instead of maintaining raw stream records, we cluster them into exemplars to reduce memory consumption. An exemplar is a sphere generalizing one or multiple records. Though inspired by micro-clusters [2], exemplars are diﬀer- ent and more complex in that they summarize labeled data. Formally, exemplars are deﬁned as follows. 
Deﬁnition 1: 1. (exemplar) An exemplar M for a set of stream records {s1,· · ·, su} arriving at time stamps {t1,· · ·,tu} is a (d + l + 3)-dimensional vector as in Eq. (5), 
M = (X,R,C,N,T) (5) 
where X is a d-dimensional vector that representing the center, R is the sample variance of all records in M, which is also the covering radius of M, C = [P(c1|M),· · ·,P(cl|M)] is an l-dimensional vector corresponding to the probability of M having class label ci (1 ≤ i ≤ l), N is the total number of records in M, and T represents the time stamp when M was last updated. 
Exemplars have several intrinsic merits for lazy learning on data streams. 
• Exemplars help summarize huge volumes of stream 
data into compact structures which well ﬁt into memory. 
• Exemplars well represent the historical data because 
nearby examples tend to share the same class label and 
can be grouped together as a prediction unit. 
• Exemplars can be easily updated. If a new example x is 
absorbed into M, the exemplar center and radius can be 
conveniently updated using Eqs.(6) and (7) respectively, 
the class label c can be updated using Eq. (8), and the 
time stamp T can be updated to the current time stamp. 
Update center X and radius R. Assume n records {(x1,y1),· · ·,(xn,yn)} have been absorbed into an exemplar M. When a new stream record x arrives, the center X of M can be updated using Eq. (6), 
n 
X ← 
1 
xi + x) = 
n 
+ 
1 
n + 1( 
(6) 
i=1 
n + 1c n + 1x, 
and the radius R of M can be updated using Eq. (7), 
1 
n 
R ← 
n 
( (xi  X)2 +(x  X)2) = 
n  1R+ 1 
i=1 
n n(x 
 X)2 (7) 
Update class label C. Assume a new record arrives with a class label cp (1 ≤ p ≤ l), then the class label vector C can be updated using Eq. (8), 
C ← [nP(c1|M),· · ·, 
nP(cp|M) + 1 nP(cl|M) 
n + 1 n + 1 
· · ·, 
n + 1 
] (8) Note that for each class 
ci (1 ≤ i ≤ l), P(ci|M) =1 
n 
n j=1 
P(ci|xj). Adding a new x with label cp, we can have the following: 
(i) for each j  p, P(cj|x) = 0, and P(cj|M ∪ x) =1 
(n 
n 
n+1 i=1 
P(cj|xi) + P(cj|x)) = 
n+1 
P(cj|M)), 
(ii) for the j = p, P(cj|x) = 1, P(cj|M ∪ x) =1 
[ni=1 P(c + P(cj|x)] = 
nj|xi) 
n+1 
P(cj|M) + 
1 
n+1 n+1. 
Algorithm 1 shows the procedure of constructing and updating a set E of exemplars on data stream S. Initially, a small portion of records S0 are read from stream S, 

========3========

Algorithm 1: Create and maintain exemplars. Input : stream S, initial number of exemplars u, maximum 
radius threshold γ, maximum exemplar threshold N. Output: A set of exemplars E. 
//initialize E ; 
Read a small portion S0 of stream records from S ; E ← K-Means(S0,u) ; 
S ← S\S0 ; 
//update E ; 
while S  ∅ do 
foreach x ∈ S do 
e ← S earch(E, x) ; 
if distance(e, x) > γ then 
e ← CreateExemplar(x, γ0) ; 
if |E| == N then // reach size threshold 
E ← Delete(E,eminT) // release memory 
E ← Insert(E,e) ; 
else 
e ← update(e, x) ; // update rules 
Output E ; 
and clustered into u clusters using K-Means, forming the initial exemplar set E. For each incoming stream record x, its nearest exemplar e is retrieved from E. If the distance between e and x is larger than the given threshold γ, a new exemplar enew will be created and inserted into E. Otherwise, x will be absorbed into e using the updating rules. 
For better understanding, we use Example 3 to illustrate the procedure of summarizing the 500 stream records given in Example 2 into exemplars. 
Example 3: The 500 stream records can be summarized into ﬁve exemplars {M1,· · ·,M5} as shown in Fig. 3. The detailed information of the ﬁve exemplars is listed in Table I. Compared to preserving all raw stream records, the exemplars consume only 1% of the memory space. 
Table I 
EXEMPLARS SUMMARIZED FROM STREAM DATA IN EXAMPLE 2. 
ID M1 M2 M3 M4 M5 
X R C N (1.5,1) 0.5 (1,0,0) 100 (1, 2.5) 0.5 (0,1,0) 100 (2.5, 2) 0.5 (0,0,1) 100 (4.5, 3) 0.5 (0,0,1) 100 (3, 4) 0.5 (0,1,0) 100 
T t100 t200 t300 t400 t500 
From Example 3, we can also observe that the number of comparisons for predicting a testing record is reduced from 500 to only 5. Formally, by using exemplars, the estimate function in Eq. (4) can be converted to Eq. (9), 
P(M1,· · ·,Mk|xn,M1,· · ·,M 
n 
 1)P(c|xn, 
M 
1, 
· · ·,Mk) 
(9) where {M1,· · ·,Mn 
 1} 
are exemplars, and M1,· · ·,Mk are xn’s k nearest exemplars. 
5 
4.5 
4 
M5 
3.5 
x 
3 
2.5 
M4 
2 
M2 
1.5 
M3 
1 
0.5 
M1 
00 
1 
2 
3 
4 
5 
Figure 3. An illustration of Example 3. 
A possible limitation of estimating Eq. (9) is that the number of exemplars continuously increases with time, a linear scan of all exemplars for prediction is still ineﬃcient for time-critical stream applications. This motivates our height-balanced L-tree structure for further improvement of predicting eﬃciency. 
IV. L-TREE INDEXING 
In this section, we introduce the L-tree structure and its three key operations: Search, Insertion, and Deletion. 
A. The L-Tree Structure 
L-Trees extend M-Trees [5]. While M-trees index objects in metric spaces such as voice, video, image, text, and numerical data, L-trees are extended to labeled data on data streams, for which additional information needs to be stored such as class labels and time stamps. L-trees index exemplars that are spherical spatial objects. This is diﬀerent from other spatial indexing structures such as R-Trees and R*-Trees that index rectangular spatial objects. 
Time table 
Tree 
O1 
Root node 
e1 
e2 
Routing node 
Time pointer 
O2 
O3 
t1 
Leaf node 
e3 
e4 
e5 
e6 
e7 
t2 
O4 
O5 
O6 
O7 
O8 
t3 
e8 
e9 
e10 
e11 
e12 
e13 
e14 
e15 
e16 
e17 
e18 
e19 
e20 
e21 
e22 
t4 
... 
Figure 4. An illustration of the L-tree structure. 
An L-Tree mainly consists of two components as shown in Fig. 4: (1) an M-tree like structure on the right-hand side storing all exemplars, and (2) a table structure on the left- hand side storing time stamps of all the exemplars. The two structures are connected by linking each time stamp in the table to its corresponding exemplar in the tree. 
The tree structure consists of two diﬀerent types of nodes: leaf nodes and routing nodes. The root node can be considered as a special routing node that has no parent. A 

========4========

leaf node contains a batch of exemplars represented in the form of, 
(pointer,distance), (10) 
where pointer references the memory location of an exem- plar, distance indicates the distance between the exemplar and its parent node. On the other hand, a routing node in the tree structure contains entries in the form of, 
(µ,r,child,distance), 
(11) 
where µ represents the center of the covering space, r rep- resents the covering radius, child is a pointer that references its child node, and distance denotes the distance of the entry to its parent node. Two important parameters of L-Trees are M and m, which denote the maximum (M) and the minimum (m) number of entries in a node. 
Similar to M-Trees, L-Trees have the following properties: 
• Routing node. A routing node has between m and M 
number of entries unless it is the root. Each entry in a 
routing node covers the tightest spatial area of its child 
nodes. 
• Leaf node. A Leaf node contains between m and M 
number of exemplars unless it is the root node, and all 
leaf nodes are at the same level. 
• Root node. The root node is a special routing node, 
which has at least two entries unless it is a leaf. 
Example 4: Fig. 5 illustrates the L-tree structure for the exemplars in Example 3. For simplicity, the time stamp table is omitted. 
O1 
Input 
e1: (1.67,1,83,1.47,child, ^) 
e2: (3.75,3.50,1.45, child, ^) 
O2 
O3 
e3: (M1,0.85) e4 :(M2,0.94) 
e5:(M3,0.85) 
e6: (M4,0.90) 
e7: (M5,0.90) 
M1 
M2 
M3 
M4 
M5 
Figure 5. L-tree for the exemplars in Example 3. 
B. Search 
Each time a new record x arrives, a search operation is invoked to calculate the class label for x. The search algorithm ﬁrst traverses the L-tree to ﬁnd its k nearest exemplars in leaf nodes. Then it calculates the class label for x by combining label information from all retrieved k exemplars using a majority voting scheme. 
Compared to a linear scan of all exemplars as shown in Algorithm 1, organizing the exemplars in a height-balanced tree can signiﬁcantly reduce the search time cost from O(N) to O(log(N)), where N is the total number of exemplars in the L-tree. Such a search method can be further improved by using a branch-and-bound technique. 
The bound b is deﬁned as follows. Assume that the search algorithm has traversed u routing entries {O1,· · ·,Ou} (u ≥ 
Algorithm 2: Search 
Input : L-tree T, incoming stream record x, parameter k. Output: x’s class label yx. 
Initialize(Q) ; // priority queue Q Initialize(U) ; // array contains k results b ← ∞; // initialize the bounding value foreach entry e ∈ T do // traverse root node 
d ← distance(x,e) ; 
if d < b then 
InQueue(Q,e) // Add to the tail of Q 
b ← U pdateBound(b,d) ; // b meets Eq.12 
U ← U pdateArrary(U,e) ; 
Q ← PriorityS ort(Q) ; // keep Q a priority queue while Q.head  Q.tail do 
q ← GetQueue(Q) ; // get the head of Q 
O ← q.child ; 
foreach entry e ∈ O do 
if |e.distance  distance(q, x)| ≤ e.r + b then 
d ← distance(e, x) ; 
if d < b then // update bound 
b ← updateBound(b,d) ; 
if e is in routing node then 
InQueue(Q,e) ; 
else 
U ← U pdateArray(U,e) ; 
DeQueue(Q.head) ; // remove the head of Q 
Q ← PriorityS ort(Q) ; 
foreach entry e ∈ U do 
calculate yx using majority voting; 
Output yx ; 
k), with the distance between each Oi and x represented as d = {d1,· · ·,du}. The bound b is deﬁned as the maximal distance of the k smallest distances in d = {d1,· · ·,du} as in Eq. (12), 
b = max {mink{d1,· · ·,du}} (12) 
x 
d(ep,x) 
ep 
ec.d 
, d(ec 
x) 
ec. 
r ec 
Figure 6. An illustration of the bound in Eq.(13). 
The bound b can signiﬁcantly reduce the search cost in L-trees. For example, as shown in Fig. 6, assume that the current entry is ec, and ep is the parent entry of ec. Then, the tree-pruning bound is |d(ec, x)  ec.r| > b, which is equivalent to solving the following Eq. (13), 
|d(ep, x) 
 ec.d| > b + ec.r, 
(13) 

========5========

where d(ep, x) denotes the distance between ep and x, ec.d is the distance between ec and ep, ec.r is ec’s covering radius, and b is the bound. Obviously, all the above distances are pre-computed, and Eq. (13) can be easily estimated. 
Algorithm 2 contains detailed procedures of the branch- and-bound search. A priority queue Q is used to perform breath-ﬁrst search. In addition, an array U is used to preserve all the k results. The main purpose of the algorithm is to re- trieve the k results using minimized number of comparisons by making full use of the bound b. For each routing entry Oi, if and only if the pruning condition in Eq. (13) is satisﬁed, the node will be traversed. The function U pdateArrary(U,e) guarantees that the array U always contains the k results by continuously removing the unsatisﬁed ones. 
5 
4.5 
4 
e7 
3.5 
x 
(4) 
(3) 
(2) 
e2 
3 
(1) 
e6 
2.5 
2 
e4 
ee11 
1.5 
e5 
1 
e3 
0.5 
00 
1 
2 
3 
4 
5 
Figure 7. An illustration of search in Example 4. 
Example 5: Consider an incoming testing record x = (2.2,3.5), we need to traverse the L-tree in Fig. 5 to predict its class label. Assume the parameter k is set to 1. The search algorithm initially pushes entries e1 and e2 in query Q. Then, it calculates the distance d(x,e1) = 1.75 and d(x,e2) = 1.55, and updates the bound b to the smaller one of 1.55 and traverses along e2. Next, it sequentially compares x with entries e6 and e7 in the leaf node O3 and obtains d(x,e6) = 2.55 and d(x,e7) = 0.94. Thus, e7 is taken as the 1-nearest neighbor, and the class label yx is set to c2. The comparison path is shown in Fig. 7. In this example, the search would involve four comparisons in the worst case. Compared to the linear scan that requires ﬁve comparisons, L-tree achieves 20% improvement in the worst case. 
C. Insertion 
Insertion operations are used to absorb new stream records into L-trees, so that they can quickly adapt to new trends and patterns in data streams. 
Algorithm 3 lists detailed procedures of the insertion operation. For each incoming record x, a search algorithm is invoked to ﬁnd its nearest leaf node O. When inserting x in the retrieved leaf node O, three diﬀerent situations need to be considered: 
Algorithm 3: Insertion 
Input : L-tree T, record x, parameters m, M. Output: Updated L-tree T. 
O ← S earchLea f(x,T) ; 
if d(x,e) < e.r then 
e ← e ∪ x ; 
T ← ad justTree(T,e) ; 
// Case 1. 
else 
enew ← CreateEntry(x) ; 
if O.entries() < m then 
O ← O ∪ enew ; 
T ← ad justTree(T,O) ; 
else 
< O1,O2 >← S plit(O,enew) ; 
T ← ad justTree(T,O1,O2) ; 
// Case 2. 
// Case 3. 
Output T ; 
• x can be absorbed in one of the entries e ∈ O. This is 
the ideal situation, and the algorithm updates the leaf 
node deﬁned in Eq. (1) according to updating rules. 
• x cannot be absorbed in any entry, and the leaf node O 
is not full. In this case, a new entry enew is generated 
and inserted into the leaf node O. 
• x cannot be absorbed in any entry, and the leaf node 
O is full. In this case, a new entry enew is generated, 
and then a node splitting operation is invoked to obtain 
spare room for insertion. 
Node splitting is the most critical step in the insertion operation. Similar to M-trees, a basic principle in splitting is that the split leaf nodes should have the minimized spatial expansion. This is equivalent to minimizing Eq. (14), 
< O1,O2 >= argmin<X,Y>|X,Y∈O∪e 
new 
(X.r + Y.r), 
(14) 
where X and Y are variables, and O1 and O2 are the new leaf nodes containing enew and all entries in O. Obviously, solving Eq. 14 requires examining all possible combinations of all entries in O ∪ enew, which is very diﬃcult especially when M is large. Alternatively, a greedy heuristic would ﬁrst randomly select two entries in O ∪ enew that has the largest distance, and then cluster all the remaining M  1 entries in O ∪ enew into the given two groups. 
O1 
Input 
e1: (1.67,1,83,1.47,child, ^) 
e2: (3.75,3.50,1.66, child, ^) 
O2 
O3 
e3: (m1,0.85) e4 :(m2,0.94) e5:(m3,0.85) 
e6: (m4,0.90) e7: (m5,0.90) e8: (m6,1.55) 
m1 
m2 
m3 
m4 
m5 
m6 
Figure 8. An illustration of L-tree in Example 3 after inserting a new stream record x. 

========6========

5 
4.5 
4 
e8 
3.5 
e7 
e2 
3 
2.5 
e6 
2 
e4 
1.5 
ee11 
e5 
1 
0.5 
e3 
Δ  e 
2.r 
00 
1 
2 
3 
4 
5 
Figure 9. An illustration of insertion in Example 4. 
Example 6: Consider inserting x = (2.2,3.5) into the L- tree in Example 3. First of all, the search algorithm locates entry e7 in the leaf node O3 as the target node. Then, it examines the distance between x and e7, which equals to 0.94 and is larger than the covering radius of e7. Thus, a new entry e8 is generated containing a new exemplar M6 as follows: M6.X = (2.2,3.5), M6.R = 0.1, M6.C = (0,1,0), M6.N = 1, and M6.T = t501. Since the leaf node O has only two entries, which is less than its capacity M = 3, then e8 is inserted into O directly. In addition, the covering space of entry e2 in the parent node is enlarged to e2.r = 1.66. The updated L-tree structure is shown in Fig. 8 and Fig. 9. 
D. Deletion 
The deletion operation discards outdated exemplars when the L-tree reaches its capacity. For example, if the largest tree size is set to four in Example 4, e3 will be discarded from the L-tree when e8 is generated, this is because e3 has the earliest time stamp t100, which means it has not been updated for a long time, and is possibly outdated. 
The detailed procedures of the deletion operation are shown in Algorithm 4. First of all, the outdated entry in a leaf node is discovered by scanning the time table, and deleted from the L-tree. After the deletion, there are two diﬀerent situations. 
• The number of entries in the leaf node is larger than m. In this case, the algorithm iteratively adjusts the covering radius r of its parent entries, making these nodes to be more compact. 
• The number of entries in the leaf node is smaller than m. In this case, a delete-then-insert method will be used. Similar methods are commonly used in spatial indexing structures. It ﬁrst iteratively deletes node(s) having entries less than m, and re-inserts their entries into the tree using the insertion operation in Algorithm 3. This method is advantageous in that: (1) It is easy to implement. (2) Re-insertion will incrementally reﬁne the spatial structure of the tree. 
Algorithm 4: Deletion 
Input : L-tree T, parameters m, M. 
Output: Updated L-Tree T. 
//locate a leaf node from the time table ; 
pointer ← Locate(Time table) ; 
O ← delete(T, pointer) ; 
//iterative deletions ; 
if O is root then 
if O.entries()==1 then // the root node 
T ← O.child ; 
delete(O) ; 
else // non-root nodes 
if O.numberO f Entries() < m then 
foreach entry e ∈ O do 
delete(e,O) ; 
T ← insert(e,O) ; 
Output T ; 
V. EXPERIMENTS 
In this section, we present extensive experiments on benchmark datasets to validate the performance of L-trees with respect to time eﬃciency for prediction, memory consumption, and predicting accuracy. All experiments are implemented in Java on a Microsoft XP machine with 3GHz CPU and 2GB memory. 
A. Experimental Settings 
Benchmark data sets. Twelve data sets from the UCI data repository [4] and stream data mining repository [22] are used in our experiments. Table II lists the basic information of the data sets. Due to the page limit, please refer to [4], [22] for detailed descriptions. The synthetic data stream contains a gradually changing concept (decision boundary) deﬁned by Eq. (15): 
τ 
 1 
g(x) = ai · 
(xi + xi+1) 
i=1 
xi 
(15) where ai controls the shape of the decision surface and g(x) determines the class label of each record x. Concept drifting can be controlled by adjusting ai to ai +αh after generating D records, where α deﬁnes the direction of change and h ∈ [0,1] deﬁnes the magnitude of change. α =  1 with probability of ρ. In our experiments, we set ρ = 0.2, h = 0.1, D = 2000, τ = 3, and generate 105 stream records from ﬁve classes. Parameter m, if not specially mentioned, is set to 1. 
Benchmark methods. For comparison purposes, we imple- mented two lazy learning and two eager learning models. (1) Global k-NN. This is the traditional k-NN method that maintains all historical stream records for prediction. (2) Local k-NN. In this method, only a small portion of the most recent stream records are preserved for prediction. Com- pared to our method, this method simply uses a linear scan 

========7========

Table II REAL-WORLD DATA SETS 
Name 
Records Attr. clas. 
Sensor KDDCUP99 Powersupply Waveform halloﬀame kr-vs-kp sick hypothyroid mushroom splice nursery musk 
2.0 × 106 4.9 × 105 2.9 × 104 5.0 × 103 1.3 × 103 3.1 × 103 3.7 × 103 3.7 × 103 8.1 × 103 3.1 × 103 1.2 × 104 6.5 × 103 
5 54 41 23 2 24 40 3 17 3 37 2 30 2 30 4 23 2 61 3 9 5 167 2 
Parameters local M γ 2000 200 5 1000 100 3 1000 100 4 100 30 6 10 5 1 20 5 1 25 5 3 20 4 1 50 10 1 50 10 1 40 10 1 40 10 1 
to retrieve the k nearest neighbors. (3) Incremental Decision Tree. This method belongs to the eager learning category. It is based on the method proposed in [8]. The source code can be downloaded from www.cs.washington.edu/dm/vfml/. (4) Incremental Naive Bayes. This is another standard eager learning model. 
Measures. We use three measures in our experiments.(1) Predicting time. By using a height-balanced tree to index all exemplars, L-trees are expected to achieve lower computa- tional costs than other two lazy learning models. (2) Memory consumption. L-trees are expected to consume much less memory space than Global k-NN. (3) Predicting accuracy. L-trees are expected to achieve similar predicting accuracy to Global k-NN, and better accuracy than Local k-NN. 
B. Parameter Study on Synthetic Data Streams 
Parameter γ. This parameter denotes the maximum radius threshold of exemplars in an L-tree. Fig. 10 shows the predicting time and predicting accuracy w.r.t. diﬀerent γ values. From the results we can observe that both the predicting time and predicting accuracy decreases with in- creasing γ. This is because the larger γ, the fewer exemplars that are generated. As a result, both the predicting time and memory consumption are reduced. On the other hand, generalizing stream records into fewer exemplars leads to more information loss and reduced predicting accuracy. 
3.5 
1 
predicting efficiency memory consumption 
3 
0.9 
2.5 
0.8 
2 
0.7 
Accuracy 
Memory(kb) 
1.5 
0.6 
1 
0.5 
0.511 
22 
33 
44 
55 
66 
77 
88 
99 
0.41010 
γ 
Figure 10. Comparisons with respect to diﬀerent γ values. M=10. 
4 
10 
memory consumption predicting efficiency 
3.5 
8 
3 
6 
2.5 
4 
Time(ms) 
Memory(kb) 
2 
2 
1.51 
2 
5 
10 M 
20 
50 
0100 
Figure 11. Comparisons with respect to diﬀerent M values. γ = 0.1 
Parameter M. This parameter denotes the maximum num- ber of entries in each node of an L-tree. Fig. 11 shows the predicting time and memory consumption with respect to diﬀerent M values. From the results we have the following observations. When M increases at the very early stage, both the predicting time and memory consumption decrease signiﬁcantly. After that, the beneﬁt becomes marginal and then turns negative with increasing M. This is because increasing M at an early stage reduces the number of routing nodes and leaf nodes. As a result, the k-nearest neighbors of an incoming query are likely to be stored in the same node, leading to reduced search and storage costs. However, when M continues to increase, exemplars that slightly overlap with each other will be mistakenly stored in the same node, leading to extra comparisons on each node and increased search time. Therefore, a desirable M value should neither be too large nor too small. 
C. Performance Study on Real-world and Synthetic Data Streams 
1.05 
1 
0.95 
0.9 
Accuracy 
0.85 
0.8 
Lazy learning adapts faster to concept drifting 
0.75 
Incremental decision tree Incremental NaiveBayes L−tree based lazy learning 
0.70 
5 
10 
15 
20 Chunk ID 
25 
30 
35 
40 
Figure 12. Comparisons between eager and lazy learning models on 40 continuous data chunks. 
Table III presents the comparison results between three lazy learning models. The parameter k is set to 3, other parameters are listed in Table II. From the results we can conclude that: (1) Compared to Global k-NN, L-tree is more eﬃcient with respect to predicting time and memory consumption, and can achieve similar predicting accuracy. This is because L-trees condense historical stream records into compact exemplars and indexes them into a high- 

========8========

Table III 
COMPARISONS AMONG DIFFERENT k-NN ALGORITHMS 
Data 
kddcup99 sensor powersupply Waveform halloﬀame kr-vs-kp sick hypothyroid mushroom splice nursery musk 
Global k-NN 
memory(kb) time(ms) accuracy 
26500 9489.00 0.9906 
11345 12235.00 0.7000 
166.77 8653.00 0.7340 
27.7 311.00 0.7436 
67.0 3429.00 0.9049 
159.8 16831.00 0.9942 
188.6 28849.00 0.8864 
188.6 26637.00 0.8562 
406.2 273467.00 0.8701 
159.5 21873.00 0.9975 
648.0 975151.00 0.9174 
329.9 398602.00 0.9993 
memory(kb) time(ms) accuracy 
104.00 277.00 0.9731 
20 143.00 0.6201 
100 124.00 0.6680 
10.0 30.00 0.7255 
10 1.68 0.8735 
20 63.00 0.9991 
25 94.00 0.8864 
20 30.00 0.8562 
50 330.00 0.8570 
50 109.00 0.9915 
40 234.00 0.9029 
40 673.00 0.9998 
balanced tree structure. In doing so, the memory cost and predicting time can be signiﬁcantly reduced without losing much useful information for prediction. (2) Compared to Local k-NN, L-tree can achieve better predicting accuracy and eﬃciency given the same memory consumption. This is because L-tree maintains more information for prediction than local k-NN. In addition, by using a high-balanced tree structure, L-tree can achieve better predicting eﬃciency. 
Table IV and Fig. 12 show the comparisons between L-tree, incremental Decision Tree, and incremental Naive Bayes learning models. From the results we can observe that: (1) In terms of time cost, L-tree incurs no training time but more predicting time. By combining training and predicting time together, we can see that L-tree is even more eﬃcient than the two eager learning models. (2) In terms of predicting accuracy, L-tree performed similarly to the eager learning models on the static data sets. L- tree did not show advantage over eager models in these experiments because the datasets did not exhibit signiﬁcant concept drifting. As shown in Fig. 12, on the synthetic data streams that exhibit signiﬁcant concept drifting and complex decision boundaries, L-tree outperformed the eager learning models because it adapts faster to the drifting concepts in data streams. 
In summary, the proposed L-tree approach enables fast lazy learning on data streams by signiﬁcantly reducing pre- dicting time and memory consumption. Moreover, compared to eager learners, lazy learning can achieve better predicting accuracy for concept drifting data streams. 
VI. RELATED WORK 
Eager learning and lazy learning. Decision trees [13] are typical eager learners and k-nearest neighbor (knn) classiﬁers [7] exemplify the simplest form of lazy learners. The distinguishing characteristics of eager and lazy learners were identiﬁed in [3]. Eager learners are model-based and parametric, where the training data are greedily compiled into a concise hypothesis (model) and then completely dis- carded. Lazy learners are instance-based and non-parametric, 
Local k-NN 
L-tree k-NN memory(kb) time(ms) accuracy 
91 65.53 0.9902 
91 52.68 0.6903 
91 22.68 0.7135 
9.7 8.00 0.7391 
9 0.30 0.8928 
17 12.00 0.9991 
21 29.00 0.8864 
16 2.50 0.8562 
46 22.27 0.8663 
46 20.09 0.9918 
37 18.90 0.9489 
37 26.99 0.9998 
where the training data are simply stored in memory and the inductive process is deferred until a query is given. Obvi- ously, lazy learners incur lower computational costs during training but much higher costs in answering queries also with greater storage requirements, not scaling well to large datasets. However, by retraining all the training data, lazy learners do not lose information in the training data, which make them capable of quickly adapting to the chancing data distributions in dynamic learning environments, such as data streams. On the other hand, for lazy learning, a prediction model is built for each individual test record. As a result, the decision is customized according to the data characteristics of each test record. In concept drifting environments, if data distributions or concepts drift rapidly, lazy learning has the advantage of relying on the loci of each test example to derive decision models and outperform global models. Data stream classiﬁcation. Data stream classiﬁcation [1] has vast real-world applications, which are usually time- critical and require fast predictions. Many sophisticated learning methods have been proposed, such as incremental learning [8] and ensemble learning [12]. These methods be- long to the eager learning category, which aims at building a global model from historical stream data for prediction. Our work diﬀers from existing approaches in that we propose and study lazy learning for data stream classiﬁcation. K-NN query on data streams. A number of k-NN query methods on data streams have been proposed with focus on various types of data, such as relational data, uncertain data, semi-structured data, spatial data, fuzzy data, and time series data [18], [16]. Our work diﬀers from theirs in two ways. First, we query data streams for classiﬁcation purposes while they do not. Second, existing k-NN query methods on data streams use a sliding window to reduce the query space for eﬃciency, which can be considered as local k-NN. In contrast, we maintain high-level compact summaries of stream records, which is an approximation of global k-NN on data streams. 
Indexing techniques on databases. k-NN query on 

========9========

Table IV 
COMPARISONS AMONG DIFFERENT LEARNING ALGORITHMS 
Data 
kddcup99 sensor powersupply Waveform halloﬀame kr-vs-kp sick hypothyroid mushroom splice nursery musk 
L-tree k-NN train(ms) test(ms) accuracy 
0 65.53 0.9902 
0 52.68 0.6903 
0 22.68 0.7135 
0 8.00 0.7391 
0 0.30 0.8928 
0 12.00 0.9991 
0 29.00 0.8864 
0 2.50 0.8562 
0 22.27 0.8663 
0 20.09 0.9918 
0 18.90 0.9489 
0 26.99 0.9998 
train(ms) test(ms) accuracy 1550.00 320.00 0.9961 1420.00 460.00 0.6631 800.00 0.00 0.7116 529.00 0.00 0.6855 46.00 0.00 
16.00 0.00 0.9080 111.00 15.00 0.8812 123.00 16.00 
219.00 16.00 
76.00 15.00 0.9093 341.00 47.00 0.9410 62.00 47.00 0.9105 
databases has been extensively studied in the databases community. Many eﬃcient indexing and hashing techniques have been proposed to partition the query space and achieve O(log(N)) query time. Examples of such techniques include k-d tree [14], vp-tree [17], to name a few. Our work diﬀers from theirs in that we index dynamically changing data streams while they index static data. 
Data stream summarization. In order to query or mine stream data, many synopsis structures [6] exist to transform large volume stream data into memory-economic compact summaries that can be rapidly updated as the stream records arrive. Typical synopses include sampling techniques and sketching methods. Our work diﬀers from theirs in that we summarize labeled data for classiﬁcation purposes. 
VII. CONCLUSIONS 
Lazy learning can be advantageous in dynamic and com- plex learning environments such as data streams. However, due to its high memory consumption and low eﬃciency for prediction, lazy learning is not favorable for stream applications where rapid predictions are essential. To over- come these limitations and enable fast lazy learning for data streams, we proposed a novel Lazy-tree indexing structure that dynamically maintains compact summaries of historical stream records to facilitate accurate and fast predictions. Experiments and comparisons demonstrated the performance gain (in terms of memory consumption, time eﬃciency for prediction, and classiﬁcation accuracies) on both synthetic and real-world data streams. 
There are many interesting topics for future work. For ex- ample, sophisticated class-aware summarization techniques [9] can be used to generate exemplars. As another example, the temporal dimension can be included in the distance computation for nearest neighbors, and various weighting schemes can also be investigated beyond the basic majority voting. Last but not least, we hope our study can help unleash the potential of lazy learning in data stream classiﬁ- cation and open up new possibilities for tackling the inherent problems and challenges in data stream applications. 
Decision Tree 
0.9154 
0.8907 0.8969 
Naive Bayes train(ms) test(ms) accuracy 1210.00 1126.00 0.9860 0.00 3410.00 0.7020 0.00 147.00 0.7129 80.00 107.00 0.7609 0.00 32.00 0.8525 16.00 31.00 0.8174 31.00 110.00 0.8376 0.00 78.00 0.8575 62.00 47.00 0.8846 61.00 31.00 0.9451 95.00 125.00 0.8523 1248.00 1096.00 0.8235 
VIII. ACKNOWLEDGEMENTS 
This research was supported by the National Science Foundation of China (NSFC) Grant (61003167), Basic Research Program of China 973 Grant (2007CB311100), National High Technology Research and Development Pro- gram of China 863 Grant (2011AA010705), Texas Norman Hackerman Advanced Research Program Grant (003656- 0035-2009), and Australian Research Council (ARC) Future Fellowship Grant (FT100100971). 
REFERENCES 
[1] C. Aggarwal. Data Streams: Models and Algorithms. Springer, 2007. [2] C. Aggarwal, J. Han, J. Wang, and P. Yu. A framework for clustering evolving 
data streams. In Proc. of VLDB 2003. 
[3] D. Aha. Lazy learning. Artif. Intell. Rev., 7:7–10, 1997. 
[4] A. Asuncion and D. Newman. UCI Machine Learning Repository. Irvine, CA, 
2007. 
[5] P. Ciaccia, M. Patella, and P. Zezula. M-tree an eﬃcient access method for 
similarity search in metric spaces. In Proc. of VLDB 1997. 
[6] G. Cormode and S. Muthukrishnan. Summarizing and mining skewed data 
streams. In Proc. of SDM 2005. 
[7] B. Dasarathy. Nearest neighbor (nn) norms: Nn pattern classiﬁcation techniques. 
IEEE Computer Society Press, 1991. 
[8] P. Domingos and G. Hulten. Mining high-speed data streams. In Proc. of KDD 
2000. 
[9] B. Gao and M. Ester. Turning clusters into patterns: Rectangle-based discrimi- 
native data description. In Proc. of ICDM 2006. 
[10] J. Gao, W. Fan, and J. Han. On appropriate assumptions to mine data streams: 
analysis and practice. In Proc. of ICDM 2007. 
[11] I. Hendrickx. Hybrid algorithms with instant-based classiﬁcation. In Proc. of 
ECML PKDD 2005. 
[12] H.Wang, W. Fan, P. Yu, and J. Han. Mining concept-drifting data streams using 
ensemble classiﬁers. In Proc. of KDD 2003. 
[13] R. O. L. Breiman, J.H. Friedman and C. Stone. Classiﬁcation and regression 
trees. Belmont, CA: Wadsworth International Group, 1984. 
[14] D. Lee and C. Wong. Worst-case analysis for region and partial region 
searches in multidimensional binary search trees and balanced quad trees. Acta 
Informatica, 1977. 
[15] A. Tsymbal. The problem of concept drift: Deﬁnitions and related work. 2004. [16] D. P. Y. Tao and Q. Shen. Continuous nearest neighbor search. In Proc. of 
VLDB 2002. 
[17] P. Yianilos. Data structures and algorithms for nearest neighbor search in general 
metric spaces. In Proc. of ACM-SIAM Symposium on Discrete algorithms, 1993. [18] K. Zhang, P. Fung, and X. Zhou. K-nearest neighbor search for fuzzy objects. 
In Proc. of SIGMOD 2010. 
[19] P. Zhang, X. Zhu, J. Tan, and L. Guo. Classiﬁer and cluster ensembles for 
mining concept drifting data streams. In Proc. of IEEE ICDM 2010 . [20] P. Zhang, X. Zhu, Y. Shi, L. Guo, and X. Wu. Robust Ensemble Learning for 
Mining Noisy Data Streams. Decision Support Systems, Vol.50 (2), 2011. [21] P. Zhang, J. Li, P. Wang, B. Gao, X. Zhu, and L. Guo. Enabling Fast Prediction 
for Ensemble Models on Data Streams. In Proc. of KDD 2011. 
[22] X. Zhu. Stream data mining repository. Available online: http://cse.fau.edu/ 
∼xqzhu/stream.html, 2010. 

========10========

