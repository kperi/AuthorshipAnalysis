Research Track Paper 
The Minimum Consistent Subset Cover Problem and its 
Applications in Data Mining 
Byron J. Gao1;2, Martin Ester1, Jin-Yi Cai2, Oliver Schulte1, and Hui Xiong3 
1 
School of Computing Science, Simon Fraser University, Canada 
2 
Computer Sciences Department, University of Wisconsin - Madison, USA 3 
Department of Management Science & Information Systems, Rutgers University, USA 
ABSTRACT 
In this paper, we introduce and study the Minimum Consis- tent Subset Cover (MCSC) problem. Given a ﬂnite ground set X and a constraint t, ﬂnd the minimum number of consis- tent subsets that cover X, where a subset of X is consistent if it satisﬂes t. The MCSC problem generalizes the tradi- tional set covering problem and has Minimum Clique Parti- tion, a dual problem of graph coloring, as an instance. Many practical data mining problems in the areas of rule learning, clustering, and frequent pattern mining can be formulated as MCSC instances. In particular, we discuss the Minimum Rule Set problem that minimizes model complexity of de- cision rules as well as some converse k-clustering problems that minimize the number of clusters satisfying certain dis- tance constraints. We also show how the MCSC problem can ﬂnd applications in frequent pattern summarization. For any of these MCSC formulations, our proposed novel graph- based generic algorithm CAG can be directly applicable. CAG starts by constructing a maximal optimal partial so- lution, then performs an example-driven speciﬂc-to-general search on a dynamically maintained bipartite assignment graph to simultaneously learn a set of consistent subsets with small cardinality covering the ground set. Our experi- ments on benchmark datasets show that CAG achieves good results compared to existing popular heuristics. 
Categories and Subject Descriptors: H.2.8 [Database Management]: Database Applications { Data Mining General Terms: algorithms, theory, performance Keywords: minimum consistent subset cover, minimum rule set, converse k-clustering, pattern summarization 
1. INTRODUCTION 
Combinatorial optimization problems such as set cover- ing and graph coloring have been extensively studied. By making connections to these classical problems, we can gain important insights into practical data mining applications. 
In this paper, we introduce and study the Minimum Con- sistent Subset Cover (MCSC) problem. Given a ﬂnite ground 
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for proﬁt or commercial advantage and that copies bear this notice and the full citation on the ﬁrst page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior speciﬁc permission and/or a fee. 
KDD'07,August12{15,2007,SanJose,California,USA. Copyright 2007 ACM 978-1-59593-609-7/07/0008 ... $5.00. 
set X and a constraint t, ﬂnd the minimum number of consis- tent subsets that cover X, where a subset of X is consistent if it satisﬂes t. The MCSC problem provides one way of generalizing the traditional set covering problem [12], where a subset of X is consistent if it is a given subset. Diﬁerent from set covering, in typical MCSC instances the consistent subsets are not explicitly given and they need to be gen- erated. For example, Minimum Clique Partition (MCP), a dual problem of graph coloring, can be considered as an MCSC instance, where a subset is consistent if it forms a clique and the cliques are not given as input. 
As a practical application of the MCSC problem in rule learning, the Minimum Rule Set (MRS) problem ﬂnds a complete and consistent set of rules with the minimum cardi- nality for a given set of labeled examples. The completeness and consistency constraints require correct classiﬂcations of all the given examples. With the goal of minimizing model complexity, the MRS problem can be motivated from both data classiﬂcation and data description applications. The MRS problem is a typical MCSC instance, where a subset is consistent if it forms a consistent rule, i.e., the bounding box of the subset contains no examples of other classes. 
As a prominent clustering model, k-clustering generates k clusters minimizing some objective, such as maximum radius as in the k-center problem [21] or maximum diameter as in the pairwise clustering problem [4]. The radius of a cluster is the maximum distance between a ﬂxed point (center) and any point in the cluster, and the diameter is the maximum distance between any two points in the cluster. Since the number of clusters is often hard to determine in advance, converse k-clustering can be a more appropriate clustering model, where a maximum radius or diameter threshold is given and the number of clusters k is to be minimized. The converse k-center and converse pairwise clustering problems are both MCSC instances, where a subset is consistent if it forms a cluster satisfying a given distance constraint. 
Frequent pattern mining has been a trademark of data mining. While the mining e–ciency has been greatly im- proved, interpretability instead became a bottleneck to its successful application. As a known problem, the overwhelm- ingly large number of generated frequent patterns containing redundant information are in fact \inaccessible knowledge" that need to be further mined and explored. Thus, sum- marization of large collections of patterns in the pursuit of usability has emerged as an important research problem. The converse k-clustering models discussed above as well as some other MCSC formulations appear to be a very reason- able and promising approach towards this problem. 
310 

========1========

These formulated MCSC instances generally feature anti- monotonic constraints, under which any subset of a con- sistent subset is also consistent. Such instances allow the design of e–cient and eﬁective heuristic algorithms. Stimu- lated by a theoretical study, our graph-based generic algo- rithm CAG starts by constructing a maximal optimal partial solution, and then performs an example-driven speciﬂc-to- general search on a dynamically maintained bipartite assign- ment graph to simultaneously learn a small consistent subset cover. The construction of initial optimal partial solution, the use of assignment graph allowing good choices of both the element and the consistent subset in an assignment, and the example-driven simultaneous learning (in contrast to the most-seen separate-and-conquer) strategy are the three novel design ideas that help to well guide the search leading to good performance of CAG, as demonstrated by our ex- periments on rule learning and graph coloring benchmarks in comparison with existing popular heuristics. 
Contributions. (1) We introduce and study the Mini- mum Consistent Subset Cover (MCSC) problem, which gen- eralizes the set covering problem and has Minimum Clique Partition, a dual problem of graph coloring, as an instance. 
(2) The MCSC problem has many practical applications in data mining. In particular, we study the Minimum Rule Set problem for rule learning. We also discuss applications in converse k-clustering and frequent pattern summarization. 
(3) To solve MCSC instances, we present a graph-based generic algorithm CAG with several novel design ideas, whose performance is justiﬂed by our experimental evaluation. 
2. PRELIMINARIES AND RELATED WORK 
In this section we provide preliminaries on graph theory and a review of related work in a very concise manner. 
Preliminaries. A graph is complete if all of its vertices are pairwise adjacent. A clique of a graph G = (V; E) is a subset of V such that the induced subgraph is complete. An independent set of G is a subset of V such that no two vertices in the subset are connected in G. The independence number of G, usually denoted by ﬁ(G), is the cardinality of the largest independent set. Independent sets and cliques are opposite in the sense that every independent set in G corresponds to a clique in the complementary graph G. 
We say V 
0 
 V is a dominating set if for all u 2 V ¡ V 
0, there is some v 2 V 
0 
such that (u; v) 2 E. A maximal independent set, say V 
0, is also a dominating set. If it is not, there must be some u 2 V ¡ V 
0 
that is not adjacent to any v 2 V 
0, then 
u can be added to V 
0 
to form a larger independent set, but then V 
0 
is not maximal. 
The Minimum Clique Partition (MCP) problem is to ﬂnd a partitioning of the vertices of a graph G into the minimum number of disjoint vertex sets, each of which must be a clique in G. That minimum number is called the clique partition number of G and usually denoted by ´(G). 
The well-known graph coloring problem is to use the min- imum number of colors to color the vertices of a graph G such that no adjacent vertices receive the same color. That minimum number is called the chromatic number of G and usually denoted by ´(G). Since the vertices of an indepen- dent set can be safely colored with the same color, the graph coloring problem precisely minimizes the number of disjoint independent sets of G that form a partition of V . 
Apparently, MCP is a dual problem of graph coloring. An 
Research Track Paper 
instance of the former on G is an instance of the latter on G. Thus, we have ´(G) = ´(G). 
Observation 1. ﬁ(G) • ´(G). 
The observation states that the clique partition number of a graph is lower-bounded by its independence number. It is rather straightforward since two vertices in an independent set cannot appear together in a clique. 
Related work. The traditional set covering problem [12] ﬂnds the minimum number of subsets from a given collec- tion of subsets that cover a given ground set. It is one of the most fundamental algorithmic problems that has many vari- ants, settings, and applications. The problem is NP-hard and there is no constant factor approximation. It is approx- imable within 1 + logn by a simple greedy algorithm [12], which iteratively selects the subset that covers the largest number of uncovered elements until the ground set is cov- ered. This greedy algorithm essentially adopts a separate- and-conquer approach as seen in most rule learners. 
In the MCSC problem we study, the subsets are not ex- plicitly given. Instead, a constraint is given and used to qualify the subsets that can be used in a cover. The MCP problem and many practical data mining applications can be formulated as MCSC instances. 
Graph coloring heuristics can be applied to complemen- tary graphs to solve MCP instances. DSATUR [3] is one of the most popular construction heuristics. In DSATUR, a vertex with the largest number of diﬁerent colors assigned to adjacent vertices is chosen and assigned with the ﬂrst feasible color, where colors are pre-ordered. Ties are broken favoring the vertex with the largest number of uncolored ad- jacent vertices. DSATUR has a cubic runtime complexity. 
In the past few decades, numerous rule learners have been developed, such as the famous AQ family, CN2 and RIP- PER [5]. Most of them follow a separate-and-conquer ap- proach, which originated from AQ and still enjoys popular- ity. The approach searches for a rule that covers a part of the given (positive) examples, removes them, and recursively conquers the remaining examples by learning more rules un- til no examples remain. Most rule learners minimize model complexity assuming the validity of Occam’s Razor. 
Previous work related to converse k-clustering and fre- quent pattern summarization will be discussed in Section 4. 
3. THE MINIMUM CONSISTENT SUBSET 
COVER PROBLEM 
In this section, we introduce the Minimum Consistent Subset Cover (MCSC) problem and study its properties, which provide important insights for our algorithm design. 3.1 Deﬁnition 
The MCSC problem ﬂnds the minimum number of con- sistent subsets that cover a given set of elements, where a subset is consistent if it satisﬂes a given constraint. 
Deﬂnition 1. (Minimum Consistent Subset Cover) Given a ﬂnite ground set X and a constraintS t, ﬂnd a collection C of consistent subsets of X with 
S2C 
S = X such that jCj is minimized, where a subset is consistent if it satisﬂes t. 
We use a tuple (X; t) to denote an MCSC instance with ground set X and constraint t. The consistent subset cover number for (X; t), denoted by (X; t), is the minimum num- ber of consistent subsets with respect to t that cover X. 
311 

========2========

Research Track Paper 
Given (X; t), we say the constraint t is granular if fxg is consistent with respect to t for any x 2 X. Apparently, with a granular constraint, (X; t) always has a non-empty feasible region. Most reasonable MCSC formulations feature granular constraints. For example, in the MCP problem, a single vertex also forms a clique. 
The set covering problem can be considered as an MCSC instance where a subset is consistent if it is given. Consistent subsets are not explicitly given in typical MCSC instances. If we take a pre-processing step to generate all the consis- tent subsets, then an MCSC instance becomes a set covering instance. Unfortunately, as argued in [7], the generated col- lection of subsets would be prohibitively large and this is not a feasible approach to solve MCSC instances. 3.2 Properties 
Covering problems have corresponding partitioning prob- lems as special cases. Disallowing overlapping, partition- ing problems are easier in the sense that they have smaller search spaces. Algorithms for partitioning problems usually work for the associated covering problems as well but typ- ically generate solutions of larger sizes. However, ﬂnding partitions can be an advantageous way of ﬂnding covers for MCSC instances under certain conditions. 
Deﬂnition 2. (anti-monotonic constraint) Given (X; t), we say t is anti-monotonic if for any subset S  X that is con- sistent, any S0  S is also consistent. 
For example, the MCP problem has an anti-monotonic constraint since a subset of a clique is still a clique. As to be shown, many practical data mining problems are also MCSC instances with anti-monotonic constraints, such as the Min- imum Rule Set and converse pairwise clustering problems. 
Theorem 1. Given (X; t) where t is anti-monotonic, any solution to (X; t) can be transformed into a solution to the associated partitioning problem with the same cardinality. 
Proof. We give a constructive proof of the theorem. Sup- pose we have a solution to (X; t) at hand which is a set of overlapping consistent subsets. For each pair of consistent subsets that overlap, we can simply assign the overlap to any of the two and remove it from the other. Since t is anti- monotonic, the consistent subset with the overlap removed remains consistent. Then, we obtain a set of consistent sub- sets of the same cardinality that form a partition of X. 
Theorem 1 implies that for (X; t) where t is anti-monotonic, optimal or good partitions are also optimal or good covers. By targeting the simpler partitioning problem, we may de- sign well-guided yet e–cient search heuristics for (X; t). 
In the following, we deﬂne the so-called consistency graph for (X; t), based on which we connect the MCSC problem to the MCP problem and derive lower-bounds for (X; t). 
Deﬂnition 3. (consistency graph) Given (X; t), a consis- tency graph for (X; t), Gc = (Vc; Ec), is a simple graph where Vc = X, and there is an edge (u; v) 2 Ec for a pair of vertices u; v 2 Vc if and only if fu; vg is consistent. 
Observation 2. Given (X; t) where t is anti-monotonic, a consistent subset forms a clique in Gc, where Gc is the con- sistency graph for (X; t). 
The observation is rather straightforward. Since t is anti- monotonic, any pair of elements in a consistent subset must 
also constitute a consistent subset and the two correspond- ing vertices in Gc will share an edge connection. 
The observation implies that a feasible solution to (X; t) is also a feasible solution to the MCP instance on Gc, thus the feasible region for (X; t) is a subset of the feasible re- gion for the MCP instance, which implies that an optimal solution to the MCP instance must be an optimal solution to (X; t). Therefore, the consistent subset cover number (X; t) is lower-bounded by the clique partition number ´(Gc), which is further lower-bounded by the independence number ﬁ(Gc) as we have discussed in the preliminaries. 
Theorem 2. ﬁ(Gc) • ´(Gc) • (X; t), where Gc is the consistency graph for (X; t) and t is anti-monotonic. 
Based on Theorem 2, since ﬁ(Gc) is the size of a maximum independent set in Gc, the size of any independent set must be less than or equal to any feasible solution to the MCSC (or MCP) problem. If the two are equal, then the solution is optimal for both. This implication has been used to conﬂrm the optimality of some of our experimental results. 3.3 Extensions 
In the following, we relax the conditions speciﬂed in the properties established above so that they can be extended to more applications. For example, converse k-center does not come with an anti-monotonic constraint. The removal of cluster centers may corrupt the consistency of clusters. 
Deﬂnition 4. (pivot-anti-monotonic constraint) Given (X; t), we say t is pivot-anti-monotonic if for any subset S  X that is consistent, there exists a pivot element p 2 S such that S0 [ fpg is consistent for any S0  S. 
Example 1. Let us consider (X; t) for the converse k-center problem, where t requires each cluster to have a radius no larger than a given threshold. t is pivot-anti-monotonic with cluster centers as pivots. As long as the center remains, any sub-cluster of a consistent cluster remains consistent. 
Obviously, if t is anti-monotonic, t must be pivot-anti- monotonic as well. A consistent subset could have multiple pivots. A pivot of a consistent subset may be present in another as a non-pivot element. The concept of pivot can be extended from a single element to a set of elements, however, we keep the case simple in this study. 
Theorem 3. Given (X; t) where t is pivot-anti-monotonic, and given that S1[S2 is consistent if S1 and S2 are consistent subsets sharing the same pivot, any solution to (X; t) can be transformed into a solution to the associated partitioning problem with the same or smaller cardinality. 
Proof. We give a constructive proof of the theorem. Sup- pose we have a solution to (X; t) at hand which is a set of overlapping consistent subsets. We ﬂrst merge all the con- sistent subsets sharing the same pivot resulting in a set of consistent subsets such that no two of them share the same pivot. Now, even if a pivot may still appear in an over- lap of two consistent subsets, it is a pivot for one of them but not both. We just need to make sure the pivot is as- signed to the one that needs it. We assign all the non-pivot elements in an overlap to either of the two overlapping con- sistent subsets and remove them from the other. Since t is pivot-anti-monotonic, the consistent subset with the non- pivot elements removed remains consistent. Then, we get a set of consistent subsets of (X; t) of the same or smaller cardinality that form a partition of X. 
312 

========3========

We also deﬂne the so-called pseudo consistency graph for (X; t), based on which a similar observation as in Obser- vation 2 and a similar conclusion as in Theorem 2 can be obtained by following similar arguments. 
Deﬂnition 5. (pseudo consistency graph) Given (X; t), a pseudo consistency graph for (X; t), G0c = (V 
0c 
; E0c), is a sim- ple graph where V 
0c 
= X, and there is an edge (u; v) 2 E0c for a pair of vertices u; v 2 V 
0c 
if and only if there exists p 2 X such that fu; v; pg is consistent. 
Observation 3. Given (X; t) where t is pivot-anti-monotonic, a consistent subset forms a clique in G0c, where G0c is the pseudo consistency graph for (X; t). 
Theorem 4. ﬁ(G0c) • ´(G0c) • (X; t), where G0c is the pseudo consistency graph for (X; t) and t is pivot-anti- monotonic. 
In this paper we focus on MCSC instances with anti- monotonic (or pivot-anti-monotonic) constraints. The the- oretical results established above as well as our proposed algorithm CAG can be applied to such instances. In the fol- lowing, we introduce several practical data mining problems that have MCSC formulations of this kind. 
4. DATA MINING APPLICATIONS 
In this section, we discuss several practical data mining applications that can be formulated as Minimum Consistent Subset Cover (MCSC) instances, in particular, the Mini- mum Rule Set (MRS) problem for rule learning, converse k-clustering, and frequent pattern summarization. 4.1 The Minimum Rule Set Problem 
The MRS problem ﬂnds a disjunctive set of if-then rules with the minimum cardinality that cover a given set of la- beled examples completely and consistently. A rule covers an example if the attribute values of the example satisfy the conditions speciﬂed in the antecedent (if-part) of the rule. 
Deﬂnition 6. (Minimum Rule Set) Given a set X of la- beled examples of multiple classes, ﬂnd a complete and con- sistent set R of propositional rules for X, i.e., for each e 2 X, there exists some r 2 RS that covers e and for each r 2 RS, all examples covered by r must have the same class label, such that jRSj is minimized. 
As the most human-comprehensible classiﬂcation tool, de- cision rules play a unique role in both research and appli- cation domains. In addition to data classiﬂcation, rules can also be used for the purpose of data description as deci- sion trees [17]. Data description focuses on existing data instead of unseen data as in classiﬂcation, seeking to reduce the volume of data by transforming it into a more compact and interpretable form while preserving accuracy. For both applications, simpler models are preferred. By the widely applied principle of Occam’s Razor, simpler models tend to generalize better to unseen data. From the understandabil- ity point of view, simpler models provide more compact and concise descriptions that are easier to comprehend. 
Decision trees can be used to extract a set of mutually exclusive rules [20]. The optimal decision tree problem has been studied to induct a perfect tree that correctly clas- siﬂes all the given examples with some objective optimized (e.g., [16]). While various objectives have been investigated, a common one is to minimize tree complexity, which can be 
Research Track Paper 
measured by the number of leaf nodes. The problem we study, MRS, can be accordingly referred to as an optimal rule set problem with an objective of the same kind. 
Another popular measure for tree complexity is the total number of tests (internal nodes), which corresponds to the total number of conditions in a rule set. The two measures tend to agree to each other. In the rule set case, fewer number of rules usually lead to fewer number of conditions, as demonstrated in our experimental study. 
Most rule learners, e.g., the AQ family, CN2 and RIP- PER [5], also implicitly reduce the complexity of rule sets in order to achieve good generalization accuracy. However, as pointed out by [11], the minimality of rule sets has not been a \seriously enforced bias", which is also evident in our experimental comparison study. 
The MCSC formulation. The MRS problem can be formulated as an MCSC instance (X; t), where X is the given example set and t requires a subset S  X to form a consistent rule. In particular, S is consistent if its bounding box contains no examples of other classes. Since any sin- gle example forms a consistent rule, t is granular and (X; t) has a non-empty feasible region. In addition, any rule that covers a subset of the examples covered by a consistent rule is also consistent, thus t is also anti-monotonic, and the re- sults in Theorem 1 and Theorem 2 are directly applicable. Recall that Theorem 2 gives ﬁ(Gc) • ´(Gc) • (X; t). In the following, we present an additional MRS-speciﬂc result. 
Lemma 1. Given (X; t) for the MRS problem, where X is in d-dimensional space with d • 2, a clique in Gc forms a consistent subset of (X; t). 
To prove the lemma, it su–ces to prove the combination of the bounding boxes of all pairs of vertices in a clique covers the bounding box for the clique, which can be proved by induction on the number of vertices. Due to the page limit, we omit the full proof, which can be found in [6]. Based on Lemma 1, the following theorem immediately follows. 
Theorem 5. Given (X; t) for the MRS problem, where X is in d-dimensional space with d • 2, ´(Gc) = (X; t). 
Lemma 1 and Theorem 5 do not hold for d > 2. We construct a counter example for d = 3. Let X = fa; b; c; eg where a = (2;4;5), b = (4;3;2), c = (7;9;4), and e = (3;5;3). Let e be the only negative example. Since e is not covered by the bounding box of any pair of examples in S = fa; b; cg, S is a clique in Gc. However, e is covered by the bounding box of S, thus S is not consistent. 
Approximate models are useful for both data classiﬂcation (to avoid overﬂtting) and data description (to improve inter- pretability) applications of decision rules. For e–ciency, it is desirable for rule learners to generate approximate mod- els during the induction process, instead of post-processing, while keeping its ability to generate perfect models. The MCSC formulation of the MRS problem provides this  bility, where we can simply adjust t and allow each rule to contain a maximum number of examples of other classes. This constraint is anti-monotonic. 
The RGB rule learner. By solving (X; t), we obtain a solution to the MRS problem containing a set of bounding boxes. The bounding boxes, which we call rectangles in the following, are a special type of rules with all the attributional conditions speciﬂed. To learn a set of compact rules (of the same cardinality) for the MRS problem, we propose the 
313 

========4========

Research Track Paper 
Algorithm 1 RGB 
Input: X, t, and b: X is the example set, a set of multi- 
class examples. t is a constraint requiring rules to be 
consistent. b is a user-speciﬂed beam search width. Output: R: a compact rule set with redundancy removed. 1: R ˆ CAG(X; t); //R stores a set of rectangle rules 2: for each r 2 R 
3: initialize b empty rules r01; r02; :::; r0b; 
4: initialize b sets of examples E1; E2; :::; Eb; 
5: while (E1 6 = ; ^ E2 6 = ; ::: ^ Eb 6 = ;)0 
6: for each r 
i, choose the top 
b conditions from r, each 
being added to r0i, to form b candidates that elimi- 
nate the most examples from Ei; 
7: choose the top b rules from the b £ b candidates as 
r01; r02; :::; r0b and update E1; E2; :::; Eb accordingly; 8: end while 
9: r ˆ r0j suppose Ej = ; caused the loop to terminate; 10: end for 
rectangle-based and graph-based algorithm RGB that takes two steps. First, we focus on rectangle rules only and solve the formulated MCSC instance. Then, we remove redundant conditions in the rectangle rules. A condition is considered redundant for a rule if its removal will not cause the inclusion of any new examples of other classes or the exclusion of any examples of the same class previously covered by the rule. 
As shown in Algorithm 1, RGB ﬂrst calls CAG (fully explained in Section 5) for the MCSC instance (X; t) and stores the set of learned rectangle rules in R. Then for each r 2 R, a general-to-speciﬂc beam search is performed to remove redundant conditions. To explain the idea, we con- sider a beam search of width 1. We initialize and maintain a set of examples of other classes, the elimination set for r, that need to be eliminated to achieve consistency. The search starts with an empty rule, i.e., true, the most general rule, and the conditions to add are chosen from the orig- inal conditions in r. For the choice of condition to add, a greedy approach is adopted favoring the condition excluding the largest number of examples from the elimination set of r. The search stops when the elimination set is empty. 
Since the conditions to add are chosen from the original conditions in the input rules, the extracted rules are more general than their original ones. Since the consistency of each rule is also guaranteed, the resulting compact rule set R is complete and consistent. The runtime of RGB is dom- inated by CAG, which we will discuss in Section 5. 
As a rule learner, RGB has many unique and/or desirable properties. Unlike most existing methods, RGB does not follow a separate-and-conquer approach, instead, all rules are learned simultaneously. Unlike most existing bottom-up methods that start the initial rule set with the example set, RGB starts with a subset containing a maximal number of examples such that no pair of them can co-exist in a single consistent rule, which constitutes a maximal optimal partial solution. Unlike many existing methods that learn a set of rules for one class at a time, RGB naturally learns a set of rules for all classes simultaneously. Unlike many existing methods that can only learn either perfect models such as early members of the AQ family, or approximate models such as CN2 and RIPPER, RGB has the y to learn both without resorting to post-processing. 
4.2 Converse k-clustering 
k-clustering methods generate k clusters that optimize a certain compactness measure, typically distance-based, that varies among diﬁerent clustering models. While some mea- sures use the sum or average of (squared) distances as in k-means and k-medoid [14], some measures use a single dis- tance value, radius or diameter, as in k-center [21] and pair- wise clustering [4]. The radius of a cluster is the maximum distance between a ﬂxed point (center) and any point in the cluster, and the diameter is the maximum distance between any two points in the cluster. 
A known limitation of k-clustering is that the appropri- ate number of clusters k is often hard to specify in advance, and methods that try to automatically determine this num- ber have been investigated [19]. As another approach to address this limitation, alternative clustering models have been explored. In converse k-clustering, a compactness mea- sure is given as threshold, and the task is to minimize the number of clusters k. Converse k-clustering models have not received much attention in the data mining community. However, in many applications a distance-based constraint is easier to provide, based on domain knowledge, than the number of clusters. For example, molecular biologists have the knowledge that how similar a pair of sequences should be so that the two proteins can be assumed sharing the same functionality with high probability, or businessmen have the knowledge that how similar two customers should be so that they would have similar purchasing behaviors. The facility location problem [15], extensively studied in the operations research community, has the general goal of minimizing the total cost of serving all the customers by facilities. One of the problem formulations minimizes the number of located facilities based on the pairwise distance knowledge of cus- tomers, which is precisely a converse k-clustering model. 
The converse k-center and converse pairwise clustering models can both be formulated as MCSC instances. In such an instance (X; t), X is the input data points for clustering and t requires a cluster to satisfy the maximum radius or diameter threshold constraint. As single distance measures, radius and diameter are more intuitive for domain experts to specify constraints than the more complex ones. 
Both MCSC instances have granular constraints since sin- gleton clusters satisfy any distance threshold. As explained in Example 1, converse k-center has a pivot-anti-monotonic constraint with cluster centers as pivots. For converse pair- wise clustering, the constraint is anti-monotonic since any sub-cluster of a cluster can only have an equal or smaller di- ameter than the cluster. The results established in Section 3 apply to the two MCSC instances. Note that in converse k- center, a union of two consistent clusters sharing the same center is also consistent, thus the additional condition re- quired by Theorem 3 is satisﬂed and the theorem applies. In addition, we have the following observation. 
Observation 4. Given (X; t) for the converse pairwise clus- tering problem, we have ´(Gc) = (X; t). 
Based on Observation 2, a consistent subset of (X; t) forms a clique in the consistency graph Gc. On the other hand, a clique in Gc must form a consistent subset, thus converse pairwise clustering is equivalent to the MCP problem on Gc. The same observation does not hold for the converse k-center problem. 
314 

========5========

4.3 Pattern Summarization 
Frequent pattern mining has been studied extensively for various kinds of patterns including itemsets, sequences, and graphs. While great progress has been made in terms of e–ciency improvements, interpretability of the results has become a bottleneck to successful application of pattern mining due to the huge number of patterns typically gen- erated. A closely related problem is that there is a lot of re- dundancy among the generated patterns. Interpretable and representative summarization of large collections of patterns has emerged as an important research direction. 
As a ﬂrst approach, maximal frequent patterns [9] and closed frequent patterns [18] have been introduced. These subsets of frequent patterns are more concise and allow to derive all the frequent patterns. However, the patterns thus generated are still too many to handle. As an alternative, [10] ﬂnds the top-k frequent closed patterns of length no less than a given threshold. While this approach eﬁectively limits the output size, the returned patterns are often not representative of the entire collection of frequent patterns. Moreover, these approaches fail to address the redundancy problem. 
Some recent work aims at ﬂnding a ﬂxed number of pat- terns representing the whole set of frequent patterns as well as possible. In [1], the objective is to maximize the size of the part of the input collection covered by the selected k sets. [23] presents an approach that takes into account not only the similarity between frequent itemsets, but also be- tween their supports. Using a similarity measure based on Kullback-Leibler divergence, they group highly correlated patterns together into k groups. 
Similar to the scenario for k-clustering, the appropriate number k of patterns to summarize the set of frequent pat- terns is often hard to specify in advance. However, the users may have the domain knowledge that how similar a group of patterns should be so that they can be represented as a whole without losing too much information. In light of this, some converse k-clustering models that can be formu- lated as MCSC instances, converse k-center or converse pair- wise clustering, appear to be very reasonable and promising to provide concise and informative pattern summarizations. Such clustering models generate clusters, i.e., groups of pat- terns, with certain quality guarantee. In such a formulation, the objective is to minimize the number of pattern groups necessary to cover the entire collection of frequent patterns, which is natural for the purpose of summarization since it maximizes interpretability of the result representing the en- tire collection and at the same time reduces redundancy. For the radius or diameter threshold, standard distance func- tions can be employed, such as the Jaccard’s coe–cient for itemsets or edit distance for sequential patterns. 
5. THE GENERIC CAG ALGORITHM 
In this section, we introduce a generic algorithm CAG that works with consistency graphs and assignment graphs to solve Minimum Consistent Subset Cover (MCSC) instances featuring anti-monotonic constraints. 
5.1 Overview 
Given an MCSC instance (X; t) where the constraint t is anti-monotonic, CAG starts by constructing a maximal optimal partial solution from the consistency graph Gc, then performs an example-driven speciﬂc-to-general search on a 
Research Track Paper 
dynamically maintained bipartite assignment graph Ga to learn a set of consistent subsets with small cardinality. 
The design of CAG has closely followed the insights pro- vided in Section 3. From the deﬂnition of Gc and Theorem 2, we know that any pair of vertices in an independent set of Gc cannot appear in the same consistent subset. Thus a maximal independent set of Gc, denoted by IS, constitutes a maximal optimal partial solution to (X; t). 
Each vertex in IS forms a singleton consistent subset, represented by a so-called condensed vertex in an assignment graph Ga. The rest of the vertices in Ga are called element vertices. Ga is deﬂned such that there is an edge between an element vertex u and a condensed vertex v if and only if u can be assigned to v while maintaining the consistency of v. Since a maximal independent set is also a dominating set as discussed in the preliminaries of Section 2, there is an edge for each u connecting to some v in the initial Ga. 
Element vertices are processed in a sequential manner and assigned to condensed vertices. With the growth of con- densed vertices, some element vertices would get isolated and new condensed vertices have to be created for them. Upon completion, Ga becomes edge-free with all vertices as- signed, and the set of condensed vertices, each representing a consistent subset, constitute a solution for (X; t). 
The dynamically maintained Ga provides the necessary information based on which eﬁective evaluation measures can be designed to guide the search by deciding which ele- ment vertex is the next to be assigned to which condensed vertex. For example, with the least degree ﬂrst criterion, we can process ﬂrst the element vertex with the least degree since it is the most likely one to get isolated. 
Note that CAG actually returns a partition of X. As ar- gued in Section 3, a solution to a partitioning problem is also a feasible solution to its corresponding covering prob- lem. Also due to Theorem 1 and Theorem 3, partitioning does not cause the increase of solution size compared to covering under certain conditions. In addition, partitioning problems have much smaller search spaces and the search can be better guided. 
Also note that under anti-monotonic constraints, if a sub- set S  X is not consistent, S0 ¶ S cannot be consistent. Thus such inconsistent S does not need to be considered and the search space can be signiﬂcantly pruned. Our as- signment graph Ga maintains consistency of all condensed vertices after each assignment transaction allowing CAG to work in a pruned search space. 
In contrast to the most-seen separate-and-conquer ap- proach, e.g., the greedy algorithm [12] for set covering and most existing rule learners, CAG adopts a less greedy example- driven strategy to learn consistent subsets simultaneously. In summary, the construction of initial optimal partial solu- tion, the use of assignment graph, and the example-driven simultaneous learning strategy are the three novel design ideas that account for the good performance of CAG. 5.2 Assignment graph 
In the following, we deﬂne and explain assignment graphs, a unique element of CAG that helps to guide the search. 
Deﬂnition 7. (assignment graph) In a bipartite assign- ment graph Ga = (Ua [ Va; Ea), Ua contains a set of ele- ment vertices and Va contains a set of condensed vertices each representing a consistent subset. (u; v) 2 Ea if and only if u 2 Ua, v 2 Va, and v [ fug is consistent. 
315 

========6========

Research Track Paper 
In CAG, Ga is dynamically maintained showing all the feasible choices for the next assignment. Each condensed vertex is essentially a set of vertices condensed together. In order to maintain the consistency of subsets, an element ver- tex u can be assigned to some condensed vertex v only via an edge connection between them. However, each assign- ment may cause some edges to disappear in Ga. For those isolated element vertices (with degree of 0), new condensed vertices have to be created. Each creation may introduce some new edges to Ga. 
Figure 1 (c) shows an initial Ga and (d)-(f) show its dy- namic evolvement along the assignment process, for which we will provide further explanations later in this section. 
Observation 5. While a vertex assignment may cause mul- tiple edge gains or losses in Ga, it can cause at most one edge gain or loss for any single element vertex. 
The observation holds because all the gained or lost edges have to be incident to the condensed vertex involved in an assignment, and any element vertex shares at most one edge with a condensed vertex. The implication of the observation is that if an element vertex has a degree of more than two, it will not become isolated after the next assignment. 
Evaluation measures. In the following, we show how the information embedded in Ga can help to guide the search by answering important questions in the assignment process: Which element vertex is the next to be assigned? Which condensed vertex should it be assigned to? In principle, the element vertex with the least degree should be considered ﬂrst since it is most likely to get isolated. Also based on Observation 5, element vertices with degree more than two will stay \safe" after such an assignment. Thus, a least degree ﬂrst criterion can be used to choose the next element vertex to be assigned. 
In addition, we want to keep as many edges as possible in Ga since they are the relational resources for the assignment of element vertices. Edges have diﬁerent degrees of impor- tance. In general, a gained or lost edge is more important if it is incident to an element vertex with a smaller degree in Ga. We design a measure, weighted edge gain, that re the in of an assignment transaction on Ga. 
In the following, we deﬂne weg(uu) and weg(uv). weg(uu) is the weighted number of edges that can be added to Ea if u is chosen to become a new condensed vertex. In such cases, u is assigned to itself, which happens only when d–(u) = 0, where d–(u) denotes the degree of u in Ga. weg(uv) is the weighted number of edges that would get lost (an edge loss is just a negative gain) if u is assigned to v via edge (u; v) 2 Ea. For weg(uu), the weight for each possibly added edge is 
10 
d–(u )+1 
where we use d–(u0) + 1 because d–(u0) can be 0 and for that case, the weight should be the largest, i.e., 1. For both measures, diﬁerent weighting schemes can be used. 
X 
weg(uu) = 
1 
d–8u02Uas:t:fu0 (u0) + 1;ugis consistent 
X 
weg(uv) = 
¡1 
d–8u0s:t:(u0;v)2Ea^ (u0);ugis inconsistent 
v [ fu0 
Once u, the next element vertex to be assigned, is chosen, a largest weg(uv) ﬂrst criterion can be used to decide which condensed vertex v that should take u so as to keep as many weighted edges as possible. 
We also deﬂne the following measure weg(u) that mea- sures the in on Ga if u is chosen as the next element vertex to be assigned. The associated largest weg(u) ﬂrst criterion can be used alone for the choice of u. It can also be used as a tie-breaking mechanism for the least degree ﬂrst criterion in choosing u.8 
< weg(uu) if d–(u) = 0 weg(u) = 
: 
weg(uv) where v = 
argmax8v0s:t:(u;v0)2Eafweg(uv0)g otherwise 
In the formula, for d–(u) = 0, weg(u) = weg(uu). For other cases, weg(u) is the largest weg(uv) value considering all the condensed vertices adjacent to u in Ga, indicating the best assignment choice for u. 
5.3 Generic CAG 
We have explained the working principles of CAG in the overview. In the following we present the algorithm and explain it in more details. 
Algorithm 2 generic CAG 
Input: (X; t): an MCSC instance. 
Output: Va: a set of condensed vertices representing a set 
of consistent subsets that cover X. 
1: initialize Ga = (Ua [ Va; Ea); 
2: while (Ua 6 = ;) 
3: choose u 2 Ua;– 
4: if (d (u) = 0) then 
5: Va ˆ Va [ fug; 
6: Ua ˆ Uanfug; 
7: Ea ˆ Ea [ f(u; u0)g for each u0 with u0 2 Ua and0 
fu; u g is consistent; 
8: else 
9: choose v 2 Va; 
10: v ˆ v [ fug; 
11: Ua ˆ Uanfug; 
12: Ea ˆ Eanf(u; u0)g for each u0 with (u0; v) 2 Ea0 
and v [ fu; u g is inconsistent; 
13: end if 
14: end while 
CAG starts by initializing the assignment graph Ga using a maximal independent set of Gc (line 1). Then, the element vertices are processed in a sequential manner (line 2). For each assignment, a decision is made on which u 2 Ua is the next element vertex to be assigned (line 3). After u is chosen, if it has a degree of 0 (line 4), a new condensed vertex has to be created for u (line 5) and u will be removed from Ua (line 6). At this moment, Ea needs to be updated (line 7). Some element vertices (that are connected to u in Ec) may be able to connect to the newly added condensed vertex u resulting in creation of some new edges. If u is not an isolated vertex (line 8), another decision is to be made on which v 2 Va should take u (line 9). After the assignment of u to v (lines 10, 11), Ea also needs to be updated (line 12). Because0 of the assignment of u to v, some element vertices, say u , that previously connected to v would lose the connection if v [ fu; u0g is inconsistent. Upon completion, Ua = ; and Ea = ;. The set of condensed vertices Va corresponds to a set of consistent subsets together covering X. 
Initializing Ga. One way of initializing Ga is to derive the consistency graph Gc for (X; t) ﬂrst then ﬂnd a maximal independent set IS of Gc using some known heuristic, e.g., 
316 

========7========

Greedy-IS introduced in [13]. With IS obtained, we let Va = IS, Ua = VcnIS, and (u; v) 2 Ea if and only if u 2 Ua, v 2 Va and (u; v) 2 Ec. 
In some cases, e.g, for the MCP problem, Gc is identical to the input graph. In other cases, Gc needs to be derived and2 
this takes O(jXj tp) time where tp denotes the consistency checking time for a pair of vertices in X. This is because each pair in X needs to be checked for consistency. To improve e–ciency, we provide another way of initializing Ga without actually deriving Gc. Initially, Va and Ea are empty and Ua = X. For each u 2 Ua, we check each v 2 Va and if v[fug is consistent, we add edge (u; v) into Ea. If no edge can be added, we add u into Va. Upon completion, Va also contains a maximal independent set IS of Gc. This procedure takes O(jISjjXjtp). Note that usually jISj ¿ jXj. 
Choosing u and v. By varying the decision making schemes, i.e., how to choose u (line 3) and v (line 9), the generic CAG can have diﬁerent instantiations that are suit- able for diﬁerent applications. By default, we use the least degree ﬂrst criterion to choose u with the largest weg(u) ﬂrst criterion for tie-breaking. Also, we use the largest weg(uv) ﬂrst criterion to choose v. 
With the default scheme, the time spent in choosing u would be on calculating weg(u) for tie-breaking. As an ap- proximation, we adopt a sampling technique. We only con- sider a constant number of element vertices with the tied least degree and for each of them, say u, we only consider a constant number of element vertices for consistency check- ing, each of which, say u0, connects to some condensed ver- tex v that is adjacent to u in Ga. This sampling technique works well because the least degree ﬂrst criterion greatly re- duces the number of vertices in consideration in calculating weg(u). Therefore, the runtime for the choice of u is O(tc),0 where tc is the consistency checking time for v[fu; u0 g given that v [ fug and v [ fu g are consistent. 
For the choice of v, the calculation of weg(uv) is part of the calculation of weg(u) and thus it takes O(tc) as well. 
Updating Ga. By varying the Ga updating mechanisms (lines 7 and 12), the generic CAG can be adapted to diﬁerent applications. These applications have diﬁerent constraints requiring diﬁerent consistency checking mechanisms. 
In line 7, the edge set Ea of Ga is updated after an isolated element vertex u becomes a new condensed vertex. This may introduce some new edges to Ea. Note that line 7 will be executed no more than jVaj ¡ jISj times where Va here represents its ﬂnal stage after CAG terminates, thus the total time spent on line 7 is at most O((jVaj ¡ jISj)jXjtp). 
In line 12, the edge set Ea of Ga is updated after an ele- ment vertex u is assigned to a condensed vertex v. This may cause the loss of some edges incident to v. Each such update could take a worst case runtime of O(jXjtc). To reduce the runtime, we adopt a late update strategy that signiﬂcantly improves e–ciency without sacriﬂcing much performance. This is based on the observation that the edge update oper- ations do not have an impact on the assignment process if the corresponding element vertices are not considered in the next assignment. Given the least degree ﬂrst criterion, we only need to perform edge update for those element vertices with the least (or small) degrees since they are likely to be considered in the next assignment. This late update strat- egy would leave some extra edges in Ga that should have been removed. However, these edges will be removed sooner or later when their corresponding member vertices get closer 
.. 
. . 
.. 
. 
.. 
. 
. 
. 
.. 
. 
. 
. . ... . 
. . . .. 
. . 
Research Track Paper 
a 
d 
a 
d 
a 
d 
b 
e 
b 
e 
b 
{ }e 
c 
f 
c 
f 
{ }c 
f 
(a) 
(b) 
(c) 
a 
a 
{ }e,d 
{ }e,d,f 
{ }e,d,f 
b 
b 
{ }c,b,a 
{ }c 
f 
{ }c 
(d) (e) (f) 
Figure 1: Running example. 
and closer to be chosen along the assignment process. There- fore, by checking consistency for a constant number of ver- tices connected to v, we only spend O(tc) time for this up- date without sacriﬂcing much performance. 
Time complexity. As we have explained previously, initializing Ga (line 1) takes O(jISjjXjtp). The update of– 
Ga for the d (u) = 0 case (line 7) takes in total O((jVaj ¡ jISj)jXjtp) time. Together they take O(jVajjXjtp) time. 
In each iteration, the choice of u (line 3), the choice of v (line 9) and the update of Ga for the d–(u) 6 = 0 case each takes tc time. There are O(jXj) iterations and thus together they take O(jXjtc) time. 
Overall, the worst case runtime is O(jVajjXjtp + jXjtc). In general, jVaj ¿ jXj as can be observed from our experi- mental results in Table 1. 
The consistency checking time tp and tc are diﬁerent for diﬁerent mechanisms and diﬁerent applications. For the MCP problem and the converse pairwise clustering problem, both tp and tc are constant. Thus, the worst case runtime for the two problems is O(jVajjXj). 
For the MRS problem, both tp and tc are O(jXj) for the brute-force consistency checking mechanism, which can be improved to log time on average by employing indexing tech- niques. The worst case runtime for the MRS problem is O(jVajjXj2). With the help of indexing, the average case runtime is O(jVajjXjlogjXj). 
Running example. In Figure 1, we provide a running example demonstrating how CAG works on a set covering instance (X; t). By deﬂnition, (X; t) does not have an anti- monotonic constraint since we are allowed to use the given subsets, not their subsets, in covering X. However, if S0 is a subset of some given subset S  X, S0 can be considered \consistent" since it can always be replaced by S. Thus, our generic algorithm CAG is perfectly applicable. 
In the ﬂgure, the (X; t) instance is given in (a). We can see that the greedy algorithm will select all the three given subsets. For clarity, we also show Gc for (X; t) in (b), where we can identify a maximal IS containing c and e. (c) is the initial Ga with c and e as the condensed vertices. 
Now we start the assignment process. Both d and f have the least degree of 1, but assigning d to feg would not cause any edge loss, thus d is chosen and assigned to feg, and (d) shows the updated Ga. Next, f is chosen and assigned to fe; dg, and (e) shows the updated Ga. We can see that Ga lost two edges since neither a nor b can join fe; d; fg while 
317 

========8========

Research Track Paper 
maintaining its consistency, i.e., fe; d; f; ag and fe; d; f; bg are inconsistent. Afterwards, a and b are assigned to fcg and (f) shows the ﬂnal Ga, where both Ua and Ea are empty and Va contains the condensed vertices corresponding to the consistent subsets that cover X. 
Solving converse k-center. Given (X; t) for the con- verse k-center problem, we have explained previously that t is pivot-anti-monotonic. Also, (X; t) satisﬂes the additional condition speciﬂed in Theorem 3, that is, two consistent sub- sets of (X; t) sharing the same pivot can merge into a single consistent subset. Based on the theoretical study, we can use the following approach to solve (X; t). First, we apply CAG where we use the pseudo consistency graph G0c in- stead of Gc to derive Ga. Then we assign the needed pivots to the learned condensed vertices to obtain consistent sub- sets. Then we can obtain a partition of X by following the constructive method described in the proof of Theorem 3. 
6. EXPERIMENTAL EVALUATION 
To experimentally evaluate the performance of CAG, we have considered two MCSC applications, the Minimum Rule Set (MRS) and Minimum Clique Partition (MCP) problems. For the MRS problem, we compared CAG with a popular rule learner AQ21 [22], the latest release of the famous AQ family, on UCI datasets [2]. In this series of experiments, RGB achieved about 40% reduction in the number of rules and 30% reduction in the number of conditions. 
Observation 2 and Theorem 2 show that the MCP prob- lem is related to all the MCSC instances within the scope of this study. The converse pairwise clustering problem, which can also be applied to frequent pattern summarization, is equivalent to the MCP problem on the consistency graph. Since MCP is a dual problem of graph coloring, many well- known graph coloring heuristics and benchmarks set up an ideal platform to evaluate the performance of CAG. The comparison partner we chose, DSATUR [3], is among the most popular ones with top performance. Experiments on over 100 DIMACS benchmarks [8] showed that CAG out- performed DSATUR in general, and in particular for the 70 hard datasets, CAG used 5% fewer cliques on average. 
6.1 MRS results 
RGB is our proposed rule learner that ﬂrst calls CAG to learn a set of rectangle rules and then extracts compact rules with redundant conditions removed. Our comparison part- ner, AQ21 [22], is the latest release of the famous AQ family. Other popular rule learners such as CN2 and RIPPER do not return perfect rule sets. Since most rule learners follow a separate-and-conquer approach originated from AQ, the performance of AQ21 can well represent the performance of most rule learners. 
Although RGB can work on both numerical and categor- ical data in principle, we have focused on numerical data in this implementation since rectangle rules provide good gen- eralization on numerical attributes only. Thus, 21 numerical datasets without missing values were chosen from the UCI repository [2] for this series of experiments. 
Table 1 presents the results. The columns in the table are dataset, ins (number of instances), dim (dimensionality), cla (number of classes), AQ21 (AQ21 results), RGB (RGB re- sults), and reduction (reduction achieved by RGB compared to AQ21, where reduction = 
AQ21 result ¡ RGB result 
AQ21 result 
). 
For the columns AQ21 and RGB, rul and con indicate the total number of rules and conditions respectively. The rul and con reductions are given under the reduction column. In addition, the columns jEcj, jISj and cliques indicate the edge set size, the maximal independent set size and the num- ber of cliques of Gc discovered by RGB respectively. 
From Table 1 we see that RGB achieved 37.1% and 30.8% averaged reductions over AQ21 in terms of the total num- ber of rules and conditions respectively. In the last row of the table, the reductions were calculated for the averaged number of rules and conditions, where we see that RGB achieved 44.2% and 51.4% reductions over AQ21 in terms of the averaged number of rules and conditions respectively. 
Consistent to Theorem 2, although many results are not optimal, each dataset in Table 1 exhibits the same trend, that is, jISj • cliques • RGB rul • AQ21 rul. As discussed in Section 3, Theorem 2 can help to conﬂrm the optimality of MCSC results. In Table 1 we can see that the IS, cliques and RGB results for datasets balance-scale, glass, and iris are optimal. Also, the IS and cliques results for ionosphere, new-thyroid, sonar, vowel, waveform, and wine are optimal. 
In addition, we studied the trend of reduction by varying data complexity measured by ins, jEcj and dim. While the detailed results can be found in [6], in summary, the reduc- tions in rul and con both increase with the increase of data complexity, which also conﬂrms that the two model com- plexity measures for rule sets, number of rules and number of conditions, are consistent with each other. 
6.2 MCP results 
The MCP problem is a dual problem of graph coloring. Algorithms for graph coloring (MCP) can be applied to com- plementary graphs to return solutions to the MCP (graph coloring) problem. In this series of experiments, we have used DSATUR [3] as our comparison partner. DSATUR is among the most popular construction heuristics for graph coloring with top performance reported in the literature. 
The experiments were performed on DIMACS benchmarks [8], for some of which the optimal solutions are known. We divide the benchmarks into two categories: easy and hard. As the rule of thumb for the division, those datasets with optimal solutions relatively easy to obtain are considered easy; otherwise hard. We used 35 easy and 70 hard datasets for our experiments. Due to the page limit, we only provide a summary of the results in Table 2. From the table we can see that out of the 35 easy datasets, DSATUR found 33 while CAG found all the 35 optimal solutions. 
category easy hard 
Table 2: MCP results # datasets DSATUR 
35 33 (optimal) 
70 41 (36 tied) 
CAG 35 (optimal) 65 (36 tied) 
The optimal solutions for many hard datasets are not known. Table 2 reports the winning times of the two meth- ods. From the table we can see that out of the 70 hard datasets, DSATUR won 41 times while CAG won 65 times where they tied for 36 datasets. To more accurately capture their performance diﬁerence, for each dataset, we calculated the reduction of cliques achieved by CAG using the formula reduction = 
DSAT UR result ¡ CAG result 
DSAT UR result 
. Then, we calcu- lated the averaged reduction over the 70 hard datasets to obtain a value of 4.99%, meaning that CAG used about 5% fewer cliques than DSATUR on average for each dataset. 
318 

========9========

dataset 
ins 
dim 
cla 
Table 1: MRS resultsAQ21 
jEcj jISj rul con 
balance-scale bupa car diabetes ecoli glass haberman ionosphere iris letter new-thyroid page-block satimage segment sonar spambase vehicle vowel waveform wine yeast average reduction 
625 345 1728 768 336 214 306 351 150 15000 215 5473 4435 2310 208 4601 846 990 5000 356 2968 
4 6 6 8 7 10 3 34 4 16 5 10 36 19 60 57 18 10 21 13 8 
3 2 4 2 8 6 2 2 3 26 3 5 6 7 2 2 4 11 3 3 10 
154 577 66 344 66 343 112 744 36 128 6 7 73 198 25 172 10 26 1160 15701 10 33 112 687 212 4379 49 382 11 186 118 2292 109 1133 76 583 266 4989 7 29 250 1740 139.4 1651.1 0 0 
We did not compare the runtime partly because CAG and DSATUR work on diﬁerent graphs, G and G, and the com- plexities of the two graphs are opposite to each other. How- ever, we note that while DSATUR has a cubic runtime [3], for the MCP problem, CAG has a subquadratic runtime of O(jVajjXj), where jVaj ¿ jXj (jVaj = cliques and jXj = ins) as can be observed from Table 1. 
7. CONCLUSION 
In this paper, we introduced and studied the Minimum Consistent Subset Cover (MCSC) problem, which general- izes the set covering problem and has many practical data mining applications in the areas of rule learning, converse k- clustering, and frequent pattern summarization. For MCSC instances with anti-monotonic constraints, our generic al- gorithm CAG was provided in accordance with the theo- retical insights featuring many novel design ideas, e.g., the construction of maximal optimal partial solution, the use of dynamically maintained bipartite assignment graph, and the example-driven simultaneous learning strategy. Our ex- perimental evaluation on benchmark datasets justiﬂed the performance of CAG. 
For future work, we would like to further explore and ex- perimentally evaluate the MCSC applications on converse k-clustering and frequent pattern summarization. We are also interested in seeking other data mining applications that can be formulated as MCSC instances and solved by our proposed CAG search framework. 
8. REFERENCES 
[1] F. Afrati, A. Gionis, and H. Mannila. Approximating a 
collection of frequent sets. SIGKDD’04. 
[2] C. Blake and C. Merz. UCI repository of machine learning 
databases. 1998. 
[3] D. Brelaz. New methods to color the vertices of a graph. 
Communications of the ACM, 22(4):251{256, 1979. 
Research Track Paper 
cliques 
31578 22149 386152 142994 13569 
5921 
6425 33058 
3562 3751964 12196 11754498 1784464 374323 10761 4686024 80643 43164 4164798 
5324 136416 1307142.0 
153 12 55 10 22 6 48 4 7 137 6 28 17 13 2 33 13 20 6 3 66 31.5 
153 19 57 18 24 6 52 4 7 221 6 37 20 15 2 38 15 20 6 3 117 40 
RGB rul con 153 676 38 224 57 340 57 420 28 131 
6 9 55 214 11 57 
7 19 552 6684 
7 31 60 369 99 1316 27 183 
5 79 64 1286 55 478 47 367 107 2347 
4 17 194 1606 77.8 802.5 0.442 0.514 
reduction rul con 0.006 -0.172 0.424 0.349 0.136 0.009 0.491 0.435 0.222 -0.023 0.000 -0.286 0.247 -0.081 0.560 0.669 0.300 0.269 0.524 0.574 0.300 0.061 0.464 0.463 0.533 0.669 0.449 0.521 0.545 0.575 0.458 0.439 0.495 0.578 0.382 0.370 0.598 0.530 0.429 0.414 0.224 0.077 0.371 0.308 
[4] M. Charikar, C. Chekuri, T. Feder, and R. Motwani. Increm- 
ental clustering and dynamic information retrieval. STOC’97. [5] J. Furnkranz. Separate-and-conquer rule learning. Artiﬂcial 
Intelligence Review, 13(1):3{54, 1999. 
[6] B. J. Gao. Hyper-rectangle-based discriminative data 
generalization and applications in data mining. Ph.D. Thesis. 
Simon Fraser University, 2006. 
[7] B. J. Gao and M. Ester. Turning clusters into patterns: 
Rectangle-based discriminative data description. ICDM’06. [8] D. B. Graphs. http://mat.gsia.cmu.edu/coloring02/index.html. [9] D. Gunopulos, H. Mannila, R. Khardon, and H. Toivonen. 
Data mining, hypergraph tranversals, and machine learning. 
PODS’97. 
[10] J. Han, J. Wang, Y. Lu, and P. Tzvetkov. Mining top-k 
frequent closed patterns without minimum support. ICDM’02. [11] S. J. Hong. R-MINI: An iterative approach for generating 
minimal rules from examples. TKDE, 9(5):709{717, 1997. [12] D. Johnson. Approximation algorithms for combinatorial 
problems. JCSS, 9:256{278, 1974. 
[13] D. Johnson. Worst-case behavior of graph-coloring algorithms. 
SEI CGTC’74. 
[14] L. Kaufman and P. Rousseeuw. Finding Groups in Data: An 
Introduction to Cluster Analysis. John Wiley & Sons, 1990. [15] J. Krarup and P. Pruzan. The simple plant location problem: 
survey and synthesis. EJOR, 12:36{81, 1983. 
[16] A. Kulkarni and L. Kanal. An optimization approach to 
hierarchical classiﬂer design. IJCPR’76. 
[17] S. Murthy. Automatic construction of decision trees from data: 
A multi-disciplinary survey. DMKD, 2:345{389, 1998. [18] N. Pasquier, Y. Bastide, R. Taouil, and L. Lakhal. Discovering 
frequent closed itemsets for association rules. ICDT’99. [19] D. Pelleg and A. Moore. X-means: Extending k-means with 
e–cient estimation of the number of clusters. ICML’00. [20] J. Quinlan. C4.5: Programs for Machine Learning. Morgan 
Kaufmann, 1993. 
[21] C. Toregas, R. Swain, C. Revelle, and L. Bergman. The 
location of emergency service facilities. Operations Research, 
19:1363{1373, 1971. 
[22] J. Wojtusiak, R. Michalski, K. Kaufman, and J. Pietrzykowski. 
Multitype pattern discovery via AQ21: A brief description of 
the method and its novel features. Technical Report, MLI 
06-2, George Mason University, 2006. 
[23] X. Yan, H. Cheng, J. Han, and D. Xin. Summarizing itemset 
patterns: A proﬂle-based approach. SIGKDD’05. 
319 

========10========

