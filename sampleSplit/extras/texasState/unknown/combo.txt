WWW 2010  Demo 
April 26-30  Raleigh  NC  USA 
Rants: A Framework for Rank Editing and Sharing 
in Web Search 
Byron J. Gao Texas State University-San Marcos 
601 University Drive 
San Marcos, TX, USA, 78666 
bgao@txstate.edu 
ABSTRACT 
With a Wiki-like search interface, users can edit ranks of search results and share the edits with the rest of the world. This is an e?ective way of personalization, as well as a practice of mass collaboration that allows users to vote for ranking and improve search performance. Currently, there are several ongoing experimentation e?orts from the indus- try, e.g., SearchWiki by Google and U Rank by Microsoft. Beyond that, there is little published research on this new search paradigm. In this paper, we make an e?ort to estab- lish a framework for rank editing and sharing in the context of web search, where we identify fundamental issues and pro- pose principled solutions. Comparing to existing systems, for rank editing, our framework allows users to specify not only relative, but also absolute preferences. For edit sharing, our framework provides enhanced ibility, allowing users to select arbitrarily aggregated views. In addition, edits can be shared among similar queries. We present a prototype system Rants, that implements the framework and provides search services through the Google web search API. Categories and Subject Descriptors: H.3.3 [Informa- tion Systems]: Information Storage and Retrieval { Infor- mation Search and Retrieval 
General Terms: Performance, Design, Algorithms Keywords: Rank editing, Edit sharing, Search interface, Personalization, Mass collaboration, Social search 
1. INTRODUCTION 
Recently, several web search giants are experimenting on a new Wiki-like search interface, where users can edit ranks of search results directly. For example, SearchWiki [3] by Google and U Rank [6] by Microsoft. This new search para- digm is an e?ective way of search personalization. It is also a practice of mass collaboration at a world-wide scale that allows users to vote for ranking of search results and improve search performance. 
SearchWiki and U Rank are under testing, often returning inconsistent or unintuitive results. It is not revealed what exactly they aim to achieve and what the approaches are. Up to our knowledge, there is no published research under this topic. Thus this paper makes the ?rst e?ort to estab- lish a framework by identifying the fundamental issues and proposing principled solutions for rank editing and sharing. 
Copyright is held by the International World Wide Web Conference Com- mittee (IW3C2). Distribution of these papers is limited to classroom use, and personal use by others. 
WWW 2010, April 2630, 2010, Raleigh, North Carolina, USA. ACM 978-1-60558-799-8/10/04. 
Joey Jan Texas State University-San Marcos 
601 University Drive 
San Marcos, TX, USA, 78666 
jj1258@txstate.edu 
The proposed framework features extended functionalities beyond SearchWiki and U Rank. For rank editing, users can specify not only relative, but also absolute preferences. For edit sharing, the notion of aggregation is generalized and users can select arbitrarily aggregated views. Beyond shar- ing among users, edits can be transferred to similar queries, which can be considered sharing among queries. 
These extensions generate bene?ts, as well as non-trivial technical complications. We deployed a prototype system Rants [7], that tackles the challenges and implements the framework, providing web search services through the Google API [5]. Figure 1 illustrates the interface of Rants. 
Rank editing. Existing systems provide two editing op- erations, promotion and demotion, where a logged-in user u can promote (move up) or demote (move down) a search result r for a query q by one or more positions. Let up and down denote the two operations, where up(r;2) means to move r up by 2 positions if possible (it may reach the top and cannot continue). 
How to interpret a move? The user intention behind a move is unfortunately ambiguous. Aggressively, it may mean an assertion for the ranking of all results after the move. Conservatively, it may mean several pairwise prefer- ences for the involving results only. In our framework, we take a conservative approach and make the least inferences from a move. For example, if after up(r;2) by user u for query q, r surpassed r0 and r00, we store two pairs (r; r0) and (r; r00), meaning that for q, user u prefers r to appear before r0 and r00. 
We adopt this least inference principle for the following reasons. Firstly, it generates the least, if not none, ambigu- ity. In the above example, although by the same move dif- ferent users may mean di?erently, all of them would mean at least those two pairwise preferences. We do not even infer on the precedence relationship between r0 and r00. Secondly, it well serves the purpose that, given the same list of unedited search results for query q, the same ranking as edited will be restored if the inferred pairwise preferences are respected. 
Based on this interpretation, up(r;2) is equivalent to two consecutive executions of up(r;1). Thus, in Rants, we only allow up(r) and down(r), meaning up(r;1) and down(r;1), indicated by the " and # arrows in Figure 1. This is not a limitation, but an emphasis on primitive functionalities, instead of syntactic sugars, for conceptual clarity. 
The two operations actually map to the same task, a swap of positions of two results. Let r0 and r be two neighboring results where r0 is immediately before r. Both up(r) and down(r0) ?re a swap of r0 and r, specifying a preference of 
1245 

========1========

WWW 2010  Demo 
Specify aggregation:  
You are logged in as test  
         All 
test 
Rants 
 rank editing and sharing  
  Search 
 r 1 
  
3 
 r 2 
  
3 
 r 3 
  
 r 4 
  
 r 5 
  
8 
 r 6 
   6 
 r 7 
  
 r 8 
  
 r 9 
  
 r 10 
  
Figure 1: Interface of Rants. 
(r; r0). In Rants, this task is implemented by a primitive function swap(r0; r). 
Extended rank editing. Promotion and demotion spec- ify relative preferences. There are often situations where users want to specify absolute preferences as well. For ex- ample, user u may always want to see result r appear among top 3 for query q. This cannot be achieved by relative pref- erences because over time, there would be new results for q taking the top 3 seats whose pairwise preferences w.r.t. r were not speci?ed and stored. 
Mostly, user u wants to stipulate that result r must appear among top k, instead of not. We use a pair (r; k) to capture this absolute preference. As shown in Figure 1, for each q, k can be entered into the box to the right of the # arrow. In Rants, this operation is implemented by a primitive function anchor(r; k). 
Edit sharing. Rank edits are user preferences that can be aggregated and shared among users. Rants utilizes func- tion aggr(U) to generalize the notion of aggregation. No matter logged in or out, user u can arbitrarily specify U, the set of users whose edits (relative or absolute) are to be used for aggregation. If U = fug, the edits from u herself will be used. If U = All, the (published) edits from all users will be used. If U = ;, the original, unedited search results will be presented without enforcing any stored preferences. 
Suppose a same edit is speci?ed by a set U0  U of users. Then the edit can be shared if 
jU0j 
jUj 
 ag, where 0  ag  1 is a tunable threshold. 
Edit transfer. Rank editing takes user e?ort. It is greatly bene?cial if we can properly transfer rank edits from a query to its similar ones. For example, if user u edited the results for query \David Dewitt", it is very likely that she wants to reuse the edits for query \David J. Dewitt". Edit transfer can be considered as edit sharing among queries, in contrast to sharing among users. 
Function trans(q) returns a query q0 such that it is ap- propriate to transfer edits for query q0 to query q. In the function, two similarity measures are used. wordSim(q; q0) compares the keywords of q and q0. rankSim(q; q0) com- pares the ranks of search results of q and q0. Both need to 
April 26-30  Raleigh  NC  USA 
pass their respective thresholds ws and rs. Obviously, the bigger the thresholds, the more conservative the transfer. Setting the thresholds to 1 shuts down edit transfer. 
Constraint enforcement. We take stored rank edits as user constraints that need to be respected and enforced in processing query q. In Rants, this is implemented by functions enfor(R) and enfor(A), which enforce a set of relative preferences R and a set of absolute preferences A. R and A are determined by q and a selected aggregation U. 
The enforcement adopts a so-called least modi?cation prin- ciple, where as little as possible modi?cation is used in en- forcing the constraints, and the degree of modi?cation is measured by edit distance between the rankings of search results before and after the enforcement. 
In Rants, consistency of relative preferences are main- tained and they are guaranteed to be completely enforced. To do so for absolute preferences entails tremendous tech- nical complications that signi?cantly slow down rank edit- ing and query processing for a minimal gain. Practically, a light-weight best-e?ort approach can achieve complete en- forcement of absolute preferences in normal cases. 
Technical challenges and contributions. In sum- mary, in this introductory study we consider the following fundamental issues. How to interpret, capture and store user preferences? How to keep them consistent and redundancy- free? How to aggregate them for sharing among users? How to transfer them to similar queries? How to enforce them in processing queries? 
In response to these questions, we construct the following functional primitives: swap(r0; r), anchor(r; k), aggr(U), trans(q), enfor(R) and enfor(A). Their correct and ef- ?cient realization generates non-trivial technical challenges. 
Thus, our contributions include the identi?cation of fun- damental issues involved in the Wiki-like web search para- digm, the formation of the corresponding functional primi- tives, the provision of solutions to the associated technical challenges, and the implementation of these solutions. 
2. RELATED WORK 
We have not seen published research on rank editing and sharing. However, web search giants Google and Microsoft are recently experimenting on this novel search paradigm through SearchWiki [3] and U Rank [6] respectively. While their approaches are not revealed to public, we use Rants to demonstrate a well-de?ned framework that features ex- tended functionalities. 
Rank editing can be considered as one way of search per- sonalization. Personalized search allows ?ne-tuning of search results based on an individuals preferences or pro?le. Both Google [2] and Yahoo! [8] provide such services. Tradition- ally, the major source for personalization is search history, which forms a user pro?le and can be used to inuence all queries from the user. In Rants, ranks of search results can be directly edited, and the edits can be used to inuence limited queries, i.e., the query itself and its similar ones. 
Edit sharing is a mass-collaboration way of improving search performance. It is also related to social search. In contrast to established algorithmic or machine-based ap- proaches, social search determines the relevance of search results by considering the interactions or contributions of users. Example social search engines include Google social search [4] and \community powered" Eurekster Swiki [1]. 
1246 

========2========

WWW 2010  Demo 
3. ALGORITHM 
In this section, we explain the primitive functions that are used in rank editing, edit sharing, and query processing. 3.1 Rank Editing 
Once a user u is logged in, she can edit the search results for a query q, specifying relative and/or absolute preferences. Since she needs to monitor her own editing process, the view must be chosen as U = fug. 
Let RP and AP be hashes storing all the relative and absolute preferences from u respectively. Then RP(q) and AP(q) indicate a set of relative preferences and a set of ab- solute preferences for q from u. For clarity we do not specify u in the notations. 
Let L(q) = (r1; r2; :::) be the list of search results for query q. For a result r 2 L(q), we use rank(r) to denote the rank of r in L(q). Let L0(q) be the list of unedited original search results. L0(q) may change over time. Some previous results may disappear. Some new ones maybe added. The content and ranking of results may change as well. 
swap(r, r). The function handles speci?cation of a rel- ative preference (r; r0), i.e., rank(r) < rank(r0), for query q. It is ?red either by up(r) or down(r0), resulting in a swap of positions of r0 and r in the search results. 
Relative preferences are transitive. E.g., with (r1; r2) and (r2; r3), we can infer (r1; r3). All the preference pairs in RP(q), if consistent, form a partial order. Precisely, it is a strict partial order, a binary relation that is irreexive and transitive, corresponding to a directed acyclic graph (dag). 
Example 1. Let L0(q) = (r1; r2; r3; r4). By swap(r1; r2) and swap(r3; r4), the user specify (r2; r1) and (r4; r3), which form a partial order. We do not infer the pairwise prefer- ences for, e.g., r1 and r4. Thus, both (r2; r1; r4; r3) and (r4; r3; r2; r1) respect the speci?ed preferences. 
Due to the dynamic nature of search results and user pref- erences, RP(q) may receive inconsistent, conicting prefer- ences that cannot be enforced simultaneously. 
Example 2. At day 1, L0(q) = (r1; r2). swap(r1; r2) adds (r2; r1) to RP(q), which will be enforced in query processing. At day 2, the user changed her mind, and swap(r2; r1) would add (r1; r2) to RP(q), which contradicts with (r2; r1). 
Only consistent user preferences can be completely en- forced. Thus it is essential to maintain the consistency of RP(q). It is also desirable to keep RP(q) redundancy-free for improved enforcement eciency. Addition of a pair may generate new inferred preferences that are redundant to ex- isting ones, as demonstrated in the following example. 
Example 3. Let RP(q) = f(r1; r3);(r2; r3)g. swap(r2; r1) adds (r1; r2) to RP(q). Then, (r1; r3) becomes redundant because it can be inferred by (r1; r2) and (r2; r3). 
swap(r0; r) maintains RP(q) as a redundancy-free dag. We omit the algorithmic details due to the space limit. The enforcement of the newly added pair (r; r0) is trivial. 
anchor(r, k). The function handles speci?cation of an absolute preference (r; k), i.e., rank(r)  k, for query q. As consistency of AP(q) is not maintained, adding (r; k) is trivial. If (r; k) is already in AP(q), update it. 
April 26-30  Raleigh  NC  USA 
Inconsistency of AP(q) arises in various cases. For exam- ple, (r;1) 2 AP(q) and (r0;1) 2 AP(q). AP(q) may not be compatible with RP(q) either. For example, (r;1) 2 AP(q) and (r0; r) 2 RP(q). However, since AP(q) is small, such anomalies would not arise in normal cases. Thus to avoid the tremendous technical complications, we do not maintain consistency and compatibility of AP(q). 
After the speci?cation of (r; k), we want to enforce it im- mediately. For this purpose, we call a recursive procedure climb(r), which is introduced in Section 3.3. 
3.2 Edit Sharing 
User edits can be shared among users, as well as among similar queries. 
aggr(U). The function performs aggregation of edits for a chosen user set of U, returning RPU and APU, the aggre- gated relative and absolute preferences over U. Although U can be arbitrarily speci?ed, it must be pre-de?ned so that RPU and APU can be pre-computed o?-line, instead of dur- ing query processing, for improved response time. 
We calculate RPU as follows. For each query q such that0 
RP(q) 6 = ; for some u 2 U, for each pair (r; r ) in RP(q), let U0  U be the set of users who speci?ed the pair. If jU0j 
jUj 
 ag, insert (r; r0) into RPU(q), where RPU(q) stores the set of aggregated relative preferences over U for query q. Recall that ag, 0  ag  1, is a tunable threshold. 
We calculate APU in a similar manner with slight mod- i?cation. For each query q such that AP(q) 6 = ; for some u 2 U, for each pair (r; k) specifying a preference on r in AP(q), let U0  U be the set of users who speci?ed the pair. If 
jU0j 
jUj 
 ag, insert (r;k) into APU(q), where APU(q) stores the set of aggregated absolute preferences over U for query q, and (k) is the averaged k speci?cations for r over U0. 
The consistency of RP implies the consistency of RPU. As in AP, the consistency of APU is not maintained. 
trans(q). The function returns a query q0, such that the edits (w.r.t. a chosen aggregation U) for query q0 can be ef- fectively utilized by query q. If none of such q0 can be found, the function returns -1, which means no user constraints will be enforced in processing query q. 
A candidate query q0 must have an entry stored in RPU or APU. If q itself is such a candidate, then q will be re-0 turned. Otherwise, trans(q) searches for some q that is similar enough to q. 
As a candidate, query q0 must also have the properties of wordSim(q; q0)  ws and rankSim(q; q0)  rs, where ws0 
and rs are tunable thresholds. wordSim(q; q0 ) is a similar- ity measure comparing the keywords of q and q . In trans(q), this comparison is done ?rst to eliminate most of the un- quali?ed candidates. rankSim(q; q0) is a similarity measure comparing the ranks of search results of q and q0, in partic- ular, L10(q) and L10(q0), the top 10 unedited results for q0 
and q respectively. In the end, trans(q) returns a quali?ed candidate q0 with the largest rankSim(q; q0). 
For computing wordSim(q; q0), we treat q and q0 as sets of keywords and use J(q; q0), the Jaccard index for q and q0, to measure their similarity. Speci?cally, 
0 J(q; q0) = 
jq \ q j 
jq [ q0j: 
For computing rankSim(q; q0), we have two options. The 
1247 

========3========

WWW 2010  Demo 
?rst option is J(L10(q); L10(q0)), i.e., the Jaccard index for0 
L10(q) and L10(q ). The second option is a rank-aware similarity measure, the Kendall tau coecient [9], a non- parametric statistic used to measure the degree of corre- spondence between two rankings. Speci?cally, 
(L10(q); L10(q0)) = 
nc  nd 
1n(n 
2 
 1); 
where nc is the number of concordant pairs between L10(q)0 
and L10(q ), and nd is the number of disconcordant pairs. In our case, n = 10, and the denominator is just the total number of pairs. 
To compute rankSim(q; q0), L10(q0) must be previously0 stored. Since it contains unedited results, potentially L10(q ) can be shared by all users for space eciency. 
3.3 Query Processing 
Di?erent from existing systems, Rants separates editing from viewing, which means one does not need to log in to share published user edits. She only needs to select a view, i.e., a user set U for aggregation. 
In processing query q, trans(q) is called ?rst, which re- turns q0. If q0 = 1, no stored user constraints need to be en- forced and the unedited result list L0(q) will be presented in-0 
tact. Otherwise, RPU(q ) and APU(q0) are retrieved. They contain the relative and absolute preferences to be enforced on L0(q), the original unedited search results. 
L0(q) is dynamic and changes over time. Potentially, this may cause problems for relative preference enforcement. Suppose (r1; r2) and (r2; r3) are in RPU(q0). It is possible that r2 maybe absent from L0(q). Then we need to make sure that (r1; r3) is enforced. 
A relative preference pair (r; r0) 2 RPU(q0) is applicable if and only if both results are present, i.e., r 2 L0(q) and0 
r 2 L0(q). An absolute preference pair (r; k) 2 APU(q0) is applicable if and only if r 2 L0(q). We use R and A0 
to denote the applicable subsets of RPU(q ) and APU(q0) respectively. Then for enforcement purposes, enfor(R) will be invoked ?rst, followed by enfor(A). 
enfor(R). The function enforces the relative preferences in R on L0(q). Since RPU(q0) is consistent, R  RPU(q0) is also consistent and completely enforceable. 
As indicated in Example 1, a partial order can be enforced in di?erent ways, which rects the fact that a dag can have many topological orderings. 
In graph theory, a topological ordering of a dag is a linear ordering of its nodes where each node comes before all nodes to which it has outbound edges. It is a total order that is compatible with the partial order. Every dag has one or more topological orderings. 
To comply with the least modi?cation principle, we com- pute a topological ordering T for R that is the closest to L0(q). Then we iteratively process the edges in T in order.0 
In more detail, for each (r; r ) 2 T, if r0 is before r in L(q), move r0 down to the position immediately after r. 
In this process, (r0; k) 2 A maybe violated. But we do nothing about it until the next stage. 
enfor(A). The function enforces the absolute preferences in A on LR(q), which is the list of search results returned0 
by enfor(R). As in APU(q ), A may not be consistent. We use a best-e?ort approach to enforce A as much as we can without violating the already enforced R. 
April 26-30  Raleigh  NC  USA 
To comely with the least modi?cation principle, we sort the results in A according to their orders in LR(q). Then we iteratively process each (r; k) 2 A in order, by invoking climb(r). 
climb(r) is recursive. If rank(r) > k, it moves r up by swapping r and r0. If r0 blocks r, it recursively calls climb(r0). r0 blocks r if (r0; k0) 2 A or (r0; r) 2 R is violated by the planned swapping. climb(r) stops when rank(r) = k, or no swapping can be conducted, in which case all results above r (including r) are blocked. 
4. DEMONSTRATION 
Rants [7] is maintained at a regular desktop PC with In- tel 3.0GHz Duo processor and 4GB memory. It was imple- mented using the Google web search API [5]. For illustration purposes, Rants only retrieves 40 HTML pages from the API for each query. 
Demonstration scenario. As a user, you can visit the Rants URL to test the system. You can either create an account to login, or use the given testing account to login. 
No matter logged in or out, you can specify the set of users whose preferences are to be used for aggregation. In \select search view", choose \All" for all users. To choose one or several users, enter a single user ID or a list of user IDs, e.g., \test" or \test1, test2, test3", in the edit box and click the radio button besides it. By leaving the edit box empty, you choose an aggregation on ;, in which case the original unedited search results will be presented. 
If logged in, you can issue web search queries and edit the results by specifying relative or absolute preferences. You can verify that these preferences are respected the next time you issue the same queries. To ease the comparison, search results are marked with their original, unedited ranks, which you can use as temporary IDs. 
Edit transfer among similar queries is performed regard- less of the login status. However, it is good to login because you need to create the edits to be transferred. For example, you can issue a query \David DeWitt" and edit the results. Then you can issue a similar query \David J. DeWitt" and see how those stored edits for \David DeWitt" are enforced in producing the query results for \David J. DeWitt". 
5. REFERENCES 
[1] Eurekster Swiki. http://www.eurekster.com. 
[2] Google Personalized Search. 
http://googleblog.blogspot.com/2007/02/personally- 
speaking.html. 
[3] Google SearchWiki. 
http://googleblog.blogspot.com/2008/11/searchwiki- 
make-search-your-own.html. 
[4] Google Social Search. 
http://googleblog.blogspot.com/2009/10/introducing- 
google-social-search-i.html. 
[5] Google Web Search API. http://code.google.com/. [6] Microsoft U Rank. 
http://research.microsoft.com/en-us/projects/urank/. [7] Rants. http://dmlab.cs.txstate.edu/rants. 
[8] Yahoo! Personalized Search. http://myweb.yahoo.com/. [9] W. Kruskal. Ordinal measures of association. Journal 
of the American Statistical Association, 
53(284):814{861, 1958. 
1248 

========4========

CMVF: A Novel Dimension Reduction Scheme for Ef?cient 
Indexing in A Large Image Database 
1 
School of Computer Science & Engineering 
The University of New South Wales 
Sydney NSW 2052, Australia fjls,jas,qshengg@cse.unsw.edu.au 
Jialie Shen1, Anne H.H. Ngu2, John Shepherd1, Du Q. Huynh3, Quan Z. Sheng1 
2 
Department of Computer Science 
Southwest Texas State University 
San Marcos, 601 University Drive, Texas, USA 
1. INTRODUCTION 
In recent years, due to the increasing volumes of multimedia data in the World Wide Web, digital library, biomedicine and other applications, ef?cient content based similarity search in large image databases is gaining considerable research at- tentions. As a result, various indexing methods known as Spatial Access Methods (SAMs) and metric trees have been proposed to support this kind of retrieval. The former in- cludes SS-tree, R+-tree and grid ?les; the latter includes the vp-tree, mvp-tree , GNAT and M-tree [3]. 
However, the optimised distance-based access methods cur- rently available for multidimensional indexing in multimedia databases are developed based on two major assumptions: a suitable distance function is known a priori and the dimen- sionality of the image features is low. Unfortunately, these assumptions do not make the problem substantially easier to solve. For example, it is extremely dif?cult to de?ne a distance function that accurately mimics human visual per- ceptioninimagesimilaritymeasurement. Also,typicalimage feature vectors are high-dimensional (dozens of dimensions). The standard approach to reducing the size of feature vec- tors is Principle Component Analysis (PCA). However, this approach might not always be possible due to the non-linear correlations in the feature vectors. 
Motivated by these concerns, we proposed and developed the CMVF (Combining Multi-Visual Features) framework, a fast and robust hybrid method for nonlinear dimensions reduc- tion of composite image features for indexing in large image database[2]. This method incorporates both the PCA and non-linear neural network techniques to reduce the dimen- sions of feature vectors, so that an optimised access method can be applied. 
In this demonstration, we show that with CMVF approach a small but well-discriminating feature vector can be obtained for effective indexing. It allows us to incorporate classi?ca- tion information based on human visual perception into the indexing. In addition, effectiveness of the indexing can be improved signi?cantly with integration of additional image features. In the following sections, we overview the design and system architecture of our CMVF system, and give per- formance evaluation. 
angu@swt.edu 
3 
School of Information Technology 
Murdoch University 
Murdoch WA 6150, Australia 
d.huynh@murdoch.edu.au 
2. SYSTEM OVERVIEW 
An effective content-basedretrieval systemcannot be achiev- ed by consideringonly a single typeoffeature such as colour, texture and shape alone. However, creating an index based on a concatenation of feature vectors(e.g., colour, shape and texture) will result in a very high dimensional feature space, rendering all existing indexing methods useless. Also as- suming that each type of visual feature contributes equally to the recognition of that image is not supported in human visual perceptron. We need to fuse the multiple single fea- ture vectors into a composite feature vector which is low in dimensions and yet preserves all the necessary information for image retrieval. Thus, non-linear dimension reduction (NLDR) method in conjunction with a multidimensional in- dex structure becomes a natural and practical solution. Fig- ure 1 shows the overall architecture of our hybrid method. The different components of the architecture will be covered in detail in this section. 
OUTPUT 
Neural Network 
HIDDEN 
Lower dimension vectors 
INPUT 
PCA Analysis 
PCA 
PCA 
PCA 
Principal components 
COLOUR 
TEXTURE 
SHAPE 
Figure 1: A hybrid image feature dimensions re- duction scheme. The linear PCA appears at the bottom, the nonlinear neural network is at the top, and the representation of lower dimensional vectors appears in the hidden layer. 
2.1 Composite Image Features 
In CMVF, we consider three types of image features: color, texture and shape. Note that our system is not limited to dealing with these three features only. It can be extended to combine other visual and topological features[9] (such as motion and spatial relationship among the objects in the image) for effective indexing. 

========1========

The colour features are extracted using the colour histogram technique. We used the colour space CIE L*u*v. The reason for selecting the CIE L*u*v instead of the normal RGB or other colour spaces is that it is more uniform perceptually. Our colour features are presented as 37-dimensional vectors. 
Texture featurescarrythepropertymeasures, suchassmooth- ness, coarseness and regularity, of an image. The texture fea- tures are extracted using a ?lter -based method. This method detects the global periodicity of intensity values in an image by identifying regions that have high energy, narrow peaks. The advantage of ?lter -based methods is in their consistent interpretation of feature data over both natural and arti?cial images. The Gabor ?lter is a frequently used ?lter in texture extraction. It measures a set of selected orientations and spa- tial frequencies. The total number of ?lters needed for our Gabor ?lter is 30. Texture features are therefore represented as 30-dimensional vectors. 
Shape is an important and powerful attribute for image re- trieval. It can represent spatial information that is not pre- sentedincolorandtexturehistogram. Inoursystemtheshape information of an image is described based on its edges. A histogram of the edge directions is used to represent global information of shape attribute for each image. We used the Canny edge operator[8] to generate edge histogram of im- ages in the prepropressing stage. To solve the scale invari- ance problem, the histograms are normalized to the number of edge points in each image. In addition, smoothing proce- dures presented in [1]are used to make histograminvariant to rotation. The histogram of edge directions is represented by 30 bins. Shape features are thus presented as 30-dimensional vectors. 
2.2 Architecture of Hybrid Image Feature Di- 
mension Reducer 
In CMVF, concatenation1 is used to form composite feature vectorsforfurtherprocessing. Withthe97-dimensionfeature vectors(37 dimensions for colour, 30 dimensions for texture and 30 dimensions for shape), the PCA[6] is useful as an initial dimension reducer while further dimension reduction for nonlinear correlations can be handled by NLDR. There are two methods for combining the PCA and NLDR: 
 Apply the PCA to the single feature vectors separately. The lower-dimensional single feature vectors are then combined to form low-dimensional composite feature vectors for NLDR and classi?cation. 
 Apply the PCA to the high-dimensional composite fea- ture vectors. The reduced-dimensional composite fea- ture vectors are then used for NLDR and classi?cation. 
1Let 
xc, xt and xs be the colour, texture and shape feature vectors, concatenation, denoted by the symbol , of these three feature vectors is de as follows: 
xc 
! 
x  xc  xt  xs = xt 
xs 
Bothmethodsareadoptedinoursystemsothatthedifferences in the reduction results could be compared. 
2.2.1 The PCA for Dimension Reduction The PCA has been employed to reduce the dimensions of single feature vectors so that an ef?cient index can be con- structed for retrieval in image databases[7]. It has also been applied to image coding, e.g., for removing correlation from highly correlated data such as face images. The advantage of the PCA transformation is that it is linear and that any linear correlations presented in the data are automatically detected. In our system, the PCA is used as a "pre-processing" step in a NLDR method where it provides optimally reduced dimen- sionalfeature vectors for the3-layer neuralnetwork, and thus speeds up the NLDR training time. 
2.2.2 Classi?cation based on Human Visual Percep- 
tion 
The human perceptual process incorporates information in colour, texture,shapeandothervisualfeaturesunderacertain context to classify images into the appropriate classes. To integrate this procedure into our system, we set up a simple on line image classi?cation experiment and asked 7 people (subjects), all of whom are from different backgrounds, to participate in the experiments. Before starting experiment, we ?rst prepared a set of images (labelled as test-images from here on), from our 10,000 image collection. This set of image covers all the different classes of images in the collection. In order to enhance robustness of our approach, some of them have image variations(e.g., color distortion, shifting,rotation....etc). Atthebeginningofeachexperiment, a query image was arbitrarily chosen from test-images and presented to the subjects. The subjects were then asked to pick 20 images that were most similar in colour, texture and shapetothequeryimage. Thoseimagesthatwereselectedby more than 3 subjects were classi?ed to the same class as the query image and were then deleted from test-images. The experiment was repeated until every image in test-images hadbeencategorizedintoanappropriateclass. Theendresult of the experiments is that images which are similar to each other in colour, texture and shape are put into the same class based on human visual perception. This classi?cation results are used in the NLDR process described below. 
2.2.3 Neural Network for Dimension Reduction In our work, a three-layer perceptron neural network with a quickprop learning algorithm[5] is used to perform dimen- sions reductions of image features. The network in fact acts as an image classi?er . The training samples are training pat- terns of the form (v;c) where v is a feature vector, which can beeitherasingle-featurevectororacompositefeaturevector, and c is the class number to which the image represented by v belongs. We note that the class number for each feature vector was determined by the experiments mentioned in the previous subsection. 
When the network has been successfully trained, the weights that connect between the input and hidden layers are entries of a transformation that map the feature vectors v to smaller 

========2========

dimensional vectors. Thus, when a high-dimensional feature vector is passed through the network, its activation values in the hidden units form a lower-dimensional vector. This lower dimensional feature vector keeps the most important information of the original feature vectors (colour, texture and shape). 
3. PERFORMANCE EVALUATION Inthissection, resultsfroma comparative study arepresented to demonstrate superiority of our hybrid dimension reduction method over using the PCA or using neural network alone. We used a collection of 10,000 images. These images were retrieved from different public domains that can be classi?ed under a number ofthemeswhichcover naturalscenery, archi- tectural buildings, plants, animals, rocks, ?ags, etc. A subset of this collection of images was selected to form the training samples(Section 2.2.2). 
3.1 Performance on Image Categorisation To determine the accuracy and ef?cienc y of the three meth- ods for dimension reduction, we introduce the measure class separation degree Ci, de?ned as: 
PN 
j=1 
Qj 
Ci = 
N(M  N); 
i = 1:::m 
wheremisthenumberofclasses, N isthenumber ofrelevant images. In the class, M is the total number of test images, Qj is the number of images whose distances to the j-th image in the class are greater than all thedistances from the j-th image to its relevant images. An image is said to be relevant to a classifitbelongsandhasbeencorrectlyassignedorclassi?ed to that class. 
Reduction Method 
PCA 
NN4 CMVF CMVF 
Average Rate 
90.2 100% 99.9% 99.9% 
Feature Vector2 
xc  xt  xs 
P 
xc  xt  xs P(xc  xt  xs) (xc)  P(xt)  P(xs) 
Learning Time(epoch3) 
N/A 
7100 
4200 
4120 
Table 1: Average class separation values with di ent method. 
From Table 1 it can be seen that all classes of the test image collection are well separated by using neural network and hybrid approach comparing using thePCA. However, dimen- sion reduction with neural network suffers from very long learning time. In contrast, our proposed hybrid method does not lose much accuracy but improve the network learning time. The ef?cienc y is gained by using a relatively small number of network inputs and the network training iterations are conducted in the direction of the largest eigenvalues for each feature. 
2Because 
there is no di in the results of methods used to organise the input feature vectors, we just present one of them. 
3Epoch 
means one complete presentation of the input data to the network being trained. 
4NN 
means neural network. 
3.2 Evaluation of Reduced Dimensional Im- 
age Features using M-trees 
We usedM-trees[4]as accessmethodforevaluatingthequal- ity of feature space reduced by the PCA, neural network and hybridmethod. ThenumberofdimensionsofM-treeswasset to 6, corresponding to the number of hidden units used in the neural networks. Every image from the collection can serve as a query image. We posed a query image to the M-trees to conduct a k-NN search,where k was set to100. Theconcepts of normalized precision and normalized recall[10] in in- formation retrieval were used to evaluate the effectiveness of similarity retrieval since not all relevant images are retrieved. 
1 
e 
t 
0.9 
a 
r 
l 
l 
a 
precision of PCA precision of neural network precision of hybrid method recall of PCA 
recall of neural network recall of hybrid method 
c 
e 
r 
0.8 
d 
n 
a 
n 
o 
i 
0.7 
s 
i 
c 
e 
r 
p 
d 
e 
0.6 
z 
i 
l 
a 
m 
r 
o 
0.5 
n 
e 
g 
a 
r 
e 
v 
0.4 
A 
0.3 
0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 
Class ID 
Figure 2: Comparing hybrid method with the PCA and neural network on average normalized recall and precision rate. 
1 
0.9 
e 
t 
a 
r 
n 
0.8 
o 
i 
s 
i 
c 
e 
0.7 
r 
p 
d 
n 
0.6 
a 
l 
l 
a 
c 
e 
0.5 
r 
d 
e 
z 
i 
0.4 
l 
a 
m 
r 
o 
0.3 
n 
e 
g 
a 
0.2 
r 
e 
v 
recall rate without shape recall rate with shape precision rate without shape precision rate with shape 
A 
0.1 
0 
0 
1 
2 
3 
4 
5 
6 
7 Class ID 
8 
9 
10 
11 
12 
13 
14 
Figure 3: Eeness of adding shape feature on hybrid method 
In Figure 2, we can see that the normalized recall and normalized precision values from the neural network and the hybrid methods are almost the same. Thus, the major difference between two approaches is the time required to train the network. One can therefore conclude that it is more advantageous to use a hybrid dimensions reduction method to reduce the dimensions of image features for effective in- dexing using M-tree. Inaddition, systemperformancecan be improved considerablely with incorporation of other visual features. As is evident from Figure 3, an addition of shape feature into our system gave approximately 15% improve- ment of recall and precision over just using color and texture histogram. 

========3========

3.3 Robustness 
Robustness is a very important feature for a content based image retrieval system because image data in real life always accompanies with noise and distortion. With incorporation of human visual perception, CMVF is robust to different kind of image variations including color distortion, sharp- ness changes, shifting and rotation. Experiment shows that CMVF is robust to 50.4 percent sharpening, 45 degree rota- tion,blurringwitha9x9Gaussian?lter ,randomspreadby10 pixels, 10 percent more saturation, 11 percent less saturation and pixelization by 9 pixels. 
4. DEMONSTRATION 
With the use of hybrid structure, CMVF illustrates its great advance in performance against other dimension reduction methods such as the PCA and neural network. To show these advance, a content based image retrieval system has been developed in C++ and Java. An online demonstration is provided5. WhentheuseraccessestheCMVFwebpage,alist of images are randomly selected and displayed as potential query images. The user can submit one of them as a query andthesystemwillsearchfortheimagesthataremostsimilar in visual content. It displays a list of similar images, in order, startingfromthemostsimilar. Thequerycanbeexecutedwith any ofthefollowingretrieval methods: PCA,neuralnetwork, CMVF and CMVF with shape. During this demonstration, we will present its advance in effectiveness, ?e xibility and robustness via the following: 
 Effectiveness: One of our conjectures is that it is possi- ble to obtain effective retrieval from low-dimensional indexing vectors, if these vectors are carefully con- structed. In CMVF, we build indexing vectors from high-dimensional raw feature vectors via PCA and a trained neural network classi?er , which incorporates manual classi?cation criteria. Although some time is required to train the neural network involved in CMVF, we will demonstrate that signi?cant improvement in classi?cation and similarity search can be achieved by CMVF than can the PCA. In comparison with the pure neural network approach, CMVF also gives good clas- si?cation and query results, with less training time and simpler system structure. 
 Flexibility: Forfurtherinvestigationsintocontent-based image retrieval, it would be useful if new indexing fea- tures could be easily incorporated into the system. The system demonstrates retrieval based on colour and tex- ture, as well as colour, texture and shape. It was rela- tively straightforward to incorporate shape into the sys- tem, and it clearly demonstrates that the addition of shape leads to superior retrieval results. 
 Robustness: In the real world, perfect image data can not be expected. Thus, it is very important for image retrieval systems to be robust to image variations such ascolordistortion,sharpnesschanges,shiftingandrota- tion. WewilldemonstratethatCMVFworkseffectively 
5http://www.cse.unsw.edu.au/imagedb/MVindex/index.html 
even in the presence of the kinds of distortion situations just mentioned. 
5. CONCLUSION 
In this demo, we present CMVF, a novel indexing scheme by combining different types of image features to support queries that involve composite multiple features. We have alsodemonstratedtheoutputqualityofourhybridmethodfor indexing the image collection using M-trees. Our proposed hybrid dimension reduction approach, signi?cantly reduces the size of image feature vectors while at the same time re- tainingeffective discriminationpower andalsoallowingus to incorporateaspectsofhumanvisualperceptionintheweights of the network. This enables any existing access method for moderate dimensions to be used ef?ciently and effectively. 
6. REFERENCES 
[1] A.K.Jain and A. Vailaya. Image retrieval using color 
and shape. Pattern Recognition, 29(8):12331244, 
1996. 
[2] Anne.H.H.Ngu, Q. Z.Sheng, D. Q.Huynh, and R. Lei. 
Combining multi-visual features for ef?cient indexing 
in a large image database. The VLDB Journal, 
9(4):280293, May 2001. 
[3] C. Bohm, S. Berchtold, and D. A. Keim. Searching in 
high-dimensional spaces: Index structures for 
improving the performance of multimedia databases. 
ACM Computing Surveys, 33(3):322  373, September 
2001. 
[4] P. Ciaccia, M. Patella, and P. Zezula. M-tree: An 
ef?cient access method for similarity search in metric 
spaces. In Proceedings of the 23rd VLDB 
International Conference, pages 426435, 
Athens,Greece, September 1997. 
[5] S. Fahlman. An empirical study of learning speed for 
back-propagation networks. Technical Report 
CMU-CS 88-162, Carnegie-Mellon University, 1988. 
[6] K. Fukunaga. Introduction to Statistical Pattern 
Recognition. Academic Press, 1990. 
[7] G.M.P.Euripdes and C. Faloutsos. Similarity searching 
in medical image databases. IEEE Trans. Knowl. Data 
Eng., 3(9):435447, June 1997. 
[8] J.Canny. A computational approach to edge detection. 
IEEE Trans. Pattern Anal. Mach. Intell., 
8(6):679698, November 1986. 
[9] M. Nabil, Anne.H.H.Ngu, and J.Shepherd. Picture 
similarity retrieval using the 2d projection interval 
representation. IEEE Trans. Knowl. Data Eng., 
8(4):533539, 1996. 
[10] G. Salton and M. McGill. Introduction to Modern 
Information Retrieval. McGraw-Hill, New York, 1993. 

========4========
