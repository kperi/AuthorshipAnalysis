Turning Clusters into Patterns: Rectangle-based Discriminative Data Description 
Byron J. Gao Martin Ester School of Computing Science, Simon Fraser University, Canada 
bgao@cs.sfu.ca ester@cs.sfu.ca 
Abstract 
The ultimate goal of data mining is to extract knowledge from massive data. Knowledge is ideally represented as human-comprehensible patterns from which end-users can gain intuitions and insights. Yet not all data mining methods produce such readily understandable knowledge, e.g., most clustering algorithms output sets of points as clusters. In this paper, we perform a systematic study of cluster descrip- tion that generates interpretable patterns from clusters. We introduce and analyze novel description formats leading to more expressive power, motivate and deﬁne novel descrip- tion problems specifying different trade-offs between inter- pretability and accuracy. We also present effective heuristic algorithms together with their empirical evaluations. 
1. Introduction 
The ultimate goal of data mining is to discover useful knowledge, ideally represented as human-comprehensible patterns, in large databases. Clustering is one of the major data mining tasks, grouping objects together into clusters that exhibit internal cohesion and external isolation. Unfor- tunately, most clustering methods simply represent clusters as sets of points and do not generalize them into patterns that provide interpretability, intuitions, and insights. 
So far, the database and data mining literature lacks sys- tematic study of cluster description that transforms clusters into human-understandable patterns. For numerical data, hyper-rectangles generalize multi-dimensional points, and a standard approach in database systems is to describe a set of points with a set of isothetic hyper-rectangles [1, 16, 18]. Due to the property of being axis-parallel, such rectangles can be speciﬁed in an intuitive manner; e.g., “3.80 • GPA • 4.33 and 0.1 • visual acuity • 0.5 and 0 • minutes in gym per week • 30” intuitively describes a group of “nerds”. 
Patterns are models with generalization capacity, as well as templates that can be used to make or to generate things. The rectangle-based expressions are interpretable models; as another practical application, they can also be used as 
search conditions in SELECT query statements to retrieve (generate) cluster contents, supporting query-based iterative mining [13] and interactive exploration of clusters. 
To be understandable, cluster descriptions should appear short in length and simple in format. Sum of Rectangles (SOR), simply taking the union of a set of rectangles, has been the canonical format for cluster descriptions in the database literature. However, this relatively restricted for- mat may produce unnecessarily lengthy descriptions. We introduce two novel description formats, leading to more expressive power yet still simple enough to be intuitively understandable. The SOR¡ format describes a cluster as the difference of its bounding box and a SOR description of the non-cluster points within the box. The kSOR§ for- mat allows describing different parts of a cluster separately, using either SOR or SOR¡ descriptions. We prove that the kSOR§-based description language is equivalently ex- pressive to the (most general) propositional language [18]. 
Meanwhile, cluster descriptions should cover cluster contents accurately, which conﬂicts with the goal of min- imizing description length. The Pareto front for the bicrite- ria problem of optimizing description accuracy and length, as illustrated in Figure 3, offers the best trade-offs between accuracy and interpretability for a given format. To solve the bicriteria problem, we introduce the novel Maximum Description Accuracy (MDA) problem with the objective of maximizing description accuracy at a given description length. The optimal solutions to the MDA problems with different length speciﬁcations up to a maximal length con- stitute the Pareto front. The maximal length to specify (20 in Figure 3) is determined by the optimal solution to the Minimum Description Length (MDL) problem, which aims at ﬁnding some shortest perfectly accurate description that covers a cluster completely and exclusively. Previous re- search only considered the MDL problem; however, per- fectly accurate descriptions can become very lengthy and hard to interpret for arbitrary shape clusters. The MDA problem allows trading accuracy for interpretability so that users can zoom in and out to view the clusters. 
The description problems are NP-hard. We present heuristic algorithms Learn2Cover for the MDL problem to 

========1========

approximate the maximal length, and starting from which DesTree for the MDA problems to iteratively build the so- called description trees approximating the Pareto front. The resulting descriptions, in the format of SOR or SOR¡, can be transformed into shorter kSOR§ descriptions with at least the same accuracy by FindClans, taking advantage of the exceeding expressive power of kSOR§ descriptions. 
Contributions. (1) Introduction and analysis of novel de- scription formats, SOR¡ andkSOR§, providing enhanced expressive power. (2) Deﬁnition and investigation of a novel description problem, MDA, allowing trading accuracy for interpretability. (3) Presentation and evaluation of effec- tive description heuristics, Learn2Cover, DesTree and Find- Clans, approximating the Pareto front. 
Related work. [1] studies grid data and deﬁnes a cluster as a set of connected dense cells. Their proposed Greedy Growth heuristic constructs an exact covering of a cluster with maximal isothetic rectangles. In the heuristic, a yet- uncovered dense cell is arbitrarily chosen to grow as much as possible along an arbitrarily chosen dimension and con- tinue with other dimensions until a hyper-rectangle is ob- tained. A greedy approach is then used to remove redun- dancy from the set of obtained rectangles. This special case of cluster description is related to the problem of cover- ing rectilinear polygons with axis-parallel rectangles [11], which is NP-complete [17], no polynomial time approxima- tion scheme [3], and usually studied in 2-dimensional space in the computational geometry community (e.g., [15]). 
[16] also studies grid data but generalizes the description problem studied in [1] by allowing covering some “don’t care” cells to reduce the cardinality of the set of rectangles. Their proposed Algorithm BP bases on Greedy Growth to generate the initial set of rectangles, then performs greedy pairwise merges of rectangles without covering undesired cells and with limited “don’t care” cells for use. 
Greedy Growth and BP explicitly work on cluster de- scription. However, despite the grid data limitation, they address the MDL problem solely while our focus is on the more useful and practical MDA problem. In addition, they only use the SOR format while we study and apply novel formats with more expressive power. 
Similar to [1] and [16], [18] is motivated by database applications too but with a focus on the theoretical formula- tion and analysis of concise descriptions. [18] also formally deﬁnes the general MDL problem for given language (L- MDL) and proves its NP-completeness. As the initial work of this study, [9] discusses cluster description formats, prob- lems and algorithms at the introductory level. [10] extends the description problem to the classiﬁcation problem. 
Axis-parallel decision trees [5] can be related to clus- ter description technically as they provide feasible solutions to the MDL and MDA problems even if with different ob- 
jectives. Consider a closed rectangular instance space, the leaf nodes of a decision tree correspond to a set of isothetic rectangles forming a partition of the training data (and the instance space). Stipulating rectangles to be disjoint, deci- sion tree methods can be considered addressing a partition- ing problem (with the additional constraint of partitioning the instance space) while cluster description is essentially a covering problem allowing overlapping. Partitioning prob- lems are “easier” than covering problems in the sense that they have a smaller search space. Algorithms for a parti- tioning problem usually work too for the associated cov- ering problem but typically generate larger covers. In de- cision tree induction, the preference for shorter trees co- incides with the preference for shorter description length in the MDL problem. As in the MDA problem, decision tree pruning allows trading accuracy (on training data) for shorter trees, and the technique can be applied to generate feasible solutions for the MDA problems. 
Indirectly related work in the theory community exists. [6] studies the red blue set cover problem. Given a set of red and blue elements and a family which is a subset of the power set of the element set, ﬁnd a subfamily of given cardinality that covers all the blue elements and the min- imum number of red elements. [7] studies the maximum box problem. Given two ﬁnite sets of points, ﬁnd a hyper- rectangle that covers the maximum number of points from one designated set and none from the other. Both problems are NP-hard and related to the MDA problem (with preci- sion at ﬁxed recall of 1 and recall at ﬁxed precision of 1 as the accuracy measures respectively, see x3.1), except that the former is given an alphabet (family) and restricted to the SOR format, and the latter is limited to use one rectangle. 
The above discussed research more or less roots in the classical minimum set cover and maximum coverage prob- lems. The former attempts to select as few as possible sub- sets from a given family such that each element in any sub- set of the family is covered; the latter, a close relative, at- tempts to select k subsets from the family such that their union has the maximum cardinality. The simple greedy al- gorithm, iteratively picking the subset that covers the maxi- mum number of uncovered elements, approximates the two NP-hard problems within (1+ln n)[14] and(1¡1e)[12] re- spectively. The ratios are optimal unless NP is constrained in quasi-polynomial time [8]. The minimum set cover and maximum coverage problems are related to the MDL and MDA (with recall at ﬁxed precision of 1 as the accuracy measure) problems respectively except that they are given an alphabet (family) and restricted to the SOR format. 
Organization of the paper. In Section 2 we introduce and analyze the description formats. In Section 3 we formalize the description problems. We present heuristic algorithms in Section 4, report empirical evaluations in Section 5, and conclude the paper in Section 6. 

========2========

2. Alphabets, formats, and languages 
In this section, we study alphabets, formats, and lan- guages for cluster descriptions in depth so as to gain insights into the description problems with different given formats. 
2.1. Preliminaries 
Given a ﬁnite set of multi-dimensional points U as the universe, and a set of isothetic hyper-rectangles § as the alphabet, each of which is a symbol and subset of U con- taining points covered by the corresponding rectangle. A description format F allows certain Boolean set expressions over the alphabet. All such expressions constitute the de- scription language L with each expression E 2 L being a possible description for a given subset C  U. The vocab- ulary for E, VE, is the set of symbols in E. We summarize the notations used and to be used for easy lookup. 
D: data space; D = D1 £ D2 £ ::: £ Dd 
U: data set; U  D 
R: rectangle or the set of points it covers; R  U §: alphabet; a set of symbols with each as a rectangle VE: vocabulary of expression E; set of symbols used in E jjEjj: length of expression E; jjEjj = jVEj 
Bu: bounding box for u; u is a set of points or rectangles C: set of points in a given cluster; C  U 
C¡: set of points in BC but not in C; C¡ = BC ¡ C EF;§: expression in format F with vocabulary in § LF;§: language comprising all EF;§ expressions 
Note that R (E) is overloaded to denote a rectangle (ex- pression) or the set of points it covers (describes). The dis- tinction should be made clear by the context. § is often left unspeciﬁed in EF;§ and LF;§ when assuming some default alphabet (to be discussed shortly). “ + ”, “¢”, “ – ” and “:” are used to denote Boolean set operators union, intersection, difference and complement. 
Two descriptions E1 and E2 are equivalent, denoted by E1 = E2, if they cover the same set of points. Logical equivalence implies equivalence but not vice versa. 
jjEjj indicates the interpretability of E for a given for- mat. There are two simple ways of deﬁning jjEjj, absolute length and relative length. Absolute length is the total num- ber of occurrences of symbols in E; relative length is the cardinality ofVE. Neither alone captures the interpretability of E perfectly. The former overestimates the repeated sym- bols; the latter underestimates the repeated symbols. The two converge if the repeated symbols are few, which we ex- pect to be the case for cluster descriptions. We deﬁne jjEjj to be the relative length of E for the ease of analysis. 
Description accuracy is another important measure for the goodness of E, which we will discuss in more details in section 3. For the use of this section, we conservatively de- ﬁne the “more accurate than” relationship for descriptions. 
A description for C is more accurate than the other if it cov- ers more points from C and less points from C¡. 
Deﬁnition 2.1 (more accurate than) Given E1 and E2 as descriptions for a cluster C, we say E1 is more accurate than E2, denoted by E1 ‚accu E2, if jE1 ¢ Cj ‚ jE2 ¢ Cj¡ 
and jE1 ¢ C j • jE2 ¢ C¡j. 
(E1¢C ¶ E2¢C)^(E1¢C¡  E2¢C¡) ) E1 ‚accu E2. 
A description problem, viewed as searching, is to search good descriptions that optimize some objective function in a given description language. A more general description language implies more expressive power. Language L1 is more general than language L2 if L1 ¶ L2. To character- ize expressive power more precisely, we deﬁne the “more expressive than” relationship for languages. A language is more expressive than the other if there is a shorter and more accurate description in it for any description from the other. 
Deﬁnition 2.2 (more expressive than) Given two descrip- tion languages L1, L2 and a cluster C, we say L1 is more expressive than L2, denoted by L1 ‚exp L2, if for any de- scription E2 2 L2, there exists some description E1 2 L1 with jjE1jj • jjE2jj and E1 ‚accu E2. 
Also, L1 =exp L2 if (L1 ‚exp L2) ^ (L2 ‚exp L1). 
A more expressive language is guaranteed to contain “better” expressions with respect to length and accuracy. Certainly, L1 ¶ L2 ) L1 ‚exp L2. Yet to restrict the search space, we do not want languages to be unnecessarily general. This concern carries on through the following dis- cussions on alphabets and formats, by which languages are speciﬁed. 
2.2. Description alphabets 
Unlike the set cover problem, alphabet§ is not explicitly given in cluster description. Assuming given §, a descrip- tion problem, MDA or MDL, can be considered searching through a given language L for the optimal expression. We call such problems L-problems; in particular, L-MDA or L- MDL. The L-problems, to be detailed shortly, are variants of the set cover problem. We study alphabets mainly for the purpose of gaining insights into the description problems by relating their L-problems to the classical set cover problem. 
§ is potentially an inﬁnite set since for a set of points, there are an inﬁnite number of covering rectangles with each being a candidate symbol. A simple ﬁnite alphabet can be deﬁned as the set of bounding boxes for the subsets of BC, i.e., §most = fBS jS  BCg. The alphabet is ﬁ- nite since there is a unique bounding box for a set of points. §most is the most general alphabet relevant to the task of describing C; however, it can be unnecessarily general for some L-problem with a mismatched format. It is desirable to have some most speciﬁc sufﬁciently general alphabets. 

========3========

Letf(L-p) denote the feasible region of anL-problemL- p; certainly, f(L-p)  L. We say § is sufﬁciently general for LF;§-p if LF;§ ‚exp f(LF;§ 
most 
-p), which roughly means § does not lose to §most if used in the L-p case. On top of being sufﬁciently general, § is most speciﬁc if removing any element from it, § would not be sufﬁciently general anymore. In the following, we deﬁne some alpha- bets with this desireable property. 
§pure = fBS jBS ¢ C¡ = ; ^ S  C ^ S = 6 ;g¡ 
§ 
pure 
= fBS jBS ¢ C = ; ^ S  C¡ ^ S = 6 ;g §mix = fBS jS  C ^ S = 6 ;g¡ 
§ 
mix 
= fBS jS  C¡ ^ S = 6 ;g 
§pure (§¡pure) contains pure rectangles covering points from C ( C¡) only. Symbols in §mix (§ 
¡ 
mix), however, can be mixed allowing points from C and C¡ to co-exist. Ap- parently, §pure  §mix  §most and §¡pure  §¡mix  §most. Multiple subsets may match to the same bounding box, e.g., B13 = B123 in Figure 1, where B13 is short for Bf1;3g and so on. The ﬁgure illustrates §pure and §mix. 
Examples of some L-problems with matching alpha- 
bet and format are LSOR;§ 
pure-MDL, 
LSOR 
¡;§¡ 
-MDL, 
pure 
LSOR;§ 
mix-MDA and 
LSOR 
¡;§¡ 
-MDA. In addition, we 
mix 
deﬁne §k = §mix + §¡pure, then LkSOR §;§k -MDL and LkSOR §;§k -MDA are also such examples. Due to the page limit, we omit further explanations. 
Such a desirable§ is assumed given by default if it is left unspeciﬁed in EF;§ or LF;§. Although these default alpha- bets are made speciﬁc, they can still be prohibitively large (e.g., O(j2Cj) and not scalable to real problems. Therefore, it is practically infeasible to generate § and apply existing set cover approximations on description problems. 
2.3. Description formats and languages 
We require descriptions to be interpretable. For descrip- tions to be interpretable, the description format has to have a simple and clean structure. Sum of Rectangles (SOR), de- noting the union of a set of rectangles, serves this purpose well and has been the canonical format for cluster descrip- tions in the literature (e.g., [1, 16]). For better interpretabil- ity, we also want descriptions to be as short as possible. To minimize the description length of SOR descriptions has been the common description problem. 
Nevertheless, there is a trade-off between our prefer- ences for simpler formats and shorter description length. Simple formats such as SOR may restrict the search space too much leading to languages with low expressive power. On the other hand, if a description format allows arbitrary Boolean operations over a given alphabet, we certainly have the most general and expressive language containing the shortest descriptions, but such descriptions are likely hard to 
1 
 B 
13  B 
123 
B12 
C2 
       
4 
R1’ 
2 
B23 
3 
C1 
C = {1, 2, 3}     C– = {4}  
    SOR : 5   SOR–: 4   kSOR±: 3  
＄pure  = { B1, B2, B3, B23} ＄mix  = { B1, B2, B3, B12, B13, B23} 
    E 
kSOR±(C) = 
 E 
SOR(C1) + 
 E 
SOR–(C2) 
                   =  BC1 + (BC2 – R1’) 
Figure 1. Alphabets. 
Figure 2. Formats. 
comprehend due to their complexity in format despite their succinctness in length. Moreover, not well-structured com- plex formats bring difﬁculties in manipulation of symbols and design of efﬁcient and effective searching strategies. 
Clearly, we require description languages with high ex- pressive power yet in intuitively understandable formats. In the following, we explore several alternative description formats beyond SOR, in particular, SOR¡ and kSOR§. 
While SOR takes the form of R1 + R2 + ::: + Rl, a¡ 
SOR description, in describing C, takes the set difference between BC and a SOR description for C¡. 
Deﬁnition 2.3 (SOR¡ description) Given a cluster C, a SOR¡ description for C, ESOR ¡ (C), is a Boolean expres- sion in the form of BC ¡ESOR(C¡), where ESOR(C¡) is¡ 
a SOR description for C . 
In addition, a SOR§ description for C, ESOR §(C), is an expression in the form of either ESOR(C) or ESOR ¡(C). Clearly, LSOR § ¶ LSOR and LSOR § ‚exp LSOR. In describing a cluster C, SOR and SOR¡ descrip- tions together nicely cover two situations where C is easier to describe or C¡ is easier to describe. Different data distri- butions, which are usually not known in advance, favor dif- ferent formats. In Figure 2, consider C2 as a single cluster¡ to be described with perfect accuracy, certainly SOR de- scriptions are favored. The shortest ESOR(C2) has length 4 whereas the shortest ESOR ¡(C2) has length 2. 
SOR¡ descriptions have a structure as simple and clean as SOR descriptions. In addition, the added BC draws a big picture of cluster C and contributes to interpretability in a positive way. The two formats together also allow us to view C from two different angles. Note that the special rectangle BC is required for the format, it is not included in the default alphabets either counted in jjEjj for simplicity. 
SOR§ descriptions generally serve well for the purpose of describing compact and distinctive clusters. Neverthe- less, arbitrary shape clusters are not uncommon and for such applications, we may want to further increase the expressive power of languages by allowing less restrictive formats. For example, if some parts of cluster C favor SOR and some 

========4========

other parts favor SOR¡, then SOR§ is too restrictive to consider different parts separately. Instead, it can only pro- vide a global treatment for C. To overcome this disadvan- tage, we introduce kSOR§ descriptions. 
Deﬁnition 2.4 (kSOR§ description) Given a cluster C, a kSOR§ description for C, EkSOR §(C), is a Boolean ex- pression in the form ofSESOR §(C1) + ESOR §(C2) + ::: +k 
ESOR §(Ck), where 
i=1 
Ci = C. 
Clearly, kSOR§ descriptions generalize SOR§ de- scriptions by allowing different parts of C to be described separately; and the latter one is a special case of the for- mer one with k = 1. In Figure 2, C1 favors SOR whereas¡ 
C2 favors SOR . The shortest ESOR(C) and ESOR ¡ (C) have length 5 and 4 respectively. kSOR§ is able to provide local treatments for C1 and C2 separately and the shortest§ EkSOR §(C) has length 3. In many situations kSOR can be found much more effective than other simpler formats; but how expressive is LkSOR § precisely? In the following, we compare it with the propositional language, the most general description language we consider (as in [18]). 
Deﬁnition 2.5 (propositional language) Given § as the al- phabet, LP;§ is the propositional language comprising ex- pressions allowing usual set operations of union, intersec- tion and difference over §. 
Theorem 2.6 LkSOR §;§k =exp LP;§ 
k 
Proof LP;§ 
k 
¶ LkSOR §;§k ) LP;§ 
k 
‚exp LkSOR §;§k ; we only need to prove LkSOR §;§k ‚exp LP;§ 
k 
. 
Consider any E 2 LP;§ 
k 
. Since E can be re-written as EDNF in disjunctive normal form with the same set of vocabulary, thus jjEjj = jjEDNFjj and E = EDNF. Each disjunct inEDNF is a conjunction of literals and each literal: 
takes the form ofR or R whereR 2 §k. Consider any dis- junct Ej in EDNF, since axis-parallel rectangles are inter-0 
section closed, Ej can be re-written as E 
j, which takes one of the following three forms: (1) E0j = R0; (2) E0j = R0 ¢: 
Rx¢ 
:Ry 
¢::: ¢ 
:Rz; (3) 
E0j = 
:Rx 
¢ 
:Ry 
¢::: ¢ 
:Rz. In all the three cases jjE0jjj • jjEjjj and E0j = Ej. 
For case 3, due to the generalized De Morgan’s law, E0j = 
:Rx 
¢ 
:Ry 
¢ ::: ¢ 
:Rz 
= 
:(Rx 
+ Ry + ::: + Rz) = BC ¡(Rx +Ry +:::+Rz), which is a SOR¡ description. 
For case 1 and 2, we suppose R0 2 §k. Then for case 1,0 
E 
j 
is a SOR description. For case 2, due to the generalized De Morgan’s law, E0j = R0 ¢ 
:(Rx 
+ Ry + ::: + Rz) = R0 ¡(Rx +Ry +:::+Rz), which is a SOR¡ description.§ 
Then, Ej can be re-written as an equivalent SOR de- scription with length • jjEjjj. We do this for every disjunct§ of EDNF, then E can be re-written as a kSOR0 descrip- tion E 2 LkSOR §;§k with jjE0jj • jjEjj and E0 = E, which implies LkSOR §;§k ‚exp LP;§ 
k 
. 
Note that for case 1 and 2, we have supposed R0 2 §k, which may not hold since §k is not intersection closed. As a simple counter-example, the intersection of two bounding boxes may not be a bounding box. However, we note that 
for the two cases, the purpose of E0j is to describe R0 ¢C = C0  C, thus R00 = BC 
0 
2 §k. As expressions of length 1 describing C0, R00 ‚accu R0 because (R00¢C0 = R0¢C0)^ (R0 
¡ ¡ 0 0 
0 
¢ C0  R0 ¢ C0 ). We replace R0 with R0 in Ej to get E00j , then E00 
j 
‚accu Ej 
0 
and jjEj 
00 
jj = jjEjjj. Then, 
0 
Ej can be re-written as a more accurate SOR§ description with length • jjEjjj. Wo do this for every disjoint of EDNF,§ 
then E can be re-written as a kSOR description E00 2 LkSOR §;§k with jjE00jj • jjEjj and E00 ‚accu E, which implies LkSOR §;§k ‚exp LP;§ 
k 
. 
Theorem 2.6 does not hold if description length jjEjj is deﬁned as the absolute length, in which case E = (R1 +0 R2)¡R3 in LP has length 3 but the equivalent E = (R1¡ R3) + (R2 ¡ R3) in LkSOR § has length 4. Nevertheless, the general conclusion persists, that is, LkSOR § is a very expressive language close or equal to LP. 
Despite its exceptional expressive power, the kSOR 
§ format is very simple and conceptually clear, allowing only one level of nesting as the SOR¡ format. It is also well- structured to ease the design of searching strategies. 
Assuming some given default alphabet, previous re- search studied cluster description as searching the shortest expression in LSOR;§ 
pure, the simplest and least expressive language we discussed in this section. We study the same problem but considering other more expressive languages LSOR § and LkSOR §, and our main focus is on the problem of ﬁnding the best trade-offs between accuracy and inter- pretability, as to be introduced in the following section. 
3. Cluster description problems 
A description problem is to ﬁnd a description for a clus- ter in a given format that optimizes some objective. In this section, we introduce cluster description problems with dif- ferent objective measures. 
3.1. Objective measures 
We want to describe a given cluster C with good inter- pretability and accuracy. Simple formats and shorter de- scriptions lead to improved interpretability. We have stud- ied alternative description formats that are intuitively com- prehensible. Within a given description format, description length is the proper objective measure for interpretability. 
In addition to interpretability, the objective of minimiz- ing description length can also be motivated from a “data compression” point of view. There are many situations when we need to retrieve the original cluster records; e.g., to 

========5========

Cluster descriptions provide a neat and standalone way of 
1 
0.8 
ccuracy 
0.6 
A 
0.4 
feasible region of MDL Pareto front 
approximation to Pareto front 
0.2 
send promotion brochures to a targeted class of customers,6 0.79 0.687 
to perform statistical analysis, or in a query-based itera-8 
0.82 0.73 tive mining environment as advocated by [13], to resume 
0.85 0.77 the mining process from stored temporary or partial results. 
9 0.87 0.8 
10 0.9 0.82 “storing and retrieving” cluster contents.11 
0.915 0.84 
In DBMS systems, an isothetic rectangle can be spec-12 0.925 0.86 iﬁed by a Boolean search condition such as13 0.937 0.871 • D1 • 
10^:::^5 • Dd • 50. A cluster description is then a com- pound search condition for the points in the cluster, which can be used in the WHERE clause of a SELECT query state- ment to retrieve the cluster contents entirely. In this sce- nario, the cluster description process resembles encoding and the cluster retrieval process resembles decoding. The compression ratio for cluster description E can be roughly deﬁned as jEj = (jjEjj £ 2), as each rectangle takes twice as much space as each point. The goal of large compres- sion ratio leads to the objective of minimizing description length. Meanwhile, shorter length also speeds up the re- trieval process by saving condition checking time [18]. 
Accuracy is another important measure for the good- ness of cluster descriptions. An accurate description should cover many points in the cluster and few points not in the cluster. To precisely characterize description accuracy, we borrow some notations from the information retrieval com- munity [2] and deﬁne recall and precision for a description E of cluster C. 
recall = jE ¢ Cj = jCj precision = jE ¢ Cj = jEj 
If we only consider recall, the bounding box BC could make a perfectly accurate description; if we only consider precision, any single point in C would do the same. The F-measure considers both recall and precision and is the harmonic mean of the two. 
f = 
2 £ recall £ precision 
recall + precision 
A perfectly accurate description with f = 1 has recall = 1 and precision = 1. In a perfectly accurate SOR or SOR¡ description, all rectangles are pure in the sense that they contain same-class points only. 
The F-measure does not ﬁt situations where users want to specify constraints on either recall or precision. We in- troduce two additional measures to provide this ﬂexibility. The ﬁrst is recall at ﬁxed precision; often, we want to ﬁx precision at 1. The second is precision at ﬁxed recall; of- ten, we want to ﬁx recall at 1. The measures can be found useful in many situations. If we can afford to lose points in C much more than to include points in C¡, then we can choose recall at ﬁxed precision of 1 to sacriﬁce recall and protect precision as in the maximum box problem [7]; in the opposite situation, we can choose precision at ﬁxed recall of 1 as in the red blue set cover problem [6]. 
feasible region of the bicriteria problem = union of feasible regions of MDA problems  
Length 
0 
0 
5 
10 15 20 25 
Figure 3. Accuracy vs. length. 
3.2. MDA problem and MDL problem 
Description length and accuracy are two conﬂicting ob- jective measures that cannot be optimized simultaneously. The Pareto front for the bicriteria problem, as illustrated in Figure 3, offers the best trade-offs between accuracy and interpretability for a given format. To solve the bicriteria problem and obtain the best trade-offs, we introduce the novel Maximum Description Accuracy (MDA) problem. 
Deﬁnition 3.1 (Maximum Description Accuracy problem) Given a cluster C, a description format F, an integer l, and an accuracy measure, ﬁnd a Boolean expression E in format F with jjEjj • l such that the accuracy measure is maximized. 
The optimal solutions to the MDA problems with differ- ent length speciﬁcations up to a maximal length constitute the Pareto front. The vertical lines in Figure 3 illustrate the feasible regions of the MDA problems, whose union is the feasible region of the bicriteria problem. The maximal length is the length of some shortest description with perfect accuracy, which is 20 in Figure 3. It is pointless to spec- ify larger lengths as the best accuracy has been achieved. To determine this maximal length, we deﬁne the Minimum Description Length (MDL) problem, whose objective is to ﬁnd some shortest description that covers a given cluster completely and exclusively, i.e., with f = 1. 
Deﬁnition 3.2 (Minimum Description Length problem) Given a cluster C and a description format F, ﬁnd a Boolean expression E in format F with minimum length such that (E ¢ C = C) ^ (E ¢ C¡ = ;). 
The optimal solution to the MDL problem gives the max- imal length to specify in solving the MDA problems. The feasible region of the MDL problem is also illustrated in Figure 3. Previous research only considered the MDL prob- lem with SOR as the description format. However, in prac- tice, perfectly accurate descriptions can become lengthy and hard to interpret for arbitrary shape clusters. The MDA problem allows trading accuracy for interpretability so that users can zoom in and out to view the clusters. From a “data 

========6========

compression” point of view, description requiring perfect accuracy resembles lossless compression while description allowing lower accuracy resembles lossy compression. 
Assuming some given default alphabets as previously discussed, the L-problems, easier than their counterparts, can be related to some variants of the set cover problem. In particular, LSOR;§ 
pure 
-MDL corresponds to the minimum set cover problem, which is known to be NP-hard. Other L- MDL problems are harder in the sense that they have larger search spaces with more general languages. 
Given the recall at ﬁxed precision of 1 accuracy measure, the LSOR;§ 
pure 
-MDA problem corresponds to the maxi- mum coverage problem, which is known to be NP-hard. Given the precision at ﬁxed recall of 1 accuracy measure, the LSOR;§ 
pure-MDA problem corresponds to the red blue set cover problem, which is also NP-hard [6]. Given the F-measure, the decision version of the LSOR;§ 
pure 
-MDA problem is reducible to the decision problem of either of the two. Other L-MDA problems are harder in the sense that they have larger search spaces. 
As we have seen, the cluster description problems, with different format and accuracy measure speciﬁcations as dis- cussed, are all NP-hard. We present efﬁcient and effective heuristic algorithms for these problems in the next section. 
4. Description algorithms 
In this section, we present three heuristic algorithms. Learn2Cover solves the MDL problem approximating the maximal length. Starting with the output of Learn2Cover, DesTree iteratively builds the so-called description tree for the MDA problems approximating the Pareto front. Find- Clans transforms the output descriptions from DesTree into shorterkSOR§ descriptions without reducing the accuracy. 
4.1. Learn2Cover 
Given a cluster C, Learn2Cover returns a description E for C in either SOR or SOR¡ format with f = 1. For this purpose, it sufﬁces to learn a set of pure rectangles < covering C and a set of pure rectangles <¡ covering C¡ completely. Learn2Cover is carefully designed such that < and <¡ are learned simultaneously in a single run; besides, the extra learning of <¡ does not come as a cost but rather a boost to the running time. 
Sketch of Learn2Cover. To better explain the main ideas, we give the pseudocode for the simpliﬁed Learn2Cover and its major procedure cover() in the following. 
In preprocessing(), the bounding box BC is determined; the points in BC are normalized against BC and sorted along a selected dimension Ds. Ties are broken arbitrarily. 
Algorithm: Learn2Cover 
1 preprocessing(); // sort BC along Ds 
2 foreach (ox 2 BC) f // processed in sorted order 3 if (ox 2 C) 
4 cover(<, ox, <¡); 
5 else 
6 cover(<¡, ox, <); g 
Procedure: cover(<, ox, <¡) 
1 foreach (R 2 <¡) f 
2 if (cost(R; ox) == 0) 
3 close R; g 
4 foreach (R 2 < && R is not closed) f 
5 if (cost(R; ox) == 0) f 
6 extend R to cover ox; 
7 return; gg 
8 foreach (R 2 <) f // processed in ascending order of cost 9 if (no violation against <¡) f 
10 expand R to cover ox; 
11 return; gg 
12 insert(<, Rnew); // ox not covered. Rnew = ox 
At the moment we suppose there are no mixed ties involv- ing points from different classes. In general, the choice of Ds does not have a signiﬁcant impact if the data is not ab- normally sparse; thus Ds can be arbitrarily chosen. Never- theless, Learn2Cover offers an option to choose Ds with the maximum variance, which makes the algorithm more robust against some rare, malicious cases such as large mixed tie groups. Learn2Cover is deterministic once Ds is chosen. 
Initially < = ; and <¡ = ;. Let ox be the next point from¡ BC in the sorted order to be processed. cover(<, ox, <¡ ) or cover(< , ox, <) is called upon depending on ox 2 C¡ 
or C . The two situations are symmetric. 
Suppose ox 2 C, procedure cover(<, ox, <¡) chooses a non-closed R 2 < covering no points covered by rec- tangles in <¡ and with the minimum covering cost with respect to ox to expand and cover ox. Otherwise, a new rectangle Rnew minimally covering ox will be created and added to < (line 12). A rectangle is closed if it cannot be expanded to cover any further point without causing a covering violation, i.e., covering points from the other class (lines 1, 2, 3). Violation checking can be expensive; therefore, we always calculate cost(R; ox) ﬁrst. If there is a non-closed R with cost(R; ox) = 0, we need to extend R only along Ds to cover ox, in which case violation check- ing is unnecessary (lines 4, 5, 6, 7). Otherwise rectangles are considered in ascending order of cost(R; ox) for viola- tion checking. The ﬁrst qualiﬁed rectangle will be used to cover ox (lines 8, 9, 10, 11). In the following we discuss covering cost and covering violation in more details. 

========7========

R1 
Ds 
R2 
R3 
R2 
R1 
R3 
R5 
  A  
ox 
 B  
R4 
R6 
(a) Choice of rectangles  
(b) Demonstration run  
Figure 4. Learn2Cover. 
Covering cost and choice of rectangles. The behavior of Learn2Cover largely depends on how the covering cost cost(R; ox) is deﬁned, i.e., the cost of R to cover point ox. This cost should estimate the reduction of the potential of R to cover further points after ox. Intuitively, we want to choose R with the minimum increased volume, so that rec- tangles can keep maximal potential for future expansions without incurring covering violations. 
Yet there are more issues to be concerned beyond this basic principle. First, when calculating the increased vol- ume for R, we should not consider Ds. Since points are sorted on Ds and processed in the sorted order, the exten- sion of R along Ds is the distance it has to travel to cover further points after ox. To keep R short on Ds does not help to keep its potential for future expansions. In Figure 4(a), if we considered Ds, R1 would have the biggest increased volume and not be chosen to cover ox. But this saved space would be part of the expanded R1 in covering any point af- ter ox, and whether R1 had been expanded already to cover ox or not would not make a difference. This suggests that the increased volume of R1 with respect to ox should be 0, ignoring Ds in the calculation. 
Second, if the expanded R has a length of 0 or close to 0 in any dimension, its volume and increased volume will be 0 or close to 0, which makes R a favorable choice. But R may have traveled far along some other dimensions to cover ox; its expansion potential would thus be limited. In Figure 4(a), both R1 and R3 require the same increased volume of 0 to cover ox since R1 has to be extended only along Ds and R3 has a length of 0 in one of its dimensions. However, R3 has to travel far along some dimensions other than Ds whereas R1 does not. Moreover, R2 does not have the in- creased volume of 0, but it seems not to be a worse choice than R3 because ox is more local to R2 than R3. Therefore, cost needs to ﬁx the illusion of 0 increased volume and take into account the locality of rectangles. 
In the following, we propose a deﬁnition for cost(R; ox) with these issues in consideration. Let lj(R) denote the0 length of rectangle R along dimension Dj, and R denote the expanded R in covering ox. 
vol(R) = 
Q 
j=1::d;j=s 6 
lj(R) 
aveIncV ol(R; ox) = (vol(R0) ¡ vol(R))1=(d¡1) dist(R; ox) = ( 
P 
j=1::d;j=s 6 
jlj(R0) ¡ lj(R)j2)1=2 cost(R; ox) = aveIncV ol(R; ox) + dist(R; ox) 
Ds is ignored if we project R and ox onto the sub- space DnDs. vol(R) is the volume of the projected R; aveIncV ol can be viewed as the increased volume aver- aged on each dimension; dist is precisely the Euclidean dis- tance from the projected ox to the projected R. cost is the sum of aveIncV ol and dist, i.e., we assign equal weights to both components of cost. According to this deﬁnition of cost(R; ox), the choices in Figure 4(a) would be R1, R2, and R3 in priority order. 
Sometimes there are no good rectangles available, in which case forcing greedy expansions may deteriorate the overall performance. Learn2Cover provides an expansion control parameter to limit the maximum distance each di- mension can travel at each expansion. Since data is normal- ized, the default choice of 0.5 means that each expansion cannot exceed half of the span of BC in each dimension. The parameter is user-speciﬁed but not sensitive. Without expansion control, Learn2Cover works generally well; but it may help in cases of extremely sparse or malicious datasets. 
Figure 4(b) is a real run of Learn2Cover on a toy dataset. Dark and light points denote points in C and C¡ respec- tively. Rectangles are numbered in ascending order of their creation time. Note that on processing A, a better choice of R3 was made while R4 was also available. R3 had cost of 0 with Ds ignored. If R4 had been chosen to cover A, it would have been closed before covering B. 
Covering violation and correctness. Learn2Cover is re- quired to output pure rectangles, any covering of inter-class points will be considered as a violation. 
BP considers all inter-class points in violation checking. In Learn2Cover, since points are processed in the sorted or- der, the only points that could lead to violations in the ex- pansion of Ri 2 < (<¡) are currently processed points in¡ 
Rj 2 < (<) that overlaps with the expanded Ri. Thus¡ 
< and < , the sets of rectangles to be learned, also ex- ist to help each other in violation checking to dramatically reduce the number of points in consideration. A simple aux- iliary data structure is maintained to avoid the possible per- formance deterioration in the presence of extremely dense and big rectangles. We omit the details due to the page limit. 
Learn2Cover outputs pure rectangle collections < and <¡ covering every point in C and C¡ respectively. We examine procedure cover() to argue the correctness. From the deﬁnition of cost, cost(R; ox) = 0 if and only if the projected ox is in the projected R. In such case, if R 2 <¡ 
(< ) and ox 2 C¡(C), R will not be able to cover any 

========8========

further point without covering ox, which causes a violation¡ and R will thus be closed (line 3). If R 2 < (< ) and ox 2 C (C¡), R can be extended along Ds to cover ox as (line 6) without causing any violation. If there existed o0 causing a violation, o0 must have been processed before ox with cost(R; o0) = 0, which would have caused R to be closed. In line 10, R is expanded to cover ox after vi- olation checking. In line 12, Rnew is a degenerate rectan- gle covering only one point ox. Thus, upon completion of Learn2Cover, each ox 2 BC is covered without violation. 
In the sketch of Learn2Cover, we have assumed there were no mixed ties involving points from different classes. This case may happen and even frequently on grid data. Mixed tying points may cause covering violation to one another. Learn2Cover identiﬁes mixed tie groups in the preprocessing() step, and then some extra checking is per- formed on processing ox belonging to a mixed tie group. 
4.2. DesTree 
The MDA problem is to ﬁnd a description E with a user- speciﬁed length l for cluster C maximizing a given accu- racy measure. Our algorithm DesTree takes the output from Learn2Cover, < or <¡ whose cardinality approximates the maximal length, iteratively builds a so-called description tree approximating the Pareto front. 
Description trees are tree structures resembling dendro- grams to provide overviews on alternative trade-off descrip- tions of different lengths. Accordingly, DesTree resembles agglomerative hierarchical clustering to iteratively merge child nodes into parent nodes until a single node is left. Each node in a description tree represents a rectangle; and a normal merge operation produces a parent node that is the bounding box of its child nodes. The tree grows bottom-up along a series of merge operations. 
Each horizontal cut in the tree deﬁnes a set of rectangles. For the so-called C-description tree, a cut set constitutes the vocabulary for a SOR description of C; for the so-called C¡-description tree, a cut set constitutes the vocabulary for a SOR description of C¡ leading to a SOR¡ description of C. The cardinality of a cut set, the description length, equals to the number of links being cut. Each cut offers an alternative trade-off between description length and accu- racy. The higher in the tree we cut, the shorter the length and the lower the accuracy. 
Consider a SOR (SOR¡) description, merging two rec- tangles into their bounding box may cause precision (recall) to decrease; removing a rectangle may cause recall (preci- sion) to decrease. Both operations trade the accuracy mea- sure, say f, for shorter length and we want to consider both. To integrate the removal operation in building description trees, we add a symbolic node, the empty set ;, into the leaf nodes and deﬁne the merge operator as follows. 
Φ 
R6 
Φ 
cut2 =  { R5, R3} 
R5 
R1 
R2 
R3 
R4 
Φ 
cut1 = {R5, R3, R4} cut0  =  R (R–) input: R (R–) 
Figure 5. Description tree. 
Deﬁnition 4.1 (merge) Ri merge Rj = Rparent = (1) bounding box for Ri and Rj if Ri = 6 ; and Rj = 6 ;; (2) ; otherwise 
DesTree is a greedy approach starting from the input leaf nodes, a set of pure rectangles < or <¡ generated by Learn2Cover, building the tree in a bottom-up fashion. Pairwise merge operations are performed iteratively, and the merging criterion is the biggest resulting accuracy mea- sure. The C-description tree and C¡-description tree are built separately in the same fashion. 
Figure 5 exempliﬁes a description tree. R1 » R4 are the input rectangles. R1 and R2 are chosen for the ﬁrst merge to give R5. The second merge of R4 and ; results in the removal of R4. R6, the parent node of R5 and R3, merges with ; to give the symbolic root. The lowest cut cut0 is < or¡ 
< . Each cut corresponds to a SOR or SOR¡ description. Take cut2 as an example. For a C-description tree, cut2¡ corresponds to ESOR(C) = R5+R3; for a C -description tree, it corresponds to ESOR ¡(C) = BC ¡(R5+R3). De- scription trees are not necessarily binary. A merge could result in more rectangles fully contained in the parent rec- tangle. Nonetheless, the merging criterion discourages branchy trees and Figure 5 is a typical example. 
The merging process can be simpliﬁed for some accu- racy measures. Given recall at ﬁxed precision of 1, for the C-description tree, only merge operation (2) (the removal operation) needs to be considered, and the root is always ;; for the C¡-description tree, only merge operation (1) (the normal merge operation) needs to be considered, and the root is always BC ¡. For both cases, precision is guaranteed to be 1 and recall reduces along the merging process. 
Given precision at ﬁxed recall of 1, for the C-description tree, only merge operation (1) needs to be considered, and the root is always BC; for the C¡-description tree, only merge operation (2) needs to be considered, and the root is always ;. For both cases, recall is guaranteed to be 1 and precision reduces along the merging process. 
We can easily prove the accuracy measure, recall at ﬁxed precision of 1 or precision at ﬁxed recall of 1, reduces monotonically along the merging process in DesTree. With respect to the F-measure, though evident in experiments, it is non-trivial to construct a proof of the same property. 

========9========

4.3. FindClans 
          
R2 
         R 4 
      R3 
R   = { R1, R2, R3, R4, R5}– 
R = {R1’, R2’, R3’}  
FindClans takes as input a cut (denoted by T in the following) from a description tree representing a SOR or SOR¡ description, outputs a kSOR§ description with shorter length and equal or better accuracy. 
The algorithm is based on the concept of clan. Let SORV be a SOR description with vocabulary V , e.g., SORV = R1 + R2 for V = fR1; R2g. Intuitively, a clan N  T is a group of rectangles that dominate (densely pop- ulate) a local region, so that by replacing them as a whole, SORN can be rewritten as a shorter and more accurate¡ 
SOR description for the targeted points in the region. 
       R1’ 
T   =  R  = { R1, R2, R3, R4, R5} ESOR(C) = R1+ R2 + R3 + R4 + R5 
       R 5 
N = {R2, R3, R4, R5} ⊆ T 
N’ = {R1’} 
BN  = bounding box for  N 
N.score = |N|  – | N’|  – 1 = 4 – 1 – 1 = 2   
R2’ 
R1 
R3’ 
EkSOR±(C) = R1+ (BN Š R1’)   
length reduction = ||EkSOR±(C)|| – ||ESOR(C)|| = 2 
Figure 6. Clan helps in length reduction. 
Deﬁnition 4.2 (clan) Given T as a cut from a C (C¡) - description tree, N  T is a clan if jNj ¡ jN0j > 1 and BN ¡ SORN 0 ‚accu SORN in describing SORN ¢ C (SORN ¢C 
¡), where 
BN is the bounding box of N and N0 a set of rectangles associated withN called the replacement of N. We also refer to jNj ¡ jN0j ¡ 1 as N:score. 
Note that the purpose of SORN is to describe SORN ¢¡ 
C (SORN ¢ C ) if T is from a C (C¡) -description tree. N:score is the possible length reduction offered by a single clan N since SORN will be replaced by BN ¡ SORN 0. Two clans N1 and N2 are disjoint if N1 \N2 = ;. For a set of mutually disjoint clans,P Clans, the total length reduction is at least Ni:score where Ni 2 Clans. 
Figure 6 uses the example in Figure 2 and illustrates how a clan can help to rewrite a SOR description represented by T into a shorter kSOR§ description. 
Suppose we have found Clans for T and T is from a C-description tree, it is straightforward to rewrite the input SOR description ESOR(C) = SORT into a kSOR§ de- scription as illustrated in Figure 6. For each N 2 Clans, we simply replace SORN in SORT by the shorter and more accurate (BN ¡ SORN 0). 
If T is from a C 
¡-description tree, the input 
SOR¡ de- scription is ESOR ¡(C) = BC ¡ SORT. For each N 2 Clans, we replace SORN in SORT by BN and add back 
(SORN¢C¡) since N0 contains pure rectangles completely¡ 
covering BN ¢C (BN ¢C). Thus given a candidate clan N,0 
N is uniquely determined andN is a clan ifjNj¡jN0j > 1. 
Algorithm Findclans, as presented in the following, con- tinues to call procedure ﬁndAClan() to ﬁnd a clan N in the updated T and insert it into Clans. ﬁndAClan() ﬁrst checks each pair of rectangles in T and ﬁnds theN with the highest score. bestN is used to keep track of the best stage of theN, which continues to grow greedily one more R 2 (T ¡ theN) at a time resulting in the largest score increase, until no more rectangles available. bestN is re- turned if it is a clan; otherwise, NULL. 
Algorithm: FindClans (T) 1 Clans ˆ ;; 
2 N ˆ findAClan(T); 3 while (N ! = NULL) f 4 insert(Clans, N); 5 T ˆ T ¡ N; 6 N ˆ findAClan(T); g 7 return Clans; 
SORN 0. As an example, let ESOR ¡ (C) = BC ¡SORT = BC ¡(R1 +R2 +R3 +R4 +R5). Suppose we have a clan0 
N = fR2; R3; R4; R5g with replacement N = fR01g, then E BC¡(R1+BN)+R1. The length reduction 
0kSOR§(C) = 
=T:score = 4¡1¡1 = jjESOR ¡(C)jj¡jjEkSOR§(C)jj = 6 ¡ 4 = 2. It is easy to verify that after all such replace- ments, the resulting EkSOR §(C) is shorter and more ac- curate than ESOR(C) or ESOR ¡(C) with respect to any accuracy measure we discussed. 
All we need is to ﬁnd Clans. To simplify the task, we deﬁne N0, the replacement of N, to contain each rec- tangle R 2 <¡ (<) overlapping with BN if T is from¡ 
a C (C ) -description tree. Then, it is guaranteed that BN ¡ SORN 0 ‚accu SORN in describing SORN ¢ C 
Procedure: ﬁndAClan(T) 
1 ﬁnd theN; // consider each pair in T 
2 bestN ˆ theN; 
3 while (theN  T) f 
4 grow theN; // consider each R 2 (T ¡ theN) 5 if (theN:score > bestN:score) 6 bestN ˆ theN; g 
7 if (bestN:score ‚ 1) 
8 return bestN; 
9 else 
10 return NULL; 
5. Experimental results 
We experimentally evaluated and compared our meth- ods against CART (Salford Systems, Version 5.0) and BP [16]. While decision tree classiﬁers, as argued in re- lated work, can be applied to the MDA and MDL problems, 

========10========

BP addresses the MDL problem only. We implemented Learn2Cover, DesTree, FindClans, and BP. For BP, we also implemented Greedy Growth and a synthetic grid data gen- erator. To make our experiments reproducible, real datasets from the UCI repository [4] with numerical attributes and without missing values were used, where data records with the same class label were treated as a cluster. Note that, in the broad sense, a cluster can be used to represent an arbi- trary class of labeled data that require discriminative gen- eralization. The notion of rectangle can be extended (but not implemented in this version) to tolerate categorical at- tributes; e.g., to use sets instead of intervals. Rectangles do not provide generalization on the categorical attributes. 
5.1. Comparisons with CART 
To approximate the Pareto front, our approach starts with applying Learn2Cover for the MDL problem, then DesTree for the MDA problems to build description trees. Decision tree classiﬁers provide feasible solutions to both the MDL and MDA problems. We compared Learn2Cover and De- sTree with CART on UCI datasets. In each experiment we described one class of points C, considering all points from other classes within BC, the bounding box for C, as C¡. 
Learn2Cover vs. CART. In each experiment, BC was fed to Learn2Cover and CART. The CART parameters were set such that a complete tree without misclassiﬁcations could be built. The entropy method was used for tree splits. 
Table 1. Learn2Cover vs. CART.¡j 
dataset cls dim jCj jC Learn2Cover CART FindClans wine 2 13 71 12 2 / 1 5 / 5 –0 iono g 34 225 11 3 / 2 7 / 6 –0 iris vir 4 50 18 3 / 3 5 / 5 –0 blocks 4 10 88 1588 31 / 12 35 / 20 –6 blocks 2 10 329 4974 26 / 19 48 / 49 –7 yeast cyt 8 463 783 69 / 97 144 / 174 –13 yeast nuc 8 429 939 74 / 87 164 / 170 –16 abalone I 8 1342 2549 165 / 179 313 / 317 –37 abalone F 8 1307 2693 214 / 259 454 / 449 –62 abalone M 8 1528 2626 251 / 278 510 / 488 –54 
For each dataset, Table 1 presents the class label, the di- mensionality, the cardinalities of C and C¡, and the results. For Learn2Cover and CART, a=b denotes the cardinalities of the two sets of rectangles covering C and C¡ respec- tively. For CART, a and b correspond to the numbers of leaf nodes of the two classes. For Learn2Cover, they corre- spond to j<j and j<¡j. The smallest a or b is highlighted in bold font. We observe that, on average, Learn2Cover needs only half of the description length required by CART. Re- sults from other UCI datasets as well as synthetic datasets are not presented. However, the above observation holds consistently through all the experiments. 
1 
f 
0.9 
0.8 
yeast(cyt) DesTree 
0.7 
blocks(2) DesTree 
0.6 
yeast(cyt) CART 
blocks(2) CART 
l 
0.5 
0 10 20 30 40 50 60 70 80 
Figure 7. DesTree vs. CART. 
The output of Learn2Cover was further fed into Find- Clans for additional length reduction. In Table 1, ¡c de- notes the additional reduction achieved by FindClans com- paring to the shortest length (in bold font). The effective- ness of FindClans depends on the size and distribution of the input data; bigger and more complex datasets are likely to exhibit more clans, leading to more length reduction. We observe that, FindClans further reduces the shortest descrip- tion length by about 20% on average and up to 50%. 
DesTree vs. CART. In each experiment, DesTree took as input < or <¡ returned by Learn2Cover. For CART, the complete tree without misclassiﬁcations was pruned step by step. In each step, the misclassiﬁcations for both classes were counted to calculate the F-measure. The number of rectangles covering the target class C or C¡ was also recorded as the description length. 
Figure 7 demonstrates the results of DesTree (C- description tree) and CART (target class C), for clarity, on only two of the UCI datasets used in Table 1. f and l denote the F-measure and description length respectively. As ex- pected, for both methods, f decreases monotonically with decreasing l. However, the DesTree results clearly domi- nate the ones from CART. For each l, DesTree achieves a signiﬁcantly higher f, and each f a signiﬁcantly smaller l. Again, this observation holds consistently for all other experiments not presented, including ones on other UCI datasets and synthetic datasets, as well as C¡-description tree experiments on both data types. 
5.2. Comparisons with BP 
While the focus of our study is on the MDA problem, BP addresses the MDL problem only. In this series of ex- periments, we compared Learn2Cover with BP on synthetic datasets as BP works on grid data. Our data generator fol- lows exactly what [16] does for BP. It takes as input the dimensionality, the number of intervals of each dimension, and the density. Dense cells are randomly generated in a grid with speciﬁed density, then one of them is randomly selected to grow a connected dense cell set as cluster C, the rest of the dense cells in BC constitute C¡. 

========11========

In our experiments with BP, we did not limit the number of “don’t care” cells for use and allowed BP to ﬁnd the best possible results. Since BP only generates one set of rectan- gles, we used < from Learn2Cover for the comparison. We studied the averaged percentage length reduction compared to BP for varying dataset size and dimensionality. We ob- served that in all cases Learn2Cover clearly outperformed BP, gaining 20% to 50% length reduction. As a general ten- dency, the reduction increased with increasing complexity of data. FindClans further improved the results, gaining an additional 25% length reduction on average. 
BP starts with the maximal rectangles generated by Greedy Growth in a greedy manner. The “don’t care” cells may come too late to be helpful, as illustrated in Figure 8. 
-1 2 3 
C: {3, 4, 5, 6, 7};    C–: {-1, -9};    “don’t care” cells: 2, 8  
Greedy Growth: R47 + R456 + R36 
4 5 6 
BP: the same as Greedy Growth since any pairwise merge 7 8 -9 
of rectangles would cause a violation covering -1 or -9  
Learn2Cover: R2356 + R4578 
Figure 8. A typical case in BP. 
Runtime was not a major concern of this study. We did not integrate the X-tree index, on which BP relies to reduce the violation checking time. Without indexing for both, we observed that Learn2Cover ran faster than BP by one to two orders of magnitude. Recall that Learn2Cover can signiﬁ- cantly reduce the number of points in consideration for vi- olation checking. If we assume constant number of rectan- gles, Learn2Cover has a worst case runtime of O(jBCj2). 
DesTree and FindClans also have quadratic worst case runtime in the number of input rectangles, O(j<j2) or O(j<¡j2) for DesTree and O(jTj2) for FindClans respec- tively. In particular, for DesTree, the accuracy calculation results of all possible pairwise merges of rectangles can be reused in each iteration, recalculation is needed only for the resulting parent rectangle, which takes linear time. For FindClans, in each call of procedure ﬁndAClan(), the results from “ﬁnd theN” (line 1) can be reused. 
6. Conclusions 
In this paper, we systematically studied rectangle-based discriminative data generalization in the context of cluster description, which transforms clusters into patterns and pro- vides the possibility of obtaining human-comprehensible knowledge from clusters. In particular, we introduced and analyzed novel description formats, SOR¡ and kSOR§, providing enhanced expressive power; we deﬁned the novel Maximum Description Accuracy (MDA) problem, allowing users to specify different trade-offs between interpretabil- ity and accuracy; we also presented heuristic algorithms to- gether with their experimental evaluations. 
The concept of cluster in our study, in the narrow sense, is the output from clustering algorithms. Our study is mo- tivated from and can ﬁnd most applications in describing such clusters. In the broad sense, a cluster can be used to represent an arbitrary class of labeled data that require dis- criminative generalization. 
Last but not least, cluster descriptions are patterns which can be stored as tuples in a relational table, so that a cluster- ing and its associated clusters become queriable data min- ing objects. Therefore, this research can serve as a ﬁrst step for integrating clustering into the framework of induc- tive databases [13], a paradigm for query-based “second- generation” database mining systems. 
References 
[1] R. Agrawal, J. Gehrke, D.Gunopulos, and P.Paghavan. Au- 
tomatic subspace clustering of high dimensional data for 
data mining applications. In SIGMOD, 1998. 
[2] R. Baeza-Yates and B. Ribeiro-Neto. Modern Information 
Retrieval. Addison-Wesley-Longman: Harlow, UK, 1999. [3] P. Berman and B. Dasgupta. Approximating rectilinear poly- 
gon cover problems. In Algorithmica, 1997. 
[4] C. Blake and C. Merz. UCI repository of machine learning 
databases. 1998. 
[5] L. Breiman, J. Friedman, R. Olshen, and C. Stone. Classiﬁ- 
cation and Regression Trees. Wadsworth, 1984. [6] R. Carr, S. Doddi, G. Konjevod, and M. Marathe. On the 
red-blue set cover problem. In SODA, 2000. 
[7] J. Eckstein, P. Hammer, Y. Liu, M. Nediak, and B. Sime- 
one. The maximum box problem and its application to data 
analysis. In Comput. Optim. Appl., volume 23, pages 285– 
298, 2002. 
[8] U. Feige. A threshold of lnn for approximating set cover. In 
Journal of the ACM, volume 45(4), pages 634–652, 1998. [9] B. Gao and M. Ester. Cluster description formats, problems, 
and algorithms. In SDM, 2006. 
[10] B. Gao and M. Ester. Right of inference: Nearest rectangle 
learning revisited. In ECML, 2006. 
[11] M. Garey and D. Johnson. Computers and Intractability: 
A guide to the Theory of NP-completeness. W.H. Freeman: 
New York, 1979. 
[12] D. S. Hochbaum. Approximation algorithms for NP-hard 
problems. PWS Publishing Company: Boston, 1997. [13] T. Imielinski and H. Mannila. A database perspective on 
knowledge discovery. In Communications of the ACM, vol- 
ume 39(11), pages 58–64, 1996. 
[14] D. Johnson. Approximation algorithms for combinatorial 
problems. In J. Comput. System Sci., 1974. 
[15] V. A. Kumar and H. Ramesh. Covering rectilinear polygons 
with axis-parallel rectangles. In STOC, 1999. 
[16] L. Lakshmanan, R. Ng, C. Wang, X. Zhou, and T. John- 
son. The generalized MDL approach for summarization. In 
VLDB, 2002. 
[17] W. Masek. Some NP-complete set covering problems. man- 
uscript, MIT, Cambridge, MA, 1979. 
[18] A. Mendelzon and K. Pu. Concise descriptions of subsets of 
structured sets. In PODS, 2003. 

========12========

